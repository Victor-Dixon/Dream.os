{
  "timestamp": "2026-01-12T18:53:01.558234",
  "command": "python audit_harness_standalone.py dup --roots src tools scripts --out audit_outputs/duplication_jscpd.json",
  "roots": [
    "src",
    "tools",
    "scripts"
  ],
  "patterns": {
    "__init__": {
      "description": "Constructor methods",
      "regex": "def __init__\\(self.*?\\):",
      "occurrences": [
        {
          "file": "src\\swarm_pulse\\intelligence_service.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_controller_modals.py",
          "count": 2,
          "lines": [
            "    def __init__(self, agent_id: str, messaging_service):",
            "    def __init__(self, messaging_service):"
          ]
        },
        {
          "file": "src\\discord_commander\\approval_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\trading_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\trading_data_service.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\contract_notifications.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_controller.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: UnifiedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_modals.py",
          "count": 9,
          "lines": [
            "    def __init__(self, agent_id: str, messaging_service):",
            "    def __init__(self, messaging_service):",
            "    def __init__(self, messaging_service, available_agents):"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_modals_base.py",
          "count": 1,
          "lines": [
            "    def __init__(self, title: str, timeout: float = 300.0, custom_id: Optional[str] = None):"
          ]
        },
        {
          "file": "src\\discord_commander\\file_share_commands.py",
          "count": 1,
          "lines": [
            "        def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot, messaging_controller: DiscordMessagingController):"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_controller.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\music_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\status_reader.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\systems_inventory_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\test_utils.py",
          "count": 5,
          "lines": [
            "    def __init__(self, *args, **kwargs):"
          ]
        },
        {
          "file": "src\\discord_commander\\tools_commands.py",
          "count": 1,
          "lines": [
            "        def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\webhook_commands.py",
          "count": 2,
          "lines": [
            "    def __init__(self, bot):",
            "    def __init__(self, webhook_id: str, webhook_service):"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_service.py",
          "count": 1,
          "lines": [
            "    def __init__(self, webhook_url: str | None = None, session=None):"
          ]
        },
        {
          "file": "src\\discord_commander\\status_change_monitor.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot, channel_id: Optional[int] = None, scheduler=None):"
          ]
        },
        {
          "file": "src\\discord_commander\\github_book_data.py",
          "count": 1,
          "lines": [
            "    def __init__(self, data_path: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_ui_components.py",
          "count": 9,
          "lines": [
            "            def __init__(self):",
            "    def __init__(self):",
            "    def __init__(self, current_page: int = 1, total_pages: int = 1, timeout: float = 300.0):"
          ]
        },
        {
          "file": "src\\discord_commander\\github_book_commands.py",
          "count": 4,
          "lines": [
            "    def __init__(self, bot):",
            "    def __init__(self, cog):",
            "    def __init__(self, cog, chapter_data: dict, current_page: int = 1):",
            "    def __init__(self, cog, goldmines: list, current_page: int = 1):"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_data.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_embeds.py",
          "count": 1,
          "lines": [
            "    def __init__(self, data_loader):"
          ]
        },
        {
          "file": "src\\discord_commander\\command_base.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot=None, messaging_controller=None):"
          ]
        },
        {
          "file": "src\\discord_commander\\embed_factory.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\music_service.py",
          "count": 1,
          "lines": [
            "    def __init__(self, cache_dir: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\discord_commander\\status_service.py",
          "count": 3,
          "lines": [
            "    def __init__(self, ttl_seconds: int = 30, max_size: int = 20):",
            "    def __init__(self, workspace_dir: Path):",
            "    def __init__(self, workspace_dir: str = \"agent_workspaces\", cache_ttl: int = 30):"
          ]
        },
        {
          "file": "src\\discord_commander\\webhook_service.py",
          "count": 2,
          "lines": [
            "    def __init__(self, bot: Optional[commands.Bot] = None):",
            "    def __init__(self, config_dir: Path):"
          ]
        },
        {
          "file": "src\\discord_commander\\inventory_service.py",
          "count": 1,
          "lines": [
            "    def __init__(self, inventory_file: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\discord_commander\\bot_runner_service.py",
          "count": 1,
          "lines": [
            "    def __init__(self, repo_root: Path):"
          ]
        },
        {
          "file": "src\\discord_commander\\onboarding_modals.py",
          "count": 3,
          "lines": [
            "    def __init__(self):",
            "    def __init__(self, title: str = \"Agent Onboarding\", timeout: float = 300.0):"
          ]
        },
        {
          "file": "src\\discord_commander\\broadcast_modals.py",
          "count": 4,
          "lines": [
            "    def __init__(self):",
            "    def __init__(self, title: str = \"Broadcast Message\", timeout: float = 300.0):"
          ]
        },
        {
          "file": "src\\discord_commander\\template_modals.py",
          "count": 4,
          "lines": [
            "    def __init__(self):",
            "    def __init__(self, title: str = \"Template Message\", timeout: float = 300.0):"
          ]
        },
        {
          "file": "src\\discord_commander\\unified_discord_bot.py",
          "count": 1,
          "lines": [
            "    def __init__(self, token: str, channel_id: int | None = None, dry_run: bool = False):"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_channel_messenger.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\broadcast_controller_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\messaging_controller_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\status_controller_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\swarm_tasks_controller_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService | None = None):"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\broadcast_templates_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\agent_messaging_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\aria_message_agent8_modal.py",
          "count": 1,
          "lines": [
            "    def __init__(self, preferences_path: Path = None):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\aria_profile_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\bump_agent_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_message_agent7_modal.py",
          "count": 1,
          "lines": [
            "    def __init__(self, preferences_path: Path = None):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_message_agent8_modal.py",
          "count": 1,
          "lines": [
            "    def __init__(self, preferences_path: Path = None):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_profile_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\confirm_restart_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\confirm_shutdown_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\help_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\swarm_snapshot_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, snapshot: Dict):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\swarm_status_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\unstall_agent_view.py",
          "count": 1,
          "lines": [
            "    def __init__(self, messaging_service: ConsolidatedMessagingService):"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\main_control_panel_view.py",
          "count": 6,
          "lines": [
            "    def __init__(self, bot=None, timeout: float = 300.0):",
            "    def __init__(self, view_instance):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\agent_management_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot, gui_controller):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\core_messaging_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot, gui_controller):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\onboarding_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot, gui_controller):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\profile_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\utility_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller: \"DiscordGUIController\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\technical_debt_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\technical_debt_base.py",
          "count": 2,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\technical_debt_core.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\debate_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\messaging_monitor_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\messaging_core_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller: \"DiscordGUIController\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\bot_messaging_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\control_panel_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller: \"DiscordGUIController\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\command_base.py",
          "count": 3,
          "lines": [
            "    def __init__(self):",
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller: \"DiscordGUIController\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\example_unified_command.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller: \"DiscordGUIController\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\thea_commands.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller):"
          ]
        },
        {
          "file": "src\\discord_commander\\handlers\\discord_event_handlers.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\lifecycle\\bot_lifecycle.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\integrations\\service_integration_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\config\\bot_config.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\monitor\\resumer_logic.py",
          "count": 2,
          "lines": [
            "    def __init__(self, bot, workspace_path: Path, scheduler=None):",
            "    def __init__(self, data):"
          ]
        },
        {
          "file": "src\\discord_commander\\ui_components\\control_panel_buttons.py",
          "count": 1,
          "lines": [
            "    def __init__(self, view_instance):"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\command_base.py",
          "count": 1,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller: \"DiscordGUIController\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\command_registry.py",
          "count": 2,
          "lines": [
            "    def __init__(self, bot: \"UnifiedDiscordBot\"):",
            "    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller: \"DiscordGUIController\"):"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\unified_command.py",
          "count": 1,
          "lines": [
            "            def __init__(self, bot, gui_controller):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\config.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config_path: str | None = None):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\database.py",
          "count": 1,
          "lines": [
            "    def __init__(self, database_url: str | None = None):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\runner.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\rate_limiter.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: Dict[str, Any]):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\job_queue.py",
          "count": 1,
          "lines": [
            "    def __init__(self, max_workers: int = 4, max_queue_size: int = 1000):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\redactor.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: Dict[str, Any]):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\summarizer.py",
          "count": 1,
          "lines": [
            "    def __init__(self, llm_config: Dict[str, Any]):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\embedding_builder.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: Dict[str, Any]):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\index_builder.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\advanced_reasoning.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: Optional[Dict[str, Any]] = None):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\scrapers\\browser_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self, headless: bool = False, use_undetected: bool = True):"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\scrapers\\scraper_progress.py",
          "count": 1,
          "lines": [
            "    def __init__(self, progress_file: str):"
          ]
        },
        {
          "file": "src\\architecture\\design_patterns.py",
          "count": 5,
          "lines": [
            "            def __init__(self):",
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\architecture\\system_integration.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\architecture\\unified_architecture_core.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\automation\\ui_onboarding.py",
          "count": 1,
          "lines": [
            "    def __init__(self, tolerance: int=3, retries: int=1, dry_run: bool=False):"
          ]
        },
        {
          "file": "src\\core\\unified_logging_system.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\vector_integration_analytics.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\stress_test_analysis_report.py",
          "count": 1,
          "lines": [
            "    def __init__(self, dashboard_data: Optional[dict[str, Any]] = None):"
          ]
        },
        {
          "file": "src\\core\\stress_test_metrics.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: Optional[dict[str, Any]] = None):"
          ]
        },
        {
          "file": "src\\core\\stress_test_metrics_analyzer.py",
          "count": 1,
          "lines": [
            "    def __init__(self, dashboard_data: Optional[dict[str, Any]] = None):"
          ]
        },
        {
          "file": "src\\core\\stress_test_metrics_integration.py",
          "count": 1,
          "lines": [
            "    def __init__(self, collector: Optional[StressTestMetricsCollector] = None):"
          ]
        },
        {
          "file": "src\\core\\task_completion_detector.py",
          "count": 1,
          "lines": [
            "    def __init__(self, timeout_seconds: int = 300):"
          ]
        },
        {
          "file": "src\\core\\activity_detector_models.py",
          "count": 1,
          "lines": [
            "    def __init__(self, tier: int, base_confidence: float):"
          ]
        },
        {
          "file": "src\\core\\activity_emitter.py",
          "count": 1,
          "lines": [
            "    def __init__(self, log_path: Path, discord_sink: Optional[Any] = None):"
          ]
        },
        {
          "file": "src\\core\\agent_activity_tracker.py",
          "count": 1,
          "lines": [
            "    def __init__(self, activity_file: str = \"data/agent_activity.json\"):"
          ]
        },
        {
          "file": "src\\core\\agent_context_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\agent_documentation_service.py",
          "count": 1,
          "lines": [
            "    def __init__(self, agent_id: str = None, vector_db=None, db_path: str = \"vector_db\"):"
          ]
        },
        {
          "file": "src\\core\\agent_lifecycle.py",
          "count": 1,
          "lines": [
            "    def __init__(self, agent_id: str):"
          ]
        },
        {
          "file": "src\\core\\agent_mode_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config_path: Path | str | None = None):"
          ]
        },
        {
          "file": "src\\core\\agent_self_healing_system.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: Optional[SelfHealingConfig] = None):"
          ]
        },
        {
          "file": "src\\core\\auto_gas_pipeline_system.py",
          "count": 1,
          "lines": [
            "    def __init__(self, workspace_root: Optional[Path] = None, monitoring_interval: int = 60):"
          ]
        },
        {
          "file": "src\\core\\coordinator_registry.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\daily_cycle_tracker.py",
          "count": 1,
          "lines": [
            "    def __init__(self, agent_id: str):"
          ]
        },
        {
          "file": "src\\core\\debate_to_gas_integration.py",
          "count": 2,
          "lines": [
            "    def __init__(self):",
            "    def __init__(self, project_root: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\core\\deferred_push_queue.py",
          "count": 1,
          "lines": [
            "    def __init__(self, queue_file: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\core\\end_of_cycle_push.py",
          "count": 1,
          "lines": [
            "    def __init__(self, agent_id: str):"
          ]
        },
        {
          "file": "src\\core\\gasline_integrations.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\hardened_activity_detector.py",
          "count": 1,
          "lines": [
            "    def __init__(self, workspace_root: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\core\\in_memory_message_queue.py",
          "count": 1,
          "lines": [
            "    def __init__(self, max_size: int = 10000):"
          ]
        },
        {
          "file": "src\\core\\local_repo_layer.py",
          "count": 1,
          "lines": [
            "    def __init__(self, base_path: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\core\\merge_conflict_resolver.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\message_queue_error_monitor.py",
          "count": 1,
          "lines": [
            "    def __init__(self, queue_file: Optional[Path] = None, alerts_file: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\core\\message_queue_performance_metrics.py",
          "count": 1,
          "lines": [
            "    def __init__(self, metrics_file: Optional[Path] = None, baseline_file: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\core\\message_queue_statistics.py",
          "count": 1,
          "lines": [
            "    def __init__(self, stats_calculator: QueueStatisticsCalculator):"
          ]
        },
        {
          "file": "src\\core\\messaging_process_lock.py",
          "count": 1,
          "lines": [
            "    def __init__(self, lock_dir: Path | None = None, timeout: int = 30):"
          ]
        },
        {
          "file": "src\\core\\mock_unified_messaging_core.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: Optional[MockDeliveryConfig] = None):"
          ]
        },
        {
          "file": "src\\core\\multi_agent_request_validator.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\multi_agent_responder.py",
          "count": 1,
          "lines": [
            "    def __init__(self, storage_dir: str = \"runtime/multi_agent_responses\"):"
          ]
        },
        {
          "file": "src\\core\\onboarding_service.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\repository_merge_improvements.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\resume_cycle_planner_integration.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\smart_assignment_optimizer.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\unified_import_system.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\coordinate_loader.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\optimized_stall_resume_prompt.py",
          "count": 1,
          "lines": [
            "    def __init__(self, workspace_root: Optional[Path] = None, scheduler=None, auto_claim_tasks: bool = True):"
          ]
        },
        {
          "file": "src\\core\\messaging_coordinate_routing.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\messaging_formatting.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\messaging_clipboard.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\messaging_validation.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\messaging_template_resolution.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\messaging_history.py",
          "count": 1,
          "lines": [
            "    def __init__(self, message_repository: Optional[Any] = None):"
          ]
        },
        {
          "file": "src\\core\\messaging_delivery_orchestration.py",
          "count": 1,
          "lines": [
            "    def __init__(self, delivery_service: Optional[Any] = None):"
          ]
        },
        {
          "file": "src\\core\\message_queue_persistence.py",
          "count": 1,
          "lines": [
            "    def __init__(self, queue_file: Path, lock_manager: Optional[Any] = None):"
          ]
        },
        {
          "file": "src\\core\\messaging_pyautogui.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\unified_service_base.py",
          "count": 1,
          "lines": [
            "    def __init__(self, service: 'UnifiedServiceBase'):"
          ]
        },
        {
          "file": "src\\core\\messaging_pyautogui_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self, pyautogui_module=None):"
          ]
        },
        {
          "file": "src\\core\\project_scanner_integration.py",
          "count": 1,
          "lines": [
            "    def __init__(self, project_root: Optional[Path] = None):"
          ]
        },
        {
          "file": "src\\core\\logging_utils.py",
          "count": 1,
          "lines": [
            "    def __init__(self, logger: logging.Logger, extra: Dict[str, Any]):"
          ]
        },
        {
          "file": "src\\core\\service_base.py",
          "count": 4,
          "lines": [
            "    def __init__(self, config: ServiceConfig):",
            "    def __init__(self, config: ServiceConfig, host: str = \"0.0.0.0\", port: int = 8000):",
            "    print(\"      def __init__(self):\")"
          ]
        },
        {
          "file": "src\\core\\logging_mixin.py",
          "count": 2,
          "lines": [
            "            def __init__(self):",
            "    def __init__(self, logger_name: Optional[str] = None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\coordinators\\analytics_coordinator.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None, intelligence_engine=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\coordinators\\processing_coordinator.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None, processors=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\batch_analytics_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\caching_engine_fixed.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None, max_size: int = 1000):"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\coordination_analytics_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\metrics_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None, metrics_repository: Optional[MetricsRepository] = None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\realtime_analytics_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\anomaly_detection_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\business_intelligence_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\business_intelligence_engine_core.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\business_intelligence_engine_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\predictive_modeling_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\anomaly_detector.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\pattern_extractor.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\trend_analyzer.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\analytics\\orchestrators\\coordination_analytics_orchestrator.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\insight_processor.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction_processor.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction\\prediction_analyzer.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction\\prediction_calculator.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction\\prediction_validator.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\config\\config_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\config\\configuration_factory.py",
          "count": 2,
          "lines": [
            "    def __init__(self):",
            "    def __init__(self, name: str, fields: Dict[str, Dict[str, Any]]):"
          ]
        },
        {
          "file": "src\\core\\consolidation\\utility_consolidation\\utility_consolidation_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config=None):"
          ]
        },
        {
          "file": "src\\core\\consolidation\\utility_consolidation\\utility_consolidation_orchestrator.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: ConsolidationConfig=None):"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\engines\\performance_monitoring_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config):"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\engines\\task_coordination_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config):"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\orchestrators\\swarm_coordination_orchestrator.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: CoordinationConfig | None = None):"
          ]
        },
        {
          "file": "src\\core\\engines\\analysis_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\data_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\integration_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\ml_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\monitoring_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\orchestration_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\performance_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\processing_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\security_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\storage_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\utility_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\validation_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\base_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: Optional[Dict[str, Any]] = None):"
          ]
        },
        {
          "file": "src\\core\\engines\\communication_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\coordination_core_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_base_helpers.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_lifecycle.py",
          "count": 1,
          "lines": [
            "    def __init__(self, engine_id: str):"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_monitoring.py",
          "count": 2,
          "lines": [
            "    def __init__(self, engine_id: str):",
            "    def __init__(self, engine_id: str, metrics: EngineMetrics):"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_state.py",
          "count": 1,
          "lines": [
            "    def __init__(self, engine_id: str, initial_state: EngineState = EngineState.UNINITIALIZED):"
          ]
        },
        {
          "file": "src\\core\\engines\\registry.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_reporting_reporter.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\component_management.py",
          "count": 2,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_classification.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_intelligence.py",
          "count": 1,
          "lines": [
            "    def __init__(self, history_window: int = 1000, analysis_interval: int = 100):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_reporting_core.py",
          "count": 1,
          "lines": [
            "    def __init__(self, component: str, time_range: timedelta = timedelta(hours=24)):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\recovery_strategies.py",
          "count": 8,
          "lines": [
            "    def __init__(self, cleanup_func: Callable):",
            "    def __init__(self, config_reset_func: Callable):",
            "    def __init__(self, degraded_func: Callable):",
            "    def __init__(self, name: str, description: str = \"\"):",
            "    def __init__(self, operation_func: Callable, extended_timeout: float = 60.0):",
            "    def __init__(self, operation_func: Callable, max_retries: int = 3, base_delay: float = 1.0):",
            "    def __init__(self, primary_func: Callable, fallback_func: Callable):",
            "    def __init__(self, service_manager: Callable):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\retry_mechanisms.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: RetryConfig):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\retry_safety_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, logger: logging.Logger | None = None):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_handling.py",
          "count": 1,
          "lines": [
            "    def __init__(self, component_name: str):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\circuit_breaker\\core.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: CircuitBreakerConfig):"
          ]
        },
        {
          "file": "src\\core\\error_handling\\circuit_breaker\\implementation.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: CircuitBreakerConfig):"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_engine_platform.py",
          "count": 1,
          "lines": [
            "    def __init__(self, base_engine, logger=None):"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: LockConfig = None):"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_engine_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self, base_engine, logger=None):"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_manager.py",
          "count": 2,
          "lines": [
            "    def __init__(self, config: LockConfig = None):",
            "    def __init__(self, manager: \"FileLockManager\", filepath: str, metadata: dict[str, Any] = None):"
          ]
        },
        {
          "file": "src\\core\\file_locking\\operations\\lock_queries.py",
          "count": 1,
          "lines": [
            "    def __init__(self, manager: FileLockManager):"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_core.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_mixins_registry.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_mixins_utils.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_registry.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_utilities.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_mixins_core.py",
          "count": 3,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\intelligent_context_emergency.py",
          "count": 1,
          "lines": [
            "    def __init__(self, engine):"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\intelligent_context_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\intelligent_context_search.py",
          "count": 1,
          "lines": [
            "    def __init__(self, engine):"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\core\\context_core.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\engines\\agent_assignment_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, parent_engine):"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\engines\\risk_assessment_engine.py",
          "count": 1,
          "lines": [
            "    def __init__(self, parent_engine):"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\unified_intelligent_context\\search_base.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\unified_intelligent_context\\search_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self, base_search, logger=None):"
          ]
        },
        {
          "file": "src\\core\\managers\\core_execution_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\core_resource_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_metrics.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_state.py",
          "count": 1,
          "lines": [
            "    def __init__(self, manager_type: ManagerType, manager_name: str | None = None):"
          ]
        },
        {
          "file": "src\\core\\managers\\registry.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_context_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_crud_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self, file_ops, lock_ops, context_ops):"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_file_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_lock_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self, lock_file: str = \"runtime/resource_locks.json\"):"
          ]
        },
        {
          "file": "src\\core\\managers\\base_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self, manager_type: ManagerType, manager_name: str = None):"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_coordinator.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_runner.py",
          "count": 1,
          "lines": [
            "    def __init__(self, tasks: dict, executions: dict, execution_threads: dict, task_executor):"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\protocol_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\task_executor.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\base_execution_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_operations.py",
          "count": 1,
          "lines": [
            "    def __init__(self, tasks: dict, task_queue: list):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\alert_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\base_monitoring_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\metric_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_crud.py",
          "count": 1,
          "lines": [
            "    def __init__(self, state: MonitoringState, rules: MonitoringRules | None = None):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_query.py",
          "count": 1,
          "lines": [
            "    def __init__(self, state: MonitoringState):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_rules.py",
          "count": 1,
          "lines": [
            "    def __init__(self, state: MonitoringState):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_state.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\widget_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_lifecycle.py",
          "count": 1,
          "lines": [
            "    def __init__(self, state: MonitoringState):"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\base_results_manager.py",
          "count": 1,
          "lines": [
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\results_processing.py",
          "count": 1,
          "lines": [
            "    def __init__(self, processors: dict, archived: dict, archive_days: int):"
          ]
        },
        {
          "file": "src\\core\\orchestration\\base_orchestrator.py",
          "count": 2,
          "lines": [
            "            def __init__(self, config=None):",
            "    def __init__(self, name: str, config: dict[str, Any] | None = None):"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_components.py",
          "count": 1,
          "lines": [
            "    def __init__(self, orchestrator_name: str):"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_events.py",
          "count": 1,
          "lines": [
            "    def __init__(self, orchestrator_name: str):"
          ]
        },
        {
          "file": "src\\core\\pattern_analysis\\pattern_analysis_orchestrator.py",
          "count": 1,
          "lines": [
            "    def __init__(self, config: PatternAnalysisConfig = None):"
          ]
        },
        {
          "file": "src\\core\\performance\\coordination_performance_monitor.py",
          "count": 3,
          "lines": [
            "            def __init__(self, collector):",
            "            def __init__(self, max_history_size: int = 10000):",
            "    def __init__(self):"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_collector.py",
          "count": 1,
          "lines": [
            "    def __init__(self, max_history_size: int = 10000):"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_monitoring_system.py",
          "count": 1,
          "lines": [
            "    def __init__(self, logger: logging.Logger | None = None):"
          ]
        }
      ],
      "total_count": 337
    },
    "logger_setup": {
      "description": "Logger initialization",
      "regex": "self\\.logger\\s*=\\s*logging\\.getLogger",
      "occurrences": [
        {
          "file": "src\\swarm_pulse\\intelligence_service.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_controller.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_controller.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\bot_runner_service.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\unified_discord_bot.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\core_messaging_commands.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\onboarding_commands.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\profile_commands.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\utility_commands.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\technical_debt_commands.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\messaging_monitor_commands.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\messaging_core_commands.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\bot_messaging_commands.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\command_base.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\handlers\\discord_event_handlers.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\lifecycle\\bot_lifecycle.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\integrations\\service_integration_manager.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\config\\bot_config.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\command_base.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(self.__class__.__name__)"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\command_registry.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\runner.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\redactor.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(\"Redactor\")"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\summarizer.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(\"Summarizer\")"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\embedding_builder.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(\"EmbeddingBuilder\")"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\index_builder.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(\"IndexBuilder\")"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\advanced_reasoning.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\architecture\\design_patterns.py",
          "count": 3,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\architecture\\system_integration.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\architecture\\unified_architecture_core.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\vector_integration_analytics.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\agent_self_healing_system.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\mock_unified_messaging_core.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\onboarding_service.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\messaging_core.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\service_base.py",
          "count": 1,
          "lines": [
            "    print(\"          self.logger = logging.getLogger(__name__)\")"
          ]
        },
        {
          "file": "src\\core\\logging_mixin.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(logger_name)"
          ]
        },
        {
          "file": "src\\core\\config\\config_manager.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\config\\configuration_factory.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\engines\\performance_monitoring_engine.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\engines\\task_coordination_engine.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\orchestrators\\swarm_coordination_orchestrator.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\engines\\base_engine.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(self.__class__.__name__)"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_reporting_reporter.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(__name__)"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_handling.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(component_name)"
          ]
        },
        {
          "file": "src\\core\\orchestration\\base_orchestrator.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(f\"orchestrator.{name}\")"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_components.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(f\"orchestrator.{orchestrator_name}.components\")"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_events.py",
          "count": 1,
          "lines": [
            "        self.logger = logging.getLogger(f\"orchestrator.{orchestrator_name}.events\")"
          ]
        }
      ],
      "total_count": 49
    },
    "import_typing": {
      "description": "Typing imports",
      "regex": "from typing import",
      "occurrences": [
        {
          "file": "src\\commandresult.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\swarmstatus.py",
          "count": 1,
          "lines": [
            "from typing import List, Optional"
          ]
        },
        {
          "file": "src\\agent_registry.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any"
          ]
        },
        {
          "file": "src\\swarm_pulse\\intelligence_service.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Optional, Any"
          ]
        },
        {
          "file": "src\\swarm_pulse\\intelligence_v2.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\approval_commands.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_agent_communication.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_template_collection.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_commands.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\trading_commands.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\trading_data_service.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\contract_notifications.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_controller.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_modals_base.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_commands.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_controller.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\music_commands.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\status_reader.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\systems_inventory_commands.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\test_utils.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\webhook_commands.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_service.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\status_change_monitor.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Optional, List, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\github_book_data.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Any, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_ui_components.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional, Dict, List"
          ]
        },
        {
          "file": "src\\discord_commander\\github_book_commands.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_data.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Any, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_embeds.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\command_base.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any, List"
          ]
        },
        {
          "file": "src\\discord_commander\\embed_factory.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Any, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\music_service.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\status_service.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, Optional, List"
          ]
        },
        {
          "file": "src\\discord_commander\\webhook_service.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Optional, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\inventory_service.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Any, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\bot_runner_service.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\onboarding_modals.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\broadcast_modals.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any, List"
          ]
        },
        {
          "file": "src\\discord_commander\\template_modals.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any, List"
          ]
        },
        {
          "file": "src\\discord_commander\\unified_discord_bot.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_channel_messenger.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\messaging_controller_view.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\status_controller_view.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\swarm_tasks_controller_view.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\utils\\message_chunking.py",
          "count": 1,
          "lines": [
            "from typing import List"
          ]
        },
        {
          "file": "src\\discord_commander\\utils\\carmyn_preferences.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\agent_messaging_view.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\aria_message_agent8_modal.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\aria_profile_view.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_message_agent7_modal.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_message_agent8_modal.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_profile_view.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\swarm_snapshot_view.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\swarm_status_view.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\main_control_panel_view.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any, List"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\utility_commands.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\technical_debt_base.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\debate_commands.py",
          "count": 1,
          "lines": [
            "from typing import Dict"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\messaging_monitor_commands.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\messaging_core_commands.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\bot_messaging_commands.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\system_control_commands.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\control_panel_commands.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\command_base.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, List"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\example_unified_command.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\thea_commands.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\handlers\\discord_event_handlers.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\handlers\\message_processing_helpers.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\lifecycle\\startup_helpers.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\lifecycle\\bot_lifecycle.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\integrations\\service_integration_manager.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\config\\bot_config.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\discord_commander\\monitor\\resumer_logic.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Any"
          ]
        },
        {
          "file": "src\\discord_commander\\ui_components\\control_panel_embeds.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any, List"
          ]
        },
        {
          "file": "src\\discord_commander\\ui_components\\control_panel_buttons.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Any, Dict, List"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\command_base.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, Optional, Any, Dict"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\command_registry.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, List, Dict, Any, Type"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\unified_command.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\config.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\database.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\runner.py",
          "count": 1,
          "lines": [
            "from typing import List, Optional, Dict, Any"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\rate_limiter.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, Optional"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\job_queue.py",
          "count": 1,
          "lines": [
            "from typing import Any, List, Optional, Callable, Dict"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\redactor.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional, Pattern"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\summarizer.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\embedding_builder.py",
          "count": 1,
          "lines": [
            "from typing import List, Dict, Any, Optional"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\index_builder.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\advanced_reasoning.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Any, Optional, Union"
          ]
        },
        {
          "file": "src\\architecture\\design_patterns.py",
          "count": 1,
          "lines": [
            "from typing import Any, Callable, TypeVar, Generic"
          ]
        },
        {
          "file": "src\\architecture\\system_integration.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\architecture\\unified_architecture_core.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\test_categories_config.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\unified_logging_system.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\core\\vector_integration_analytics.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any"
          ]
        },
        {
          "file": "src\\core\\unified_data_processing_system.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\stress_test_analysis_report.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\stress_test_metrics.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\stress_test_metrics_analyzer.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\stress_test_metrics_integration.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\task_completion_detector.py",
          "count": 1,
          "lines": [
            "from typing import Any, Callable, Dict, Optional, Tuple"
          ]
        },
        {
          "file": "src\\core\\activity_detector_helpers.py",
          "count": 1,
          "lines": [
            "from typing import List, Set, Tuple"
          ]
        },
        {
          "file": "src\\core\\activity_detector_models.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional"
          ]
        },
        {
          "file": "src\\core\\activity_emitter.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, Optional"
          ]
        },
        {
          "file": "src\\core\\activity_source_checkers.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Set"
          ]
        },
        {
          "file": "src\\core\\activity_source_checkers_tier2.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Optional, Set"
          ]
        },
        {
          "file": "src\\core\\agent_activity_tracker.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\agent_context_manager.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, Optional"
          ]
        },
        {
          "file": "src\\core\\agent_documentation_service.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\agent_lifecycle.py",
          "count": 1,
          "lines": [
            "from typing import Optional, List, Dict, Any"
          ]
        },
        {
          "file": "src\\core\\agent_mode_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\agent_self_healing_system.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Optional, Any, Tuple"
          ]
        },
        {
          "file": "src\\core\\auto_gas_pipeline_system.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional"
          ]
        },
        {
          "file": "src\\core\\command_execution_wrapper.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, Optional, Tuple"
          ]
        },
        {
          "file": "src\\core\\config_thresholds.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\coordinator_interfaces.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, Optional, Protocol"
          ]
        },
        {
          "file": "src\\core\\coordinator_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\coordinator_registry.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, Optional"
          ]
        },
        {
          "file": "src\\core\\daily_cycle_tracker.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional"
          ]
        },
        {
          "file": "src\\core\\debate_to_gas_integration.py",
          "count": 2,
          "lines": [
            "from typing import Any, Dict, List, Optional",
            "from typing import Dict, List, Optional"
          ]
        },
        {
          "file": "src\\core\\deferred_push_queue.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional"
          ]
        },
        {
          "file": "src\\core\\end_of_cycle_push.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional"
          ]
        },
        {
          "file": "src\\core\\enhanced_activity_status_checker.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Tuple"
          ]
        },
        {
          "file": "src\\core\\gasline_integrations.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Optional"
          ]
        },
        {
          "file": "src\\core\\hardened_activity_detector.py",
          "count": 1,
          "lines": [
            "from typing import List, Optional, Set"
          ]
        },
        {
          "file": "src\\core\\in_memory_message_queue.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional"
          ]
        },
        {
          "file": "src\\core\\keyboard_control_lock.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\core\\local_repo_layer.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, Optional, List, Tuple"
          ]
        },
        {
          "file": "src\\core\\merge_conflict_resolver.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List, Optional, Tuple"
          ]
        },
        {
          "file": "src\\core\\message_queue_error_monitor.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional"
          ]
        },
        {
          "file": "src\\core\\message_queue_helpers.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, Optional"
          ]
        },
        {
          "file": "src\\core\\message_queue_interfaces.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Protocol"
          ]
        },
        {
          "file": "src\\core\\message_queue_performance_metrics.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional"
          ]
        },
        {
          "file": "src\\core\\message_queue_statistics.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List"
          ]
        },
        {
          "file": "src\\core\\messaging_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\messaging_protocol_models.py",
          "count": 1,
          "lines": [
            "from typing import Protocol"
          ]
        },
        {
          "file": "src\\core\\messaging_templates.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\mock_unified_messaging_core.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional, Dict"
          ]
        },
        {
          "file": "src\\core\\multi_agent_request_validator.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\core\\multi_agent_responder.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\onboarding_service.py",
          "count": 1,
          "lines": [
            "from typing import Protocol"
          ]
        },
        {
          "file": "src\\core\\pydantic_config.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\repository_merge_improvements.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Optional, Tuple, List"
          ]
        },
        {
          "file": "src\\core\\resume_cycle_planner_integration.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any"
          ]
        },
        {
          "file": "src\\core\\self_healing_helpers.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Tuple"
          ]
        },
        {
          "file": "src\\core\\self_healing_integration.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\core\\self_healing_operations.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Optional, Tuple"
          ]
        },
        {
          "file": "src\\core\\smart_assignment_optimizer.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List"
          ]
        },
        {
          "file": "src\\core\\stall_resumer_guard.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, Optional, Tuple"
          ]
        },
        {
          "file": "src\\core\\stress_test_runner.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional"
          ]
        },
        {
          "file": "src\\core\\unified_import_system.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Callable, Any, Optional, Union, Tuple, Set"
          ]
        },
        {
          "file": "src\\core\\vector_database.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, Optional"
          ]
        },
        {
          "file": "src\\core\\workspace_agent_registry.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Callable, Any, Optional, Union, Tuple, Set"
          ]
        },
        {
          "file": "src\\core\\coordinate_loader.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\messaging_core.py",
          "count": 1,
          "lines": [
            "from typing import Any, Protocol"
          ]
        },
        {
          "file": "src\\core\\optimized_stall_resume_prompt.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any, List"
          ]
        },
        {
          "file": "src\\core\\messaging_coordinate_routing.py",
          "count": 1,
          "lines": [
            "from typing import Tuple, Optional"
          ]
        },
        {
          "file": "src\\core\\messaging_formatting.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\core\\messaging_clipboard.py",
          "count": 1,
          "lines": [
            "from typing import Optional"
          ]
        },
        {
          "file": "src\\core\\messaging_template_texts.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\messaging_validation.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any, Tuple"
          ]
        },
        {
          "file": "src\\core\\messaging_template_resolution.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any"
          ]
        },
        {
          "file": "src\\core\\messaging_history.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any, List"
          ]
        },
        {
          "file": "src\\core\\messaging_delivery_orchestration.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Any"
          ]
        },
        {
          "file": "src\\core\\message_queue_persistence.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Callable"
          ]
        },
        {
          "file": "src\\core\\messaging_pyautogui.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Callable, Any, Optional, Union, Tuple, Set"
          ]
        },
        {
          "file": "src\\core\\message_queue_registry.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, Optional, Type"
          ]
        },
        {
          "file": "src\\core\\health_check.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, Optional"
          ]
        },
        {
          "file": "src\\core\\unified_service_base.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, Optional, Callable"
          ]
        },
        {
          "file": "src\\core\\message_queue_impl.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Callable"
          ]
        },
        {
          "file": "src\\core\\messaging_pyautogui_operations.py",
          "count": 1,
          "lines": [
            "from typing import Tuple, Optional"
          ]
        },
        {
          "file": "src\\core\\project_scanner_integration.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, Optional"
          ]
        },
        {
          "file": "src\\core\\logging_utils.py",
          "count": 1,
          "lines": [
            "from typing import Optional, Dict, Any"
          ]
        },
        {
          "file": "src\\core\\error_handling.py",
          "count": 1,
          "lines": [
            "from typing import Any, Callable, Dict, Optional, Type, TypeVar, Union"
          ]
        },
        {
          "file": "src\\core\\service_base.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Union"
          ]
        },
        {
          "file": "src\\core\\logging_mixin.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\analytics\\coordinators\\analytics_coordinator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\coordinators\\processing_coordinator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\batch_analytics_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\caching_engine_fixed.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\coordination_analytics_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\metrics_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\realtime_analytics_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\anomaly_detection_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\business_intelligence_engine_core.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\business_intelligence_engine_operations.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\predictive_modeling_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\anomaly_detector.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\pattern_extractor.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\trend_analyzer.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\models\\coordination_analytics_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\orchestrators\\coordination_analytics_orchestrator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\prediction\\base_analyzer.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\insight_processor.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction_processor.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction\\prediction_analyzer.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction\\prediction_calculator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction\\prediction_validator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\config\\config_accessors.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, Any"
          ]
        },
        {
          "file": "src\\core\\config\\config_dataclasses.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\config\\config_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\config\\configuration_factory.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Union"
          ]
        },
        {
          "file": "src\\core\\consolidation\\base.py",
          "count": 1,
          "lines": [
            "from typing import Iterable"
          ]
        },
        {
          "file": "src\\core\\consolidation\\utility_consolidation\\utility_consolidation_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\consolidation\\utility_consolidation\\utility_consolidation_orchestrator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\constants\\fsm_constants.py",
          "count": 1,
          "lines": [
            "from typing import Any, Final"
          ]
        },
        {
          "file": "src\\core\\constants\\fsm_utilities.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\constants\\decision.py",
          "count": 1,
          "lines": [
            "from typing import Final"
          ]
        },
        {
          "file": "src\\core\\constants\\manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\constants\\fsm\\configuration_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\constants\\fsm\\state_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\constants\\fsm\\transition_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\coordination\\agent_strategies.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, List"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\coordination_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\engines\\performance_monitoring_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\engines\\task_coordination_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\orchestrators\\swarm_coordination_orchestrator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\analysis_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\data_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\integration_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\ml_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\monitoring_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\orchestration_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\performance_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\processing_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\security_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\storage_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\utility_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\validation_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\base_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional, Dict"
          ]
        },
        {
          "file": "src\\core\\engines\\communication_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\contracts.py",
          "count": 1,
          "lines": [
            "from typing import Any, Protocol"
          ]
        },
        {
          "file": "src\\core\\engines\\coordination_core_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_base_helpers.py",
          "count": 1,
          "lines": [
            "from typing import Any, Callable"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_lifecycle.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_monitoring.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_state.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional"
          ]
        },
        {
          "file": "src\\core\\engines\\registry.py",
          "count": 2,
          "lines": [
            "from typing import Any, Optional, Type",
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_reporting_reporter.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_reporting_utilities.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_utilities_core.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\component_management.py",
          "count": 2,
          "lines": [
            "from typing import Any",
            "from typing import Any, TypeVar"
          ]
        },
        {
          "file": "src\\core\\error_handling\\coordination_decorator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_classification.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Callable, Any, Optional, Union, Tuple, Set"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_config.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_context_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_decision_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_execution.py",
          "count": 1,
          "lines": [
            "from typing import TypeVar, TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_intelligence.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_models_core.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_reporting_core.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_response_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_response_models_core.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_response_models_specialized.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\recovery_strategies.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Callable, Any, Optional, Union, Tuple, Set"
          ]
        },
        {
          "file": "src\\core\\error_handling\\retry_mechanisms.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\retry_safety_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_handling.py",
          "count": 1,
          "lines": [
            "from typing import Dict, Any, Optional, Callable, List, Type, Union"
          ]
        },
        {
          "file": "src\\core\\error_handling\\unified_error_handler.py",
          "count": 1,
          "lines": [
            "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union"
          ]
        },
        {
          "file": "src\\core\\error_handling\\circuit_breaker\\__init__.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\core\\error_handling\\circuit_breaker\\implementation.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\circuit_breaker\\protocol.py",
          "count": 1,
          "lines": [
            "from typing import Any, Callable, Protocol, TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\core\\error_handling\\circuit_breaker\\provider.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\core\\error_handling\\metrics\\error_alerts.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\error_handling\\metrics\\error_metrics.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_manager.py",
          "count": 2,
          "lines": [
            "from typing import Any",
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\file_locking\\operations\\lock_queries.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\gamification\\achievements.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\gamification\\competition_storage.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\core\\gamification\\leaderboard.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Callable, Any, Optional, Union, Tuple, Set"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_core.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional, Union"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_registry.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_mixins_core.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Callable, Any, Optional, Union, Tuple, Set"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\analysis_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\emergency_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\agent_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\context_results.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\core_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\intelligent_context_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\intelligent_context_optimization_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\metrics.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\metrics_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\mission_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\search_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\core\\context_core.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\engines\\agent_assignment_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\engines\\risk_assessment_engine.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\unified_intelligent_context\\search_base.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\unified_intelligent_context\\models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\base_manager_helpers.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\config_defaults.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\core_execution_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\core_recovery_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\core_resource_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\core_results_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\core_service_coordinator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_lifecycle.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, Any"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_metrics.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_operations.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, Any"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_state.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\registry.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_context_operations.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_crud_operations.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_file_operations.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_lock_operations.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\base_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\contracts.py",
          "count": 1,
          "lines": [
            "from typing import Any, Protocol"
          ]
        },
        {
          "file": "src\\core\\managers\\core_onboarding_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\core_service_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_coordinator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_runner.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\protocol_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\task_executor.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\base_execution_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_operations.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\alert_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\base_monitoring_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\metric_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\metrics_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_crud.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_query.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_rules.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING, Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_state.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\widget_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_lifecycle.py",
          "count": 1,
          "lines": [
            "from typing import TYPE_CHECKING"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\analysis_results_processor.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\base_results_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\results_processing.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\results_query_helpers.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\results_validation.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\validation_results_processor.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\managers\\domains\\execution_domain_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Callable"
          ]
        },
        {
          "file": "src\\core\\managers\\domains\\lifecycle_domain_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict"
          ]
        },
        {
          "file": "src\\core\\managers\\domains\\resource_domain_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional, Union"
          ]
        },
        {
          "file": "src\\core\\managers\\domains\\results_domain_manager.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict"
          ]
        },
        {
          "file": "src\\core\\orchestration\\base_orchestrator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\orchestration\\contracts.py",
          "count": 1,
          "lines": [
            "from typing import Any, Protocol"
          ]
        },
        {
          "file": "src\\core\\orchestration\\core_orchestrator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\orchestration\\integration_orchestrator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_components.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_events.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_lifecycle.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_utilities.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\orchestration\\registry.py",
          "count": 1,
          "lines": [
            "from typing import Dict, List, Callable, Any, Optional, Union, Tuple, Set"
          ]
        },
        {
          "file": "src\\core\\orchestration\\service_orchestrator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\pattern_analysis\\pattern_analysis_orchestrator.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\pattern_analysis\\pattern_analysis_models.py",
          "count": 1,
          "lines": [
            "from typing import Any"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_decorators.py",
          "count": 1,
          "lines": [
            "from typing import Callable"
          ]
        },
        {
          "file": "src\\core\\performance\\coordination_performance_monitor.py",
          "count": 2,
          "lines": [
            "        from typing import Any, Dict, List, Optional",
            "from typing import Dict, Any"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_cli.py",
          "count": 1,
          "lines": [
            "from typing import Dict"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_collector.py",
          "count": 1,
          "lines": [
            "from typing import Any, Dict, List, Optional"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_monitoring_system.py",
          "count": 1,
          "lines": [
            "from typing import Any, Optional"
          ]
        }
      ],
      "total_count": 359
    },
    "try_except": {
      "description": "Exception handling blocks",
      "regex": "try:\\s*$[\\s\\S]*?except.*?:",
      "occurrences": [
        {
          "file": "src\\agent_registry.py",
          "count": 1,
          "lines": [
            "    try:\n        data = json.loads(coord_file.read_text(encoding=\"utf-8\"))\n    except (json.JSONDecodeError, IOError):"
          ]
        },
        {
          "file": "src\\swarm_pulse\\intelligence_service.py",
          "count": 10,
          "lines": [
            "        try:\n            # Analyze event sequences for collaboration patterns\n            if len(events) < 2:\n                return patterns\n\n            # Look for agent interaction patterns\n            agent_sequences = self._extract_agent_sequences(events)\n\n            for sequence in agent_sequences:\n                pattern = self._analyze_sequence_pattern(sequence)\n                if pattern:\n                    patterns.append(pattern)\n\n            return patterns\n\n        except Exception as e:",
            "        try:\n            # Get agent expertise and workload data\n            agent_scores = {}\n            for agent_id in available_agents:\n                score = self._calculate_routing_score(event, agent_id)\n                agent_scores[agent_id] = score\n\n            # Sort agents by score (higher is better)\n            sorted_agents = sorted(\n                agent_scores.keys(),\n                key=lambda x: agent_scores[x],\n                reverse=True\n            )\n\n            self.logger.debug(f\"Intelligent routing for {event.event_type}: {sorted_agents}\")\n            return sorted_agents\n\n        except Exception as e:",
            "        try:\n            # This would analyze actual bottleneck data\n            # For now, return generic bottlenecks\n            if agent_id:\n                bottlenecks = [\n                    f\"Response time for {agent_id}\",\n                    f\"Task queue backlog for {agent_id}\"\n                ]\n            else:\n                bottlenecks = [\n                    \"Inter-agent communication delays\",\n                    \"Task routing inefficiencies\",\n                    \"Resource allocation imbalances\"\n                ]\n\n        except Exception as e:",
            "        try:\n            # This would analyze:\n            # - Response times\n            # - Task completion rates\n            # - Collaboration effectiveness\n            # - Communication patterns\n\n            # Simplified calculation for now\n            base_score = 0.7\n\n            # Agent-specific adjustments\n            adjustments = {\n                \"Agent-1\": 0.1,  # Integration specialist\n                \"Agent-4\": 0.1,  # Captain/coordination\n                \"Agent-6\": 0.1,  # Co-captain\n            }\n\n            return min(base_score + adjustments.get(agent_id, 0), 1.0)\n\n        except Exception as e:",
            "        try:\n            # This would include complex logic based on:\n            # - Agent expertise matching event content\n            # - Current workload\n            # - Historical performance\n            # - Collaboration patterns\n\n            # Simplified scoring for now\n            base_score = 0.5\n\n            # Boost score based on event type preferences\n            if event.event_type == \"coordination\" and agent_id in [\"Agent-6\", \"Agent-4\"]:\n                base_score += 0.3\n            elif event.event_type == \"infrastructure\" and agent_id in [\"Agent-3\", \"Agent-2\"]:\n                base_score += 0.3\n            elif event.event_type == \"development\" and agent_id in [\"Agent-7\", \"Agent-1\"]:\n                base_score += 0.3\n\n            return min(base_score, 1.0)\n\n        except Exception as e:",
            "        try:\n            current_sequence = []\n            for event in events:\n                if hasattr(event, 'agent_id') and event.agent_id:\n                    if event.agent_id not in current_sequence:\n                        current_sequence.append(event.agent_id)\n                else:\n                    if len(current_sequence) > 1:\n                        sequences.append(current_sequence)\n                    current_sequence = []\n\n            # Add final sequence\n            if len(current_sequence) > 1:\n                sequences.append(current_sequence)\n\n        except Exception as e:",
            "        try:\n            from .vectordb.query import embed_events, get_similar_events\n\n            embeddings = embed_events([event])\n            if not embeddings:\n                return []\n\n            similar_events = get_similar_events(embeddings[0], limit=top_k * 2)\n            if not similar_events:\n                return []\n\n            suggestions = []\n            for similar_event, score in similar_events:\n                if score >= threshold:\n                    suggestion = create_suggestion(event, similar_event, score)\n                    if suggestion:\n                        suggestions.append(suggestion)\n\n            # Sort by score and return top_k\n            suggestions.sort(key=lambda x: x.score, reverse=True)\n            return suggestions[:top_k]\n\n        except Exception as e:",
            "        try:\n            if len(sequence) < 2:\n                return None\n\n            # Simple pattern detection\n            pattern_type = \"sequential\" if len(sequence) == 2 else \"multi_agent\"\n\n            return CollaborationPattern(\n                pattern_type=pattern_type,\n                agents_involved=sequence,\n                frequency=1,  # Would be calculated from historical data\n                efficiency_score=0.8,  # Would be calculated based on outcomes\n                description=f\"{'  '.join(sequence)} collaboration pattern\"\n            )\n\n        except Exception as e:",
            "        try:\n            metrics = CoordinationMetrics(\n                agent_id=agent_id or \"all\",\n                coordination_score=0.0,\n                efficiency_rating=\"unknown\",\n                bottlenecks=[],\n                recommendations=[]\n            )\n\n            # Calculate coordination metrics\n            if agent_id:\n                metrics.coordination_score = self._calculate_agent_coordination_score(agent_id)\n            else:\n                # Calculate system-wide coordination\n                all_scores = []\n                for aid in self._get_all_agent_ids():\n                    score = self._calculate_agent_coordination_score(aid)\n                    all_scores.append(score)\n\n                if all_scores:\n                    metrics.coordination_score = sum(all_scores) / len(all_scores)\n\n            # Determine efficiency rating\n            metrics.efficiency_rating = self._calculate_efficiency_rating(metrics.coordination_score)\n\n            # Identify bottlenecks and recommendations\n            metrics.bottlenecks = self._identify_bottlenecks(agent_id)\n            metrics.recommendations = self._generate_recommendations(metrics)\n\n            return metrics\n\n        except Exception as e:",
            "        try:\n            score = metrics.coordination_score\n\n            if score < 0.7:\n                recommendations.extend([\n                    \"Improve inter-agent communication protocols\",\n                    \"Optimize task routing algorithms\",\n                    \"Enhance resource allocation strategies\"\n                ])\n\n            if metrics.efficiency_rating in [\"poor\", \"needs_improvement\"]:\n                recommendations.extend([\n                    \"Implement automated coordination monitoring\",\n                    \"Establish coordination performance baselines\",\n                    \"Develop coordination training programs\"\n                ])\n\n            if not recommendations:\n                recommendations = [\"Maintain current coordination excellence\"]\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_controller_modals.py",
          "count": 3,
          "lines": [
            "        try:\n            # Send message through messaging service\n            success = self.messaging_service.send_message(\n                agent=self.agent_id,  # Fixed: 'agent' not 'agent_id'\n                message=message,\n                priority=priority,\n            )\n\n            if success:\n                embed = discord.Embed(\n                    title=\" Message Sent\",\n                    description=f\"Message sent to **{self.agent_id}**\",\n                    color=discord.Color.green(),\n                    timestamp=datetime.now(),\n                )\n                embed.add_field(name=\"Message\", value=message[:500], inline=False)\n                embed.add_field(name=\"Priority\", value=priority, inline=True)\n                embed.add_field(name=\"From\", value=\"Discord User\", inline=True)\n\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n            else:\n                embed = discord.Embed(\n                    title=\" Message Failed\",\n                    description=f\"Failed to send message to **{self.agent_id}**\",\n                    color=discord.Color.red(),\n                    timestamp=datetime.now(),\n                )\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n\n        except Exception as e:",
            "        try:\n            success = self.messaging_service.broadcast_message(\n                message=message, from_agent=\"Discord-User\", priority=priority\n            )\n\n            if success:\n                embed = discord.Embed(\n                    title=\" Broadcast Sent\",\n                    description=\"Message broadcasted to all agents\",\n                    color=discord.Color.green(),\n                    timestamp=datetime.now(),\n                )\n                embed.add_field(name=\"Message\", value=message[:500], inline=False)\n                embed.add_field(name=\"Priority\", value=priority, inline=True)\n\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n            else:\n                embed = discord.Embed(\n                    title=\" Broadcast Failed\",\n                    description=\"Failed to broadcast message\",\n                    color=discord.Color.red(),\n                    timestamp=datetime.now(),\n                )\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n\n        except Exception as e:",
            "try:\n    import discord\n\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\approval_commands.py",
          "count": 2,
          "lines": [
            "        try:\n            approval_path.parent.mkdir(parents=True, exist_ok=True)\n            with open(approval_path, 'w', encoding='utf-8') as f:\n                f.write(approval_content)\n\n            embed = discord.Embed(\n                title=\" Phase 1 Approval Confirmed!\",\n                description=\"**Approval received and recorded**\",\n                color=discord.Color.green(),\n            )\n\n            embed.add_field(\n                name=\"Approval Recorded\",\n                value=f\"Approval saved to: `{approval_path}`\",\n                inline=False\n            )\n\n            embed.add_field(\n                name=\"Next Steps\",\n                value=\"1. Agent-1 will run dry-run tests\\n2. Review dry-run results\\n3. Execute Batch 1 (case variations)\\n4. Monitor progress\",\n                inline=False\n            )\n\n            await ctx.send(embed=embed)\n\n            # Notify agents\n            logger.info(f\" Phase 1 approval received from {ctx.author.display_name}\")\n\n        except Exception as e:",
            "        try:\n            plan_path = self.project_root / \"docs/organization/PHASE1_DETAILED_APPROVAL_EXPLANATION.md\"\n            \n            if not plan_path.exists():\n                await ctx.send(\" Approval plan document not found!\")\n                return\n\n            # Read the plan\n            with open(plan_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n\n            # Create embed with summary\n            embed = discord.Embed(\n                title=\" Phase 1 Consolidation - Approval Plan\",\n                description=\"**What You're Signing Off On**\",\n                color=discord.Color.blue(),\n            )\n\n            # Add key information\n            embed.add_field(\n                name=\" The Numbers\",\n                value=\"**Current**: 75 repos\\n**After Phase 1**: 49 repos\\n**Reduction**: 26 repos (35% reduction)\\n\\n **Note**: fastapi (external library) will be skipped - keep both as dependencies\",\n                inline=False\n            )\n\n            embed.add_field(\n                name=\" What Will Happen\",\n                value=\"1. **Case Variations** (12 repos) -  ZERO RISK\\n2. **Vision Attempts** (4 repos) -  LOW RISK\\n3. **Dream Projects** (2 repos) -  MEDIUM RISK\\n4. **Trading Repos** (3 repos) -  MEDIUM RISK\\n5. **Agent Systems** (2 repos) -  MEDIUM RISK\\n6. **Streaming Tools** (1 repo) -  LOW RISK\\n7. **DaDudekC Projects** (1 repo) -  LOW RISK\\n8. **ML Models** (1 repo) -  LOW RISK\\n9. **Resume/Templates** (1 repo) -  LOW RISK\",\n                inline=False\n            )\n\n            embed.add_field(\n                name=\" Critical Protections\",\n                value=\" AutoDream_Os (repo 7) - **PROTECTED** (your current project)\\n prompt-library (repo 11) - **PROTECTED** (messaging protocol)\\n prompts (repo 65) - **PROTECTED** (messaging protocol)\\n All merged repos will be **ARCHIVED** (not deleted)\",\n                inline=False\n            )\n\n            embed.add_field(\n                name=\" Safety Measures\",\n                value=\" Backups before every merge\\n Dry-run mode testing\\n Rollback capability\\n Batch execution (start with safest)\",\n                inline=False\n            )\n\n            embed.add_field(\n                name=\" Full Details\",\n                value=f\"See: `docs/organization/PHASE1_DETAILED_APPROVAL_EXPLANATION.md`\\n\\n**File Location**: `{plan_path}`\",\n                inline=False\n            )\n\n            embed.set_footer(text=\"Review the detailed document before approving\")\n\n            await ctx.send(embed=embed)\n\n            # Also send a file attachment if possible\n            try:\n                await ctx.send(\n                    file=discord.File(plan_path),\n                    content=\" **Full Detailed Approval Explanation** - Review this document carefully!\"\n                )\n            except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_agent_communication.py",
          "count": 8,
          "lines": [
            "        try:\n            agents = [f\"Agent-{i}\" for i in range(1, 9)]\n            successful_deliveries = 0\n            failed_deliveries = []\n\n            for agent in agents:\n                result = await self.send_to_agent_inbox(agent, message, sender)\n                if result.success:\n                    successful_deliveries += 1\n                else:\n                    failed_deliveries.append(f\"{agent}: {result.message}\")\n\n            if successful_deliveries == len(agents):\n                return create_command_result(\n                    success=True,\n                    message=f\"Broadcast successfully delivered to all {len(agents)} agents\",\n                    data={\n                        \"successful_deliveries\": successful_deliveries,\n                        \"total_agents\": len(agents),\n                    },\n                )\n            else:\n                return create_command_result(\n                    success=False,\n                    message=f\"Broadcast partially failed: {successful_deliveries}/{len(agents)} delivered\",\n                    data={\n                        \"successful_deliveries\": successful_deliveries,\n                        \"failed_deliveries\": failed_deliveries,\n                    },\n                )\n\n        except Exception as e:",
            "        try:\n            from src.utils.inbox_utility import create_inbox_message\n            \n            # Use inbox utility for file creation (separate from messaging system)\n            success = create_inbox_message(\n                recipient=agent,\n                sender=f\"{sender} (via Discord Commander)\",\n                content=message,\n                priority=\"urgent\",\n                message_type=\"text\",\n                tags=[\"discord\", \"captain\"]\n            )\n            \n            if success:\n                # Get the created file path for response\n                inbox_dir = Path(\"agent_workspaces\") / agent / \"inbox\"\n                message_files = sorted(inbox_dir.glob(\"INBOX_MESSAGE_*.md\"), key=lambda p: p.stat().st_mtime, reverse=True)\n                latest_file = message_files[0] if message_files else None\n                \n                self.logger.info(f\"Message sent to {agent}'s inbox via utility\")\n                \n                return create_command_result(\n                    success=True,\n                    message=f\"Message successfully delivered to {agent}'s inbox\",\n                    data={\"filename\": latest_file.name if latest_file else \"unknown\", \"path\": str(latest_file) if latest_file else \"unknown\"},\n                    agent=agent,\n                )\n            else:\n                raise Exception(\"Inbox utility returned False\")\n                \n        except Exception as e:",
            "        try:\n            inbox_path = self._utility.path.join(os.getcwd(), \"agent_workspaces\", agent, \"inbox\")\n\n            if not self._utility.path.exists(inbox_path):\n                return 0\n\n            cleaned_count = 0\n            current_time = datetime.utcnow().timestamp()\n            max_age_seconds = max_age_hours * 3600\n\n            for filename in os.listdir(inbox_path):\n                if filename.endswith(\".md\"):\n                    file_path = self._utility.path.join(inbox_path, filename)\n                    file_age = current_time - os.path.getmtime(file_path)\n\n                    if file_age > max_age_seconds:\n                        os.remove(file_path)\n                        cleaned_count += 1\n\n            self.logger.info(f\"Cleaned up {cleaned_count} old messages from {agent}'s inbox\")\n            return cleaned_count\n\n        except Exception as e:",
            "        try:\n            return await self.send_to_agent_inbox(\"Agent-4\", prompt, sender)\n        except Exception as e:",
            "        try:\n            self.logger.info(f\"Executing command on {agent}: {command}\")\n            await asyncio.sleep(1)  # Simulate command execution\n\n            execution_time = asyncio.get_event_loop().time() - start_time\n\n            return create_command_result(\n                success=True,\n                message=f\"Command executed successfully on {agent}\",\n                execution_time=execution_time,\n                agent=agent,\n            )\n\n        except Exception as e:",
            "        try:\n            status_file = self.get_agent_status_file_path(agent)\n\n            if self._utility.path.exists(status_file):\n                with open(status_file) as f:\n                    return json.load(f)\n            else:\n                self.logger.warning(f\"Status file not found for {agent}\")\n                return None\n\n        except Exception as e:",
            "try:\n    from .discord_models import CommandResult, create_command_result\nexcept ImportError:",
            "try:\n    from src.utils.unified_utilities import get_unified_utility\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_template_collection.py",
          "count": 1,
          "lines": [
            "try:\n    from .templates.broadcast_templates import ENHANCED_BROADCAST_TEMPLATES\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_commands.py",
          "count": 7,
          "lines": [
            "        try:\n            embed = self.embed_factory.create_excellence_embed()\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "        try:\n            embed = self.embed_factory.create_overview_embed()\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "        try:\n            embed = self.embed_factory.create_profile_embed(agent_id)\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "        try:\n            embed = self.embed_factory.create_roadmap_embed()\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "        try:\n            embed = self.embed_factory.create_tasks_embed()\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "        try:\n            self.data_loader.refresh_data()\n            await ctx.send(\" Swarm showcase data has been refreshed!\")\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\trading_commands.py",
          "count": 3,
          "lines": [
            "            try:\n                analysis = self.trading_service.analyze_symbol(symbol)\n                price = analysis.get(\"price\")\n                confidence = analysis.get(\"confidence\", 0.0)\n\n                if price and confidence > 0.3:\n                    setups.append({\n                        \"symbol\": symbol,\n                        \"setup_type\": setup_type,\n                        \"signal\": analysis.get(\"signal\", \"UNKNOWN\"),\n                        \"entry\": price * 0.99,  # Approximate entry\n                        \"exit\": price * 1.02,   # Approximate exit\n                        \"confidence\": confidence,\n                        \"notes\": analysis.get(\"reason\", \"Analysis complete\")\n                    })\n            except Exception as e:",
            "        try:\n            # Show thinking message\n            await ctx.send(\" Analyzing market data and generating report...\")\n\n            # Generate report\n            report = self._generate_trading_report()\n\n            # Create embed\n            embed = self._create_trading_report_embed(report)\n\n            await ctx.send(embed=embed)\n            logger.info(f\" Trading report generated for {ctx.author}\")\n\n        except Exception as e:",
            "        try:\n            analysis = self.trading_service.analyze_symbol(\"TSLA\")\n\n            # Determine call/put day\n            signal = analysis.get(\"signal\", \"UNKNOWN\")\n            confidence = analysis.get(\"confidence\", 0.0)\n            price = analysis.get(\"price\")\n\n            if signal == \"CALL\":\n                day_type = \"CALL DAY\"\n                reasoning = analysis.get(\"reason\", \"Bullish signals detected\")\n            elif signal == \"PUT\":\n                day_type = \"PUT DAY\"\n                reasoning = analysis.get(\"reason\", \"Bearish signals detected\")\n            else:\n                day_type = \"NEUTRAL\"\n                reasoning = \"Mixed or unclear signals\"\n\n            return {\n                \"day_type\": day_type,\n                \"confidence\": confidence,\n                \"reasoning\": reasoning,\n                \"price\": price,\n                \"timestamp\": datetime.now().isoformat()\n            }\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\trading_data_service.py",
          "count": 11,
          "lines": [
            "                try:\n                    self.strategy_manager = StrategyManager()\n                    self.strategy_manager.add_strategy(TrendFollowingStrategy())\n                    self.strategy_manager.add_strategy(MeanReversionStrategy())\n                    logger.info(\" Strategy manager initialized\")\n                except Exception as e:",
            "            try:\n                close_prices = spy_data['close'] if 'close' in spy_data.columns else spy_data['Close']\n                if len(close_prices) >= 2:\n                    current = close_prices.iloc[-1]\n                    previous = close_prices.iloc[-2]\n                    conditions['spy_trend'] = \"BULLISH\" if current > previous else \"BEARISH\"\n            except:",
            "            try:\n                close_prices = vix_data['close'] if 'close' in vix_data.columns else vix_data['Close']\n                conditions['vix'] = f\"{close_prices.iloc[-1]:.2f}\"\n            except:",
            "            try:\n                data = self.broker_client.get_historical_data(\n                    symbol=symbol,\n                    timeframe=interval,\n                    limit=100\n                )\n                if not data.empty:\n                    return data\n            except Exception as e:",
            "            try:\n                results = self.strategy_manager.analyze_symbol(symbol, data)\n                \n                # Get consensus signal\n                if results:\n                    buy_signals = sum(1 for r in results if r.signal.name == \"BUY\")\n                    sell_signals = sum(1 for r in results if r.signal.name == \"SELL\")\n                    avg_confidence = sum(r.confidence for r in results) / len(results) if results else 0.0\n                    \n                    if buy_signals > sell_signals:\n                        signal = \"CALL\" if symbol == \"TSLA\" else \"LONG\"\n                        reason = f\"{buy_signals} strategies bullish\"\n                    elif sell_signals > buy_signals:\n                        signal = \"PUT\" if symbol == \"TSLA\" else \"SHORT\"\n                        reason = f\"{sell_signals} strategies bearish\"\n                    else:\n                        signal = \"HOLD\"\n                        reason = \"Mixed signals\"\n                    \n                    return {\n                        \"symbol\": symbol,\n                        \"signal\": signal,\n                        \"confidence\": avg_confidence,\n                        \"reason\": reason,\n                        \"price\": current_price,\n                        \"indicators\": self._calculate_indicators(data),\n                        \"strategy_results\": len(results)\n                    }\n            except Exception as e:",
            "            try:\n                self.broker_client.connect()\n                logger.info(\" Connected to broker API\")\n            except Exception as e:",
            "            try:\n                ticker = yf.Ticker(symbol)\n                data = ticker.history(period=period, interval=interval)\n                if not data.empty:\n                    # Rename columns to match expected format\n                    data.columns = [col.lower() for col in data.columns]\n                    return data\n            except Exception as e:",
            "        try:\n            # Try to import config (may fail due to Pydantic validation)\n            try:\n                from trading_robot.config.settings import config as trading_config\n            except Exception as e:",
            "        try:\n            close_prices = data['close'] if 'close' in data.columns else data['Close']\n            \n            # Simple moving averages\n            if len(close_prices) >= 20:\n                indicators['sma_20'] = float(close_prices.tail(20).mean())\n            if len(close_prices) >= 50:\n                indicators['sma_50'] = float(close_prices.tail(50).mean())\n            \n            # Current price\n            indicators['current'] = float(close_prices.iloc[-1])\n            \n            # Price change\n            if len(close_prices) >= 2:\n                change = close_prices.iloc[-1] - close_prices.iloc[-2]\n                change_pct = (change / close_prices.iloc[-2]) * 100\n                indicators['change'] = float(change)\n                indicators['change_pct'] = float(change_pct)\n            \n        except Exception as e:",
            "try:\n    from trading_robot.core.broker_factory import BrokerFactory\n    from trading_robot.core.broker_interface import BrokerInterface\n    from trading_robot.strategies.base_strategy import StrategyManager\n    from trading_robot.strategies.strategy_implementations import TrendFollowingStrategy, MeanReversionStrategy\n    TRADING_ROBOT_AVAILABLE = True\n    logger.info(\" Trading robot modules available\")\nexcept (ImportError, Exception) as e:",
            "try:\n    import yfinance as yf\n    YFINANCE_AVAILABLE = True\n    logger.info(\" yfinance available\")\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\contract_notifications.py",
          "count": 4,
          "lines": [
            "        try:\n            response = requests.post(self.webhook_url, json=payload, timeout=TimeoutConstants.HTTP_SHORT)\n            return response.status_code == 204\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_controller.py",
          "count": 3,
          "lines": [
            "        try:\n            agents = [f\"Agent-{i}\" for i in range(1, 9)]\n            success_count = 0\n            sender = (\n                f\"Discord User ({getattr(discord_user, 'name', '')})\"\n                if discord_user\n                else \"Discord GUI\"\n            )\n            discord_user_id = str(getattr(discord_user, \"id\", \"\")) if discord_user else None\n\n            # Delay between agents to prevent race conditions and routing problems\n            # Queue operations need time to process and prevent conflicts\n            INTER_AGENT_QUEUE_DELAY = 0.5  # 500ms delay between queue operations\n\n            for agent in agents:\n                result = self.messaging_service.send_message(\n                    agent=agent, \n                    message=message, \n                    priority=priority, \n                    use_pyautogui=True,\n                    wait_for_delivery=False,  # Don't wait - queue processor will handle delivery\n                    discord_user_id=discord_user_id,\n                    apply_template=True,\n                    message_category=MessageCategory.D2A,\n                    sender=sender,\n                )\n                if result.get(\"success\"):\n                    success_count += 1\n                \n                # Add delay between agents to prevent race conditions in queue routing\n                if agent != agents[-1]:  # Don't delay after the last agent\n                    await asyncio.sleep(INTER_AGENT_QUEUE_DELAY)\n\n            return success_count == len(agents)\n        except Exception as e:",
            "        try:\n            sender = (\n                f\"Discord User ({getattr(discord_user, 'name', '')})\"\n                if discord_user\n                else \"Discord GUI\"\n            )\n            discord_user_id = str(getattr(discord_user, \"id\", \"\")) if discord_user else None\n\n            result = self.messaging_service.send_message(\n                agent=agent_id, \n                message=message, \n                priority=priority, \n                use_pyautogui=True,\n                wait_for_delivery=False,  # Don't wait - queue processor will handle delivery\n                stalled=stalled,\n                discord_user_id=discord_user_id,\n                apply_template=True,\n                message_category=MessageCategory.D2A,\n                sender=sender,\n            )\n            return result.get(\"success\", False)\n        except Exception as e:",
            "        try:\n            status_reader = StatusReader()\n            agents_status = {}\n\n            for i in range(1, 9):\n                agent_id = f\"Agent-{i}\"\n                status = status_reader.get_agent_status(agent_id)\n                if status:\n                    agents_status[agent_id] = status\n\n            return agents_status\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_modals.py",
          "count": 1,
          "lines": [
            "try:\n    import discord\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_gui_modals_base.py",
          "count": 4,
          "lines": [
            "        try:\n            await interaction.response.send_message(\n                f\" An error occurred: {str(error)}\", ephemeral=True\n            )\n        except discord.InteractionResponded:",
            "        try:\n            diagram_code = self.message_input.value\n\n            # Basic validation\n            if not diagram_code.strip():\n                await interaction.response.send_message(\" Please enter Mermaid diagram code.\", ephemeral=True)\n                return\n\n            # For now, just acknowledge - full Mermaid rendering would need additional setup\n            await interaction.response.send_message(\n                f\" Mermaid diagram submitted!\\n```\\n{diagram_code[:500]}{'...' if len(diagram_code) > 500 else ''}\\n```\",\n                ephemeral=True\n            )\n\n        except Exception as e:",
            "        try:\n            message = self.message_input.value\n            priority = getattr(self, 'priority_input', None)\n            priority = priority.value if priority else \"regular\"\n\n            preview = self._get_message_preview(message)\n\n            if message_type == \"agent\":\n                result = self._send_to_agent(target, message, priority, discord_user=interaction.user)\n                if result.get(\"success\"):\n                    await interaction.response.send_message(\n                        f\" Message sent to {target}!\\n Preview: {preview}\", ephemeral=True\n                    )\n                else:\n                    await interaction.response.send_message(\n                        f\" Failed to send message to {target}: {result.get('error', 'Unknown error')}\",\n                        ephemeral=True\n                    )\n            elif message_type == \"broadcast\":\n                agents = self._get_all_agent_ids()\n                success_count, errors = self._broadcast_to_agents(agents, message, priority, discord_user=interaction.user)\n                if success_count > 0:\n                    await interaction.response.send_message(\n                        f\" Broadcast sent to {success_count} agents!\\n Preview: {preview}\", ephemeral=True\n                    )\n                else:\n                    await interaction.response.send_message(\n                        f\" Broadcast failed: {self._format_error_message(errors)}\", ephemeral=True\n                    )\n            elif message_type == \"jetfuel\":\n                # Jet Fuel specific handling\n                result = self._send_jet_fuel_message(target, message, priority, discord_user=interaction.user)\n                if result.get(\"success\"):\n                    await interaction.response.send_message(\n                        f\" Jet Fuel coordination sent!\\n Preview: {preview}\", ephemeral=True\n                    )\n                else:\n                    await interaction.response.send_message(\n                        f\" Jet Fuel coordination failed: {result.get('error', 'Unknown error')}\",\n                        ephemeral=True\n                    )\n\n        except Exception as e:",
            "try:\n    import discord\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\file_share_commands.py",
          "count": 2,
          "lines": [
            "            try:\n                clean_path = relative_path.strip().lstrip(\"/\").replace(\"\\\\\", \"/\")\n                target_path = (self.project_root / clean_path).resolve()\n\n                # Safety: ensure path is inside repo\n                if not str(target_path).startswith(str(self.project_root)):\n                    await ctx.send(\" Path is outside the repository.\")\n                    return\n\n                if not target_path.exists() or not target_path.is_file():\n                    await ctx.send(f\" File not found: `{clean_path}`\")\n                    return\n\n                size = target_path.stat().st_size\n                if size > self.max_bytes:\n                    await ctx.send(\n                        f\" File too large for Discord upload ({size/1_000_000:.2f} MB > 7.5 MB).\\n\"\n                        f\"Path: `{clean_path}`\"\n                    )\n                    return\n\n                await ctx.send(\n                    content=f\" `{clean_path}`\",\n                    file=discord.File(target_path),\n                )\n            except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_commands.py",
          "count": 1,
          "lines": [
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\messaging_controller.py",
          "count": 4,
          "lines": [
            "        try:\n            # Validate agent name is in allowed list (Agent-1 through Agent-8)\n            from src.discord_commander.discord_agent_communication import AgentCommunicationEngine\n            engine = AgentCommunicationEngine()\n            if not engine.is_valid_agent(agent_id):\n                self.logger.warning(f\"Invalid agent name: {agent_id} (must be Agent-1 through Agent-8)\")\n                return False\n\n            return self.messaging_service.send_message(\n                agent=agent_id,  # Fixed: 'agent' not 'agent_id'\n                message=message,\n                priority=priority,\n            )\n        except Exception as e:",
            "        try:\n            if hasattr(self.messaging_service, \"agent_data\"):\n                return {\n                    agent_id: {\n                        \"active\": agent_info.get(\"active\", False),\n                        \"coordinates\": agent_info.get(\"coordinates\", (0, 0)),\n                        \"name\": agent_info.get(\"name\", agent_id),\n                    }\n                    for agent_id, agent_info in self.messaging_service.agent_data.items()\n                }\n        except Exception as e:",
            "        try:\n            return self.messaging_service.broadcast_message(\n                message=message, from_agent=\"Discord-Controller\", priority=priority\n            )\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\music_commands.py",
          "count": 7,
          "lines": [
            "        try:\n            # Join voice channel\n            if not await self.music_service.join_voice_channel(ctx):\n                await status_msg.edit(embed=discord.Embed(\n                    title=\" Cannot Play Music\",\n                    description=\"Unable to join your voice channel!\",\n                    color=discord.Color.red()\n                ))\n                return\n\n            # Download audio\n            audio_path = await self.music_service.download_youtube_audio(song_title)\n\n            if not audio_path:\n                await status_msg.edit(embed=discord.Embed(\n                    title=\" Download Failed\",\n                    description=\"Could not download the requested song. Please try a different title or URL.\",\n                    color=discord.Color.red()\n                ))\n                return\n\n            # Play music\n            if await self.music_service.play_music(ctx, audio_path):\n                embed = discord.Embed(\n                    title=\" Now Playing\",\n                    description=f\"**{audio_path.stem}**\",\n                    color=discord.Color.green()\n                )\n                embed.add_field(\n                    name=\"Requested by\",\n                    value=ctx.author.display_name,\n                    inline=True\n                )\n                embed.add_field(\n                    name=\"Voice Channel\",\n                    value=ctx.author.voice.channel.name if ctx.author.voice else \"Unknown\",\n                    inline=True\n                )\n                await status_msg.edit(embed=embed)\n            else:\n                await status_msg.edit(embed=discord.Embed(\n                    title=\" Playback Failed\",\n                    description=\"Could not start music playback.\",\n                    color=discord.Color.red()\n                ))\n\n        except Exception as e:",
            "        try:\n            # Only process for our bot\n            if member.id != self.bot.user.id:\n                return\n\n            # Check if bot was disconnected or moved\n            if before.channel is not None and after.channel is None:\n                # Bot was disconnected\n                self.music_service.current_voice_client = None\n                self.music_service.current_song = None\n                logger.info(\"Bot disconnected from voice channel\")\n\n            elif before.channel is not None and after.channel is not None and before.channel != after.channel:\n                # Bot was moved to different channel\n                logger.info(f\"Bot moved from {before.channel.name} to {after.channel.name}\")\n\n        except Exception as e:",
            "        try:\n            if await self.music_service.disconnect_voice(ctx):\n                embed = discord.Embed(\n                    title=\" Disconnected\",\n                    description=\"Left the voice channel.\",\n                    color=discord.Color.blue()\n                )\n                await ctx.send(embed=embed)\n            else:\n                embed = discord.Embed(\n                    title=\" Not Connected\",\n                    description=\"Not currently connected to a voice channel.\",\n                    color=discord.Color.blue()\n                )\n                await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            if await self.music_service.stop_music(ctx):\n                embed = discord.Embed(\n                    title=\" Music Stopped\",\n                    description=\"Playback has been stopped.\",\n                    color=discord.Color.orange()\n                )\n                await ctx.send(embed=embed)\n            else:\n                embed = discord.Embed(\n                    title=\" No Music Playing\",\n                    description=\"There is no music currently playing.\",\n                    color=discord.Color.blue()\n                )\n                await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            removed_count = self.music_service.cleanup_cache()\n\n            embed = discord.Embed(\n                title=\" Music Cache Cleaned\",\n                description=f\"Removed {removed_count} old audio files from cache.\",\n                color=discord.Color.green()\n            )\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            song_info = self.music_service.get_current_song_info()\n\n            if song_info:\n                embed = discord.Embed(\n                    title=\" Now Playing\",\n                    description=f\"**{song_info['title']}**\",\n                    color=discord.Color.green()\n                )\n                embed.add_field(\n                    name=\"Voice Channel\",\n                    value=ctx.author.voice.channel.name if ctx.author.voice else \"Unknown\",\n                    inline=True\n                )\n                await ctx.send(embed=embed)\n            else:\n                embed = discord.Embed(\n                    title=\" No Music Playing\",\n                    description=\"There is no music currently playing.\",\n                    color=discord.Color.blue()\n                )\n                await ctx.send(embed=embed)\n\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\status_reader.py",
          "count": 4,
          "lines": [
            "        try:\n            await self.status_service.refresh_cache()\n            embed = discord.Embed(\n                title=\" Status Cache Refreshed\",\n                description=\"Agent status cache has been updated with latest information.\",\n                color=discord.Color.green()\n            )\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "        try:\n            embed = await self.status_service.get_status_embed(agent_id)\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "        try:\n            embed = await self.status_service.get_status_summary_embed()\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\systems_inventory_commands.py",
          "count": 5,
          "lines": [
            "        try:\n            # Get inventory stats\n            stats = self.inventory_service.get_inventory_stats()\n\n            embed = discord.Embed(\n                title=\" Systems Inventory - Complete Overview\",\n                description=\"**Agent Cellphone V2 - Complete Systems Inventory**\",\n                color=discord.Color.blue()\n            )\n\n            # Statistics fields\n            embed.add_field(\n                name=\" Statistics\",\n                value=f\"**Systems:** {stats['total_systems']} ({stats['active_systems']} active)\\n\"\n                      f\"**Tools:** {stats['total_tools']}\\n\"\n                      f\"**Services:** {stats['total_services']} ({stats['active_services']} running)\",\n                inline=True\n            )\n\n            embed.add_field(\n                name=\" Version Info\",\n                value=f\"**Version:** {stats['version']}\\n\"\n                      f\"**Updated:** {stats['last_updated'][:10] if stats['last_updated'] else 'Unknown'}\",\n                inline=True\n            )\n\n            # Systems overview\n            systems_overview = self.inventory_service.format_systems_overview()\n            if len(systems_overview) > 1024:\n                systems_overview = systems_overview[:1020] + \"...\"\n\n            embed.add_field(\n                name=\" Systems Overview\",\n                value=systems_overview or \"No systems found\",\n                inline=False\n            )\n\n            # Quick actions\n            embed.add_field(\n                name=\" Quick Access\",\n                value=\" `!systems_list` - Detailed systems list\\n\"\n                      \" `!tools_list` - All tools\\n\"\n                      \" `!services_list` - All services\",\n                inline=False\n            )\n\n            embed.set_footer(text=f\"Requested by {ctx.author.display_name} | Use specific commands for detailed views\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            # Validate limit\n            if limit < 1 or limit > 100:\n                limit = 20\n\n            tools_list = self.inventory_service.format_tools_list(limit)\n\n            embed = discord.Embed(\n                title=f\" Tools Inventory ({len(self.inventory_service.get_tools_list()) if limit >= len(self.inventory_service.get_tools_list()) else f'{limit}+'})\",\n                description=\"List of all available tools and utilities\",\n                color=discord.Color.purple()\n            )\n\n            # Split into chunks if too long for Discord\n            if len(tools_list) > 4000:\n                chunks = [tools_list[i:i+4000] for i in range(0, len(tools_list), 4000)]\n                embed.add_field(\n                    name=\" Tools (Part 1)\",\n                    value=chunks[0],\n                    inline=False\n                )\n                for i, chunk in enumerate(chunks[1:], 2):\n                    embed.add_field(\n                        name=f\" Tools (Part {i})\",\n                        value=chunk,\n                        inline=False\n                    )\n            else:\n                embed.add_field(\n                    name=\" Available Tools\",\n                    value=tools_list or \"No tools found in inventory\",\n                    inline=False\n                )\n\n            embed.set_footer(text=f\"Showing up to {limit} tools | Use higher limit to see more\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            services_list = self.inventory_service.format_services_list()\n\n            embed = discord.Embed(\n                title=f\" Services Inventory ({len(self.inventory_service.get_services_list())} services)\",\n                description=\"List of all available services and their status\",\n                color=discord.Color.teal()\n            )\n\n            if len(services_list) > 4000:\n                # Split into chunks\n                chunks = [services_list[i:i+4000] for i in range(0, len(services_list), 4000)]\n                for i, chunk in enumerate(chunks, 1):\n                    embed.add_field(\n                        name=f\" Services (Part {i})\",\n                        value=chunk,\n                        inline=False\n                    )\n            else:\n                embed.add_field(\n                    name=\" Available Services\",\n                    value=services_list or \"No services found in inventory\",\n                    inline=False\n                )\n\n            embed.set_footer(text=f\"Requested by {ctx.author.display_name} | Status indicators:  Running,  Stopped,  Error\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            systems = self.inventory_service.get_systems_list()\n\n            if not systems:\n                embed = discord.Embed(\n                    title=\" Systems List\",\n                    description=\"No systems found in inventory\",\n                    color=discord.Color.orange()\n                )\n                await ctx.send(embed=embed)\n                return\n\n            embed = discord.Embed(\n                title=f\" Systems List ({len(systems)} systems)\",\n                description=\"Detailed list of all systems in the inventory\",\n                color=discord.Color.green()\n            )\n\n            # Group systems by category for better organization\n            categories = {}\n            for system in systems:\n                category = system.get(\"category\", \"Uncategorized\")\n                if category not in categories:\n                    categories[category] = []\n                categories[category].append(system)\n\n            # Add fields for each category\n            for category, systems_list in categories.items():\n                field_value = \"\"\n                for system in systems_list[:5]:  # Limit per category\n                    status_emoji = \"\" if system[\"status\"] == \"active\" else \"\" if system[\"status\"] == \"maintenance\" else \"\"\n                    field_value += f\"{status_emoji} **{system['name']}**\\n\"\n                    field_value += f\" {system['description'][:80]}{'...' if len(system['description']) > 80 else ''}\\n\\n\"\n\n                if field_value:\n                    if len(field_value) > 1024:\n                        field_value = field_value[:1020] + \"...\"\n                    embed.add_field(\n                        name=f\" {category} ({len(systems_list)})\",\n                        value=field_value,\n                        inline=False\n                    )\n\n            embed.set_footer(text=f\"Total: {len(systems)} systems | Use !systems_inventory for overview\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\tools_commands.py",
          "count": 4,
          "lines": [
            "            try:\n                all_tools = self.registry.list_tools()\n                \n                # Filter by category if provided\n                if category:\n                    category_lower = category.lower()\n                    all_tools = [\n                        tool for tool in all_tools\n                        if category_lower in tool[\"id\"].lower() or \n                           category_lower in tool[\"name\"].lower() or\n                           any(category_lower in flag.lower() for flag in tool.get(\"flags\", []))\n                    ]\n                \n                if not all_tools:\n                    await ctx.send(f\" No tools found matching '{category}'\")\n                    return\n                \n                # Group tools for pagination (Discord has 2000 char limit)\n                tools_per_page = 10\n                total_pages = (len(all_tools) + tools_per_page - 1) // tools_per_page\n                \n                for page_num in range(total_pages):\n                    start_idx = page_num * tools_per_page\n                    end_idx = min(start_idx + tools_per_page, len(all_tools))\n                    page_tools = all_tools[start_idx:end_idx]\n                    \n                    embed = discord.Embed(\n                        title=f\" Toolbelt Tools\" + (f\" - {category}\" if category else \"\"),\n                        description=f\"Page {page_num + 1}/{total_pages} ({len(all_tools)} total tools)\",\n                        color=discord.Color.blue()\n                    )\n                    \n                    for tool in page_tools:\n                        tool_id = tool[\"id\"]\n                        name = tool[\"name\"]\n                        description = tool.get(\"description\", \"No description\")\n                        flags = \", \".join(tool.get(\"flags\", [])[:3])  # First 3 flags\n                        if len(tool.get(\"flags\", [])) > 3:\n                            flags += f\" (+{len(tool.get('flags', [])) - 3} more)\"\n                        \n                        # Truncate description if too long\n                        if len(description) > 100:\n                            description = description[:97] + \"...\"\n                        \n                        embed.add_field(\n                            name=f\"`{tool_id}` - {name}\",\n                            value=f\"{description}\\n**Flags:** `{flags}`\",\n                            inline=False\n                        )\n                    \n                    if page_num == 0:\n                        await ctx.send(embed=embed)\n                    else:\n                        await ctx.send(embed=embed)\n            \n            except Exception as e:",
            "            try:\n                if not tool_id:\n                    await ctx.send(\" Please specify a tool ID or flag. Use `!tools` to list all tools.\")\n                    return\n                \n                # Try to find tool by ID or flag\n                tool_config = None\n                found_tool_id = None\n                \n                # First try by tool ID\n                if tool_id in TOOLS_REGISTRY:\n                    tool_config = TOOLS_REGISTRY[tool_id]\n                    found_tool_id = tool_id\n                else:\n                    # Try by flag\n                    tool_config = self.registry.get_tool_for_flag(tool_id)\n                    if tool_config:\n                        # Find the tool ID for this config\n                        for tid, config in TOOLS_REGISTRY.items():\n                            if config == tool_config:\n                                found_tool_id = tid\n                                break\n                \n                if not tool_config:\n                    # Try partial match\n                    matching_tools = [\n                        (tid, config) for tid, config in TOOLS_REGISTRY.items()\n                        if tool_id.lower() in tid.lower() or\n                           tool_id.lower() in config.get(\"name\", \"\").lower()\n                    ]\n                    \n                    if len(matching_tools) == 1:\n                        found_tool_id, tool_config = matching_tools[0]\n                    elif len(matching_tools) > 1:\n                        # Show matches\n                        matches = \"\\n\".join([f\"`{tid}`\" for tid, _ in matching_tools[:10]])\n                        await ctx.send(\n                            f\" Multiple tools match '{tool_id}':\\n{matches}\\n\\n\"\n                            f\"Use `!tool <exact-id>` to get details.\"\n                        )\n                        return\n                    else:\n                        await ctx.send(f\" Tool '{tool_id}' not found. Use `!tools` to list all tools.\")\n                        return\n                \n                # Create detailed embed\n                embed = discord.Embed(\n                    title=f\" {tool_config.get('name', 'Unknown Tool')}\",\n                    description=tool_config.get(\"description\", \"No description available\"),\n                    color=discord.Color.green()\n                )\n                \n                # Add tool ID\n                embed.add_field(name=\"Tool ID\", value=f\"`{found_tool_id}`\", inline=True)\n                \n                # Add module\n                embed.add_field(name=\"Module\", value=f\"`{tool_config.get('module', 'N/A')}`\", inline=True)\n                \n                # Add main function\n                embed.add_field(name=\"Main Function\", value=f\"`{tool_config.get('main_function', 'main')}`\", inline=True)\n                \n                # Add flags\n                flags = tool_config.get(\"flags\", [])\n                if flags:\n                    flags_str = \"\\n\".join([f\"`{flag}`\" for flag in flags])\n                    embed.add_field(name=\"Flags\", value=flags_str, inline=False)\n                \n                # Add args passthrough\n                args_passthrough = tool_config.get(\"args_passthrough\", True)\n                embed.add_field(name=\"Args Passthrough\", value=\" Yes\" if args_passthrough else \" No\", inline=True)\n                \n                # Add usage example\n                if flags:\n                    primary_flag = flags[0]\n                    embed.add_field(\n                        name=\"Usage Example\",\n                        value=f\"```bash\\npython -m tools.toolbelt {primary_flag} [args]\\n```\",\n                        inline=False\n                    )\n                \n                await ctx.send(embed=embed)\n            \n            except Exception as e:",
            "            try:\n                unified_tools = [\n                    tool for tool in self.registry.list_tools()\n                    if \"unified\" in tool[\"id\"].lower()\n                ]\n                \n                if not unified_tools:\n                    await ctx.send(\" No unified tools found\")\n                    return\n                \n                embed = discord.Embed(\n                    title=\" Unified Tools (Consolidated)\",\n                    description=f\"{len(unified_tools)} unified tools consolidating multiple individual tools\",\n                    color=discord.Color.purple()\n                )\n                \n                for tool in unified_tools:\n                    tool_id = tool[\"id\"]\n                    name = tool[\"name\"]\n                    description = tool.get(\"description\", \"No description\")\n                    flags = \", \".join(tool.get(\"flags\", [])[:2])\n                    \n                    # Extract consolidation info from description\n                    if \"consolidates\" in description.lower():\n                        # Try to extract number\n                        import re\n                        match = re.search(r'consolidates?\\s+(\\d+)\\+?', description, re.IGNORECASE)\n                        if match:\n                            count = match.group(1)\n                            description = f\"Consolidates {count}+ tools\"\n                    \n                    embed.add_field(\n                        name=f\"`{tool_id}` - {name}\",\n                        value=f\"{description}\\n**Flags:** `{flags}`\",\n                        inline=False\n                    )\n                \n                await ctx.send(embed=embed)\n            \n            except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\webhook_commands.py",
          "count": 7,
          "lines": [
            "        try:\n            # Determine target channel\n            if channel is None:\n                channel = ctx.channel\n\n            # Validate permissions\n            if not channel.permissions_for(ctx.guild.me).manage_webhooks:\n                embed = discord.Embed(\n                    title=\" Permission Denied\",\n                    description=\"I need 'Manage Webhooks' permission to create webhooks.\",\n                    color=discord.Color.red()\n                )\n                await ctx.send(embed=embed)\n                return\n\n            # Create webhook using service\n            webhook = await self.webhook_service.create_webhook(\n                channel=channel,\n                name=webhook_name,\n                reason=reason\n            )\n\n            if webhook:\n                embed = discord.Embed(\n                    title=\" Webhook Created\",\n                    description=f\"Successfully created webhook in {channel.mention}\",\n                    color=discord.Color.green()\n                )\n                embed.add_field(name=\"Name\", value=webhook.name, inline=True)\n                embed.add_field(name=\"ID\", value=str(webhook.id), inline=True)\n                embed.add_field(name=\"Channel\", value=channel.name, inline=True)\n\n                # Show URL privately\n                url_embed = discord.Embed(\n                    title=\" Webhook URL (Private)\",\n                    description=\"Keep this URL secure!\",\n                    color=discord.Color.orange()\n                )\n                url_embed.add_field(name=\"URL\", value=f\"||{webhook.url}||\", inline=False)\n\n                await ctx.send(embed=embed)\n                try:\n                    await ctx.author.send(embed=url_embed)\n                except discord.Forbidden:",
            "        try:\n            # Get webhooks for current channel\n            channel_webhooks = await self.webhook_service.list_channel_webhooks(ctx.channel)\n\n            if not channel_webhooks:\n                embed = discord.Embed(\n                    title=\" No Webhooks Found\",\n                    description=f\"No webhooks found in {ctx.channel.mention}\",\n                    color=discord.Color.blue()\n                )\n                embed.add_field(\n                    name=\"Create One\",\n                    value=\"Use `!create_webhook` to create a webhook\",\n                    inline=False\n                )\n                await ctx.send(embed=embed)\n                return\n\n            embed = discord.Embed(\n                title=f\" Webhooks in #{ctx.channel.name}\",\n                description=f\"Found {len(channel_webhooks)} webhook(s)\",\n                color=discord.Color.blue()\n            )\n\n            for webhook in channel_webhooks[:10]:  # Limit to 10\n                created = webhook.get('created_at', 'Unknown')\n                user = webhook.get('user', 'Unknown')\n                embed.add_field(\n                    name=f\" {webhook['name']}\",\n                    value=f\"ID: `{webhook['id']}`\\nCreated: {created[:10] if created != 'Unknown' else 'Unknown'}\\nBy: {user}\",\n                    inline=False\n                )\n\n            if len(channel_webhooks) > 10:\n                embed.set_footer(text=f\"Showing 10 of {len(channel_webhooks)} webhooks\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            # Show confirmation dialog\n            embed = discord.Embed(\n                title=\" Confirm Deletion\",\n                description=f\"Are you sure you want to delete webhook `{webhook_id}`?\",\n                color=discord.Color.orange()\n            )\n            embed.add_field(\n                name=\" Warning\",\n                value=\"This action cannot be undone!\",\n                inline=False\n            )\n\n            view = WebhookDeletionView(webhook_id, self.webhook_service)\n            await ctx.send(embed=embed, view=view)\n\n        except Exception as e:",
            "        try:\n            # Test the webhook\n            success = await self.webhook_service.test_webhook(\n                webhook_id=webhook_id,\n                test_message=f\" Webhook test from {ctx.author.display_name}\"\n            )\n\n            if success:\n                embed = discord.Embed(\n                    title=\" Webhook Test Successful\",\n                    description=f\"Webhook `{webhook_id}` is working correctly!\",\n                    color=discord.Color.green()\n                )\n                embed.add_field(\n                    name=\"Test Message\",\n                    value=\"Check if a test message appeared in the configured channel\",\n                    inline=False\n                )\n            else:\n                embed = discord.Embed(\n                    title=\" Webhook Test Failed\",\n                    description=f\"Webhook `{webhook_id}` failed to send test message\",\n                    color=discord.Color.red()\n                )\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            info = await self.webhook_service.get_webhook_info(webhook_id)\n\n            if not info:\n                embed = discord.Embed(\n                    title=\" Webhook Not Found\",\n                    description=f\"Could not find webhook `{webhook_id}`\",\n                    color=discord.Color.red()\n                )\n                await ctx.send(embed=embed)\n                return\n\n            embed = discord.Embed(\n                title=f\" Webhook Information\",\n                description=f\"Details for webhook `{webhook_id}`\",\n                color=discord.Color.blue()\n            )\n\n            embed.add_field(name=\"Name\", value=info.get(\"name\", \"Unknown\"), inline=True)\n            embed.add_field(name=\"ID\", value=info.get(\"id\", \"Unknown\"), inline=True)\n            embed.add_field(name=\"Status\", value=info.get(\"status\", \"Unknown\"), inline=True)\n\n            channel = info.get(\"channel\", \"Unknown\")\n            guild = info.get(\"guild\", \"Unknown\")\n            embed.add_field(name=\"Channel\", value=channel, inline=True)\n            embed.add_field(name=\"Server\", value=guild, inline=True)\n\n            created = info.get(\"created_at\", \"Unknown\")\n            if created and created != \"Unknown\":\n                embed.add_field(name=\"Created\", value=created[:19], inline=True)\n\n            user = info.get(\"user\", \"Unknown\")\n            embed.add_field(name=\"Created By\", value=user, inline=False)\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            success = await self.webhook_service.delete_webhook(\n                webhook_id=self.webhook_id,\n                reason=f\"Deleted by {interaction.user.display_name}\"\n            )\n\n            if success:\n                embed = discord.Embed(\n                    title=\" Webhook Deleted\",\n                    description=f\"Successfully deleted webhook `{self.webhook_id}`\",\n                    color=discord.Color.green()\n                )\n            else:\n                embed = discord.Embed(\n                    title=\" Deletion Failed\",\n                    description=f\"Failed to delete webhook `{self.webhook_id}`\",\n                    color=discord.Color.red()\n                )\n\n            await interaction.response.edit_message(embed=embed, view=None)\n\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_service.py",
          "count": 10,
          "lines": [
            "                try:\n                    with open(config_path) as f:\n                        config = json.load(f)\n                        webhook_url = config.get(\"webhook_url\")\n                except Exception:",
            "        try:\n            embed = self._create_agent_status_embed(agent_status)\n            payload = {\n                \"embeds\": [embed],\n                \"username\": \"V2_SWARM Status Monitor\",\n                \"avatar_url\": os.getenv(\"DISCORD_AVATAR_URL\", None),\n            }\n\n            response = self.session.post(self.webhook_url, json=payload)\n            return response.status_code == 204\n\n        except Exception as e:",
            "        try:\n            embed = self._create_coordination_embed(coordination_data)\n            payload = {\n                \"embeds\": [embed],\n                \"username\": \"V2_SWARM Coordinator\",\n                \"avatar_url\": os.getenv(\"DISCORD_AVATAR_URL\", None),\n            }\n\n            response = self.session.post(self.webhook_url, json=payload)\n            return response.status_code == 204\n\n        except Exception as e:",
            "        try:\n            embed = self._create_devlog_embed(devlog_data)\n            payload = {\n                \"embeds\": [embed],\n                \"username\": \"V2_SWARM DevLog Monitor\",\n                \"avatar_url\": os.getenv(\"DISCORD_AVATAR_URL\", None),\n            }\n\n            response = self.session.post(self.webhook_url, json=payload)\n            return response.status_code == 204\n\n        except Exception as e:",
            "        try:\n            from datetime import datetime, timezone\n\n            # Format as proper D2A message\n            timestamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n            message = f\"\"\"[D2A] All Agents\n\n**Priority**: REGULAR\n**Status**: DEVLOG_MONITOR_ALERT\n**Source**: Discord DevLog Monitor\n**Timestamp**: {timestamp}\n\n **DISCORD DEVLOG MONITORING ALERT**\nAutomated DevLog monitoring has detected new activity requiring swarm awareness.\n\n**DevLog Activity Detected:**\n **Title:** {devlog_data['title']}\n **Category:** {devlog_data['category'].title()}\n **Agent:** {devlog_data['agent']}\n **Summary:** {devlog_data['description'][:200]}...\n\n**DevLog monitoring is active and Discord notifications are enabled.**\n**WE ARE SWARM - Stay coordinated!**\n\n WE. ARE. SWARM. \n\n---\n*Automated DevLog Monitor - D2A Template Compliant*\n\"\"\"\n            result = await self.agent_engine.broadcast_to_all_agents(\n                message, sender=\"Discord_DevLog_Monitor\"\n            )\n\n            if result.success:\n                print(f\" Notified {result.data.get('successful_deliveries', 0)} agents\")\n            else:\n                print(\" Failed to notify agents about devlog\")\n\n        except Exception as e:",
            "        try:\n            new_devlogs = self._find_new_devlogs()\n            for devlog_path in new_devlogs:\n                await self._process_devlog(devlog_path)\n        except Exception as e:",
            "        try:\n            test_payload = {\n                \"content\": \" **Discord Webhook Test**\\n\\nV2_SWARM DevLog integration is now operational!\",\n                \"username\": \"V2_SWARM Test Bot\",\n            }\n\n            response = self.session.post(self.webhook_url, json=test_payload)\n\n            if response.status_code == 204:\n                print(\" Discord webhook connection successful!\")\n                return True\n            else:\n                print(f\" Discord webhook test failed: {response.status_code}\")\n                return False\n\n        except Exception as e:",
            "        try:\n            while self.is_running:\n                await self._check_for_new_devlogs()\n                await asyncio.sleep(check_interval)\n        except KeyboardInterrupt:",
            "        try:\n            with open(devlog_path, encoding=\"utf-8\") as f:\n                content = f.read()\n\n            metadata = self._parse_devlog_filename(devlog_path.name)\n            devlog_data = {\n                \"title\": metadata.get(\"title\", devlog_path.name),\n                \"description\": self._extract_devlog_summary(content),\n                \"category\": metadata.get(\"category\", \"general\"),\n                \"agent\": metadata.get(\"agent\", \"Unknown\"),\n                \"filepath\": str(devlog_path),\n                \"timestamp\": datetime.utcnow().isoformat(),\n            }\n\n            print(f\" Processing devlog: {devlog_data['title']}\")\n\n            if self.send_devlog_notification(devlog_data):\n                print(\" Discord notification sent\")\n                await self._notify_agents_of_devlog(devlog_data)\n            else:\n                print(\" Failed to send Discord notification\")\n\n        except Exception as e:",
            "try:\n    from .discord_agent_communication import AgentCommunicationEngine\n    from .discord_embeds import (\n        create_agent_status_embed,\n        create_coordination_embed,\n        create_devlog_embed,\n    )\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\status_change_monitor.py",
          "count": 7,
          "lines": [
            "            try:\n                # Non-blocking stat\n                current_mtime = await asyncio.to_thread(lambda: status_file.stat().st_mtime)\n                last_mtime = self.last_modified.get(agent_id, 0)\n\n                if current_mtime > last_mtime:\n                    # Read with retry\n                    new_status = await self._read_json_with_retry(status_file)\n                    if not new_status:\n                        continue\n\n                    old_status = self.last_status.get(agent_id, {})\n                    changes = self._detect_changes(old_status, new_status)\n\n                    if changes:\n                        logger.info(f\" Change detected for {agent_id}: {list(changes.keys())}\")\n                        # Add to pending updates (Debouncing)\n                        self.pending_updates[agent_id] = {\n                            \"status\": new_status,\n                            \"changes\": changes,\n                            \"timestamp\": datetime.now()\n                        }\n                    \n                    self.last_modified[agent_id] = current_mtime\n                    self.last_status[agent_id] = new_status.copy()\n            except Exception as e:",
            "            try:\n                def _read():\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                            return json.load(f)\n                return await asyncio.to_thread(_read)\n            except json.JSONDecodeError:",
            "            try:\n                if not self.monitor_status_changes.is_running():\n                    self.monitor_status_changes.start()\n                    logger.info(\" Status change monitor started\")\n                else:\n                    logger.info(\" Status change monitor already running\")\n            except Exception as e:",
            "        try:\n            # 1. Check for File Changes\n            # We run this check every 15 seconds logic-wise (or every loop iteration)\n            # but for debouncing we need frequent checks.\n            # Let's keep file checking on every iteration (5s)\n            \n            await self._check_files()\n            \n            # 2. Process Pending Updates (Debouncing)\n            await self._process_pending_updates()\n            \n            # 3. Update Persistent Dashboard\n            await self._update_dashboard()\n\n            # 4. Inactivity Check (Less frequent - e.g., every minute)\n            if datetime.now().second < 10:  # Approx once per minute\n                await self._run_inactivity_checks()\n\n        except Exception as e:",
            "        try:\n            # Re-generate dashboard embed logic here (simplified for now)\n            # In a real impl, we'd call StatusReader -> generate combined embed\n            # For now, we skip or assume external call sets it up.\n            pass\n        except Exception as e:",
            "        try:\n            await channel.send(embed=embed)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import tasks\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\github_book_data.py",
          "count": 1,
          "lines": [
            "        try:\n            if self.data_path.exists():\n                with open(self.data_path, 'r', encoding='utf-8') as f:\n                    self._data = json.load(f)\n                logger.info(f\"Loaded GitHub data from {self.data_path}\")\n            else:\n                logger.warning(f\"GitHub data file not found: {self.data_path}\")\n                self._data = self._get_default_data()\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_ui_components.py",
          "count": 2,
          "lines": [
            "                try:\n                    page_num = int(self.page_input.value)\n                    if 1 <= page_num <= max_pages:\n                        await callback_function(interaction, page_num)\n                    else:\n                        await interaction.response.send_message(\n                            f\"Page number must be between 1 and {max_pages}!\",\n                            ephemeral=True\n                        )\n                except ValueError:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\github_book_commands.py",
          "count": 1,
          "lines": [
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_data.py",
          "count": 2,
          "lines": [
            "                        try:\n                            with open(status_file, 'r', encoding='utf-8') as f:\n                                status_data = json.load(f)\n                                status_data['agent_id'] = agent_dir.name\n                                statuses.append(status_data)\n                        except Exception as e:",
            "            try:\n                content = self.master_task_log.read_text(encoding='utf-8')\n                # Simple parsing - could be enhanced\n                lines = content.split('\\n')\n                current_section = None\n\n                for line in lines:\n                    line = line.strip()\n                    if line.startswith('## '):\n                        section = line[3:].lower()\n                        if 'this week' in section:\n                            current_section = 'active'\n                        elif 'completed' in section:\n                            current_section = 'completed'\n                        elif 'waiting' in section:\n                            current_section = 'blocked'\n                        else:\n                            current_section = None\n                    elif line.startswith('- [') and current_section:\n                        task = line[2:].strip()\n                        if current_section == 'active':\n                            tasks['active'].append(task)\n                        elif current_section == 'completed':\n                            tasks['completed_today'].append(task)\n\n                tasks['total_active'] = len(tasks['active'])\n                tasks['total_completed'] = len(tasks['completed_today'])\n\n            except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\swarm_showcase_embeds.py",
          "count": 1,
          "lines": [
            "try:\n    import discord\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\command_base.py",
          "count": 6,
          "lines": [
            "        try:\n            agent_num = int(agent_id[6:])  # Extract number after \"Agent-\"\n            return 1 <= agent_num <= 8\n        except (ValueError, IndexError):",
            "        try:\n            if hasattr(ctx_or_interaction, 'response'):  # Interaction\n                if ephemeral:\n                    await ctx_or_interaction.response.send_message(\n                        content=content, embed=embed, ephemeral=True\n                    )\n                else:\n                    await ctx_or_interaction.response.send_message(\n                        content=content, embed=embed\n                    )\n            else:  # Context\n                await ctx_or_interaction.send(content=content, embed=embed)\n            return True\n        except Exception as e:",
            "        try:\n            if hasattr(ctx_or_interaction, 'user'):  # Interaction\n                return ctx_or_interaction.user.display_name\n            else:  # Context\n                return ctx_or_interaction.author.display_name\n        except Exception:",
            "        try:\n            if not self.messaging_controller:\n                return {\"success\": False, \"error\": \"Messaging controller not available\"}\n\n            if agent_list is None:\n                agent_list = [f\"Agent-{i}\" for i in range(1, 9)]\n\n            results = []\n            for agent_id in agent_list:\n                result = await self._send_agent_message(agent_id, message, priority)\n                results.append({\"agent\": agent_id, \"success\": result[\"success\"]})\n\n            success_count = sum(1 for r in results if r[\"success\"])\n            return {\n                \"success\": success_count > 0,\n                \"total\": len(results),\n                \"successful\": success_count,\n                \"results\": results\n            }\n\n        except Exception as e:",
            "        try:\n            if not self.messaging_controller:\n                return {\"success\": False, \"error\": \"Messaging controller not available\"}\n\n            result = await self.messaging_controller.send_agent_message(\n                agent_id=agent_id,\n                message=message,\n                priority=priority,\n                sender=\"Discord-Bot\"\n            )\n\n            return {\"success\": result is not None, \"result\": result}\n\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\embed_factory.py",
          "count": 1,
          "lines": [
            "try:\n    import discord\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\music_service.py",
          "count": 8,
          "lines": [
            "        try:\n            # Clean query for filename\n            safe_query = re.sub(r'[^\\w\\s-]', '', query)[:50]\n            filename = f\"{safe_query}.mp3\"\n            output_path = self.cache_dir / filename\n\n            # Check cache first\n            if output_path.exists():\n                logger.info(f\"Using cached audio: {filename}\")\n                return output_path\n\n            # Download with yt-dlp\n            with yt_dlp.YoutubeDL(self.ytdl_opts) as ydl:\n                # Search for video if not a URL\n                if not query.startswith(('http://', 'https://')):\n                    query = f\"ytsearch1:{query}\"\n\n                info = ydl.extract_info(query, download=True)\n\n                if 'entries' in info:\n                    # Search result\n                    video_info = info['entries'][0]\n                else:\n                    # Direct URL\n                    video_info = info\n\n                # Get the actual filename\n                title = video_info.get('title', 'unknown')\n                safe_title = re.sub(r'[^\\w\\s-]', '', title)[:50]\n                actual_path = self.cache_dir / f\"{safe_title}.mp3\"\n\n                if actual_path.exists():\n                    logger.info(f\"Successfully downloaded: {title}\")\n                    return actual_path\n                else:\n                    logger.error(f\"Download failed - file not found: {actual_path}\")\n                    return None\n\n        except Exception as e:",
            "        try:\n            for audio_file in self.cache_dir.glob(\"*.mp3\"):\n                if audio_file.stat().st_mtime < cutoff_time:\n                    audio_file.unlink()\n                    removed_count += 1\n\n            logger.info(f\"Cleaned up {removed_count} old audio files\")\n            return removed_count\n\n        except Exception as e:",
            "        try:\n            if ctx.author.voice is None:\n                await ctx.send(\" You must be in a voice channel to play music!\")\n                return False\n\n            voice_channel = ctx.author.voice.channel\n\n            if ctx.voice_client is not None:\n                # Already in a voice channel\n                if ctx.voice_client.channel == voice_channel:\n                    return True\n                else:\n                    # Move to new channel\n                    await ctx.voice_client.move_to(voice_channel)\n                    return True\n            else:\n                # Join new channel\n                self.current_voice_client = await voice_channel.connect()\n                return True\n\n        except Exception as e:",
            "        try:\n            voice_client = ctx.voice_client\n            if voice_client and voice_client.is_playing():\n                voice_client.stop()\n                self.current_song = None\n                logger.info(\"Music playback stopped\")\n                return True\n            return False\n\n        except Exception as e:",
            "        try:\n            voice_client = ctx.voice_client\n            if voice_client is None:\n                logger.error(\"No voice client available\")\n                return False\n\n            # Create audio source\n            audio_source = FFmpegPCMAudio(str(audio_path))\n\n            # Play audio\n            voice_client.play(audio_source)\n\n            # Store current song info\n            self.current_song = {\n                'path': audio_path,\n                'title': audio_path.stem,\n                'ctx': ctx\n            }\n\n            logger.info(f\"Started playing: {audio_path.name}\")\n            return True\n\n        except Exception as e:",
            "        try:\n            voice_client = ctx.voice_client\n            if voice_client:\n                await voice_client.disconnect()\n                self.current_voice_client = None\n                self.current_song = None\n                logger.info(\"Disconnected from voice channel\")\n                return True\n            return False\n\n        except Exception as e:",
            "try:\n    import discord\n    from discord import FFmpegPCMAudio\n    DISCORD_AVAILABLE = True\nexcept ImportError:",
            "try:\n    import yt_dlp\n    YT_DLP_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\status_service.py",
          "count": 1,
          "lines": [
            "        try:\n            with open(status_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                logger.debug(f\"Successfully read status for {agent_id}\")\n                return data\n        except (json.JSONDecodeError, IOError) as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\webhook_service.py",
          "count": 9,
          "lines": [
            "        try:\n            # Check permissions\n            if not channel.permissions_for(channel.guild.me).manage_webhooks:\n                raise ValueError(\"Missing 'Manage Webhooks' permission\")\n\n            # Create webhook\n            webhook = await channel.create_webhook(\n                name=name,\n                avatar=await self._get_avatar_bytes(avatar_url) if avatar_url else None,\n                reason=reason\n            )\n\n            # Store in config\n            config_data = {\n                \"id\": str(webhook.id),\n                \"name\": webhook.name,\n                \"channel_id\": str(channel.id),\n                \"channel_name\": channel.name,\n                \"guild_id\": str(channel.guild.id),\n                \"guild_name\": channel.guild.name,\n                \"url\": webhook.url,\n                \"token\": webhook.token,\n                \"created_at\": str(discord.utils.utcnow()),\n                \"created_by\": \"agent_automation\"\n            }\n            self.config_manager.add_webhook(str(webhook.id), config_data)\n\n            logger.info(f\"Created webhook '{name}' in #{channel.name}\")\n            return webhook\n\n        except Exception as e:",
            "        try:\n            # Find webhook across all guilds\n            webhook = None\n            for guild in self.bot.guilds:\n                try:\n                    webhook = await guild.fetch_webhook(int(webhook_id))\n                    break\n                except (discord.NotFound, discord.Forbidden):",
            "        try:\n            # Get webhook config\n            config = self.config_manager.get_webhook(webhook_id)\n            if not config:\n                raise ValueError(f\"Webhook {webhook_id} not found in config\")\n\n            # Create webhook from URL\n            webhook = discord.Webhook.from_url(config[\"url\"], session=self.bot.http._HTTPClient__session)\n\n            # Send test message\n            await webhook.send(\n                content=test_message,\n                username=\"Webhook Test\",\n                avatar_url=None\n            )\n\n            logger.info(f\"Successfully tested webhook {webhook_id}\")\n            return True\n\n        except Exception as e:",
            "        try:\n            # Try to fetch from Discord API first\n            webhook = None\n            for guild in self.bot.guilds:\n                try:\n                    webhook = await guild.fetch_webhook(int(webhook_id))\n                    break\n                except (discord.NotFound, discord.Forbidden):",
            "        try:\n            config[\"last_updated\"] = str(discord.utils.utcnow()) if DISCORD_AVAILABLE else None\n            with open(self.config_file, 'w', encoding='utf-8') as f:\n                json.dump(config, f, indent=2, ensure_ascii=False)\n        except Exception as e:",
            "        try:\n            import aiohttp\n            async with aiohttp.ClientSession() as session:\n                async with session.get(avatar_url) as response:\n                    if response.status == 200:\n                        return await response.read()\n        except Exception as e:",
            "        try:\n            webhooks = await channel.webhooks()\n            return [{\n                \"id\": str(webhook.id),\n                \"name\": webhook.name,\n                \"created_at\": str(webhook.created_at) if webhook.created_at else None,\n                \"user\": str(webhook.user) if webhook.user else \"Unknown\"\n            } for webhook in webhooks]\n\n        except Exception as e:",
            "        try:\n            with open(self.config_file, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError) as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\inventory_service.py",
          "count": 1,
          "lines": [
            "        try:\n            if not self.inventory_file.exists():\n                logger.warning(f\"Inventory file not found: {self.inventory_file}\")\n                return self._create_empty_inventory()\n\n            with open(self.inventory_file, 'r', encoding='utf-8') as f:\n                inventory = json.load(f)\n\n            self._inventory_cache = inventory\n            logger.info(f\"Loaded inventory with {len(inventory.get('systems', {}))} systems\")\n            return inventory\n\n        except (json.JSONDecodeError, IOError) as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\bot_runner_service.py",
          "count": 5,
          "lines": [
            "            try:\n                self.logger.info(f\" Starting Discord bot (attempt {attempt + 1}/{max_retries})\")\n\n                token = os.getenv('DISCORD_BOT_TOKEN')\n                if not token:\n                    self.logger.error(\" No Discord token found\")\n                    return 1\n\n                # Run the bot\n                await bot.start(token)\n\n            except discord.LoginFailure:",
            "        try:\n            # Import the unified bot\n            from src.discord_commander.unified_discord_bot import UnifiedDiscordBot\n\n            # Get the bot token\n            token = os.getenv('DISCORD_BOT_TOKEN')\n            if not token:\n                raise ValueError(\"DISCORD_BOT_TOKEN not set\")\n\n            # Create the unified bot (intents are configured internally)\n            bot = UnifiedDiscordBot(token=token)\n\n            self.logger.info(\" Bot instance created successfully\")\n            return bot\n\n        except Exception as e:",
            "        try:\n            # Print startup banner\n            self.print_startup_banner()\n\n            # Setup logging\n            self.setup_logging()\n\n            # Validate environment\n            if not self.validate_environment():\n                return 1\n\n            # Create bot instance\n            bot = self.create_bot_instance()\n            if not bot:\n                return 1\n\n            # Run bot with reconnection logic\n            return await self.run_bot_with_reconnection(bot)\n\n        except KeyboardInterrupt:",
            "        try:\n            self.logger.info(\" Initiating graceful shutdown...\")\n\n            # Close the bot connection\n            await bot.close()\n\n            # Cancel all running tasks\n            tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]\n            if tasks:\n                self.logger.info(f\" Cancelling {len(tasks)} background tasks...\")\n                for task in tasks:\n                    task.cancel()\n\n                # Wait for tasks to cancel\n                await asyncio.gather(*tasks, return_exceptions=True)\n\n            self.logger.info(\" Graceful shutdown completed\")\n\n        except Exception as e:",
            "try:\n    import discord\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\onboarding_modals.py",
          "count": 2,
          "lines": [
            "        try:\n            comm_style = self.children[3].value if len(self.children) > 3 else \"Collaborative\"\n\n            # Here you would typically register the agent\n            # For now, just return success\n            message = f\"\"\"\n**Agent Profile Created:**\n **Name:** {agent_name}\n **Role:** {primary_role}\n **Specialization:** {specialization or 'General'}\n **Communication Style:** {comm_style}\n\n*Feel free to explore the swarm and contribute when ready!*\n *We. Are. Swarm.* \n            \"\"\".strip()\n\n            return {\n                \"success\": True,\n                \"message\": message,\n                \"agent_data\": {\n                    \"name\": agent_name,\n                    \"role\": primary_role,\n                    \"specialization\": specialization,\n                    \"comm_style\": comm_style,\n                    \"onboarding_type\": \"soft\"\n                }\n            }\n\n        except Exception as e:",
            "        try:\n            security_level = self.children[3].value if len(self.children) > 3 else None\n            compliance_ack = self.children[4].value if len(self.children) > 4 else None\n\n            # Validate required fields\n            if not security_level:\n                return {\n                    \"success\": False,\n                    \"error\": \"Security clearance level is required for hard onboarding.\"\n                }\n\n            if compliance_ack != \"I ACKNOWLEDGE\":\n                return {\n                    \"success\": False,\n                    \"error\": \"Compliance acknowledgment must be exactly 'I ACKNOWLEDGE'.\"\n                }\n\n            # Validate specialization is provided\n            if not specialization:\n                return {\n                    \"success\": False,\n                    \"error\": \"Specialization is required for hard onboarding.\"\n                }\n\n            # Here you would typically perform security validation\n            # and register the agent with elevated permissions\n            message = f\"\"\"\n** Critical Agent Onboarding Complete**\n\n**Agent Profile:**\n **Name:** {agent_name}\n **Role:** {primary_role}\n **Specialization:** {specialization}\n **Security Level:** {security_level}\n **Compliance:**  Acknowledged\n\n**System Access Granted:**\n Production environment access\n Elevated permissions enabled\n Audit logging activated\n Compliance monitoring active\n\n*Agent is now active in the swarm with full operational capabilities.*\n *We. Are. Swarm.* \n            \"\"\".strip()\n\n            return {\n                \"success\": True,\n                \"message\": message,\n                \"agent_data\": {\n                    \"name\": agent_name,\n                    \"role\": primary_role,\n                    \"specialization\": specialization,\n                    \"security_level\": security_level,\n                    \"compliance_acknowledged\": True,\n                    \"onboarding_type\": \"hard\",\n                    \"elevated_permissions\": True\n                }\n            }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\broadcast_modals.py",
          "count": 3,
          "lines": [
            "        try:\n            broadcast_type = self.children[2].value if len(self.children) > 2 else \"general\"\n            urgency = self.children[3].value if len(self.children) > 3 else \"normal\"\n\n            # Here you would typically broadcast to all agents\n            formatted_message = f\"\"\"\n** Swarm Broadcast Sent**\n\n**Broadcast Type:** {broadcast_type}\n**Urgency:** {urgency}\n**From:** {interaction.user.display_name}\n**Title:** {title}\n\n**Content:**\n{content}\n\n*Broadcast queued for delivery to all swarm agents.*\n *We. Are. Swarm.* \n            \"\"\".strip()\n\n            return {\n                \"success\": True,\n                \"message\": formatted_message,\n                \"broadcast_type\": broadcast_type,\n                \"urgency\": urgency,\n                \"message_type\": \"swarm_broadcast\"\n            }\n\n        except Exception as e:",
            "        try:\n            target_agent = self.children[2].value\n\n            # Here you would typically send the message via the messaging service\n            # For now, just return success with formatted message\n            formatted_message = f\"\"\"\n** Agent Message Sent**\n\n**To:** {target_agent}\n**From:** {interaction.user.display_name}\n**Title:** {title}\n\n**Content:**\n{content}\n\n*Message queued for delivery through the swarm messaging system.*\n            \"\"\".strip()\n\n            return {\n                \"success\": True,\n                \"message\": formatted_message,\n                \"recipient\": target_agent,\n                \"message_type\": \"agent_direct\"\n            }\n\n        except Exception as e:",
            "        try:\n            target_group = self.children[2].value\n            priority = self.children[3].value if len(self.children) > 3 else \"regular\"\n\n            # Here you would typically broadcast to the selected group\n            formatted_message = f\"\"\"\n** Selective Broadcast Sent**\n\n**Target Group:** {target_group}\n**Priority:** {priority}\n**From:** {interaction.user.display_name}\n**Title:** {title}\n\n**Content:**\n{content}\n\n*Broadcast queued for delivery to {target_group} agents.*\n            \"\"\".strip()\n\n            return {\n                \"success\": True,\n                \"message\": formatted_message,\n                \"target_group\": target_group,\n                \"priority\": priority,\n                \"message_type\": \"selective_broadcast\"\n            }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\template_modals.py",
          "count": 3,
          "lines": [
            "        try:\n            energy_intensity = self.children[2].value if len(self.children) > 2 else \"high\"\n            celebration_type = self.children[3].value if len(self.children) > 3 else \"general\"\n\n            # Add energy indicators\n            energy_indicators = {\n                \"maximum\": \"\",\n                \"high\": \"\",\n                \"medium\": \"\",\n                \"low\": \"\"\n            }\n\n            energy_emoji = energy_indicators.get(energy_intensity.lower(), energy_indicators[\"high\"])\n\n            rendered_content = self._render_template(template_type, custom_content)\n\n            formatted_message = f\"\"\"\n**{energy_emoji} JET FUEL SWARM BROADCAST {energy_emoji}**\n\n**Energy Intensity:** {energy_intensity.upper()}\n**Celebration Type:** {celebration_type.upper()}\n**From:** {interaction.user.display_name}\n\n{rendered_content}\n\n**{energy_emoji} UNITED WE THRIVE! {energy_emoji}**\n *We. Are. Swarm.* \n**{energy_emoji} TOGETHER WE CONQUER! {energy_emoji}**\n            \"\"\".strip()\n\n            return {\n                \"success\": True,\n                \"message\": formatted_message,\n                \"template_type\": template_type,\n                \"energy_intensity\": energy_intensity,\n                \"celebration_type\": celebration_type,\n                \"energy_emoji\": energy_emoji,\n                \"rendered_content\": rendered_content,\n                \"message_type\": \"jet_fuel_broadcast\"\n            }\n\n        except Exception as e:",
            "        try:\n            energy_level = self.children[2].value if len(self.children) > 2 else \"high\"\n\n            # Add energy indicators based on level\n            energy_indicators = {\n                \"maximum\": \"\",\n                \"high\": \"\",\n                \"medium\": \"\",\n                \"low\": \"\"\n            }\n\n            energy_emoji = energy_indicators.get(energy_level.lower(), energy_indicators[\"high\"])\n\n            rendered_content = self._render_template(template_type, custom_content)\n\n            formatted_message = f\"\"\"\n**{energy_emoji} JET FUEL MESSAGE {energy_emoji}**\n\n**Energy Level:** {energy_level.upper()}\n**From:** {interaction.user.display_name}\n\n{rendered_content}\n\n**{energy_emoji} FEEL THE ENERGY! {energy_emoji}**\n *We. Are. Swarm.* \n            \"\"\".strip()\n\n            return {\n                \"success\": True,\n                \"message\": formatted_message,\n                \"template_type\": template_type,\n                \"energy_level\": energy_level,\n                \"energy_emoji\": energy_emoji,\n                \"rendered_content\": rendered_content,\n                \"message_type\": \"jet_fuel_message\"\n            }\n\n        except Exception as e:",
            "        try:\n            target_audience = self.children[2].value if len(self.children) > 2 else \"all\"\n\n            rendered_content = self._render_template(template_type, custom_content)\n\n            formatted_message = f\"\"\"\n** Template Broadcast**\n\n**Template:** {template_type}\n**Target:** {target_audience}\n**From:** {interaction.user.display_name}\n\n{rendered_content}\n\n *We. Are. Swarm.* \n            \"\"\".strip()\n\n            return {\n                \"success\": True,\n                \"message\": formatted_message,\n                \"template_type\": template_type,\n                \"target_audience\": target_audience,\n                \"rendered_content\": rendered_content,\n                \"message_type\": \"template_broadcast\"\n            }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\unified_discord_bot.py",
          "count": 4,
          "lines": [
            "        try:\n            # Get Discord token from environment\n            token = os.getenv('DISCORD_BOT_TOKEN')\n            if not token:\n                print(\" DISCORD_BOT_TOKEN not found in environment variables\")\n                print(\"   Please set your Discord bot token: export DISCORD_BOT_TOKEN=your_token_here\")\n                return\n\n            # Create bot instance\n            bot = UnifiedDiscordBot(token)\n\n            # Start the bot\n            print(\" Starting unified Discord bot...\")\n            await bot.start(token)\n\n        except KeyboardInterrupt:",
            "        try:\n            # Test token validity by attempting a minimal connection\n            # This will fail with PrivilegedIntentsRequired if intents aren't enabled\n            await self.login(self.token)\n\n            # If login succeeds, logout immediately (dry-run success)\n            await self.close()\n            self.logger.info(\" Discord connection validation successful (dry-run)\")\n            return True\n\n        except discord.PrivilegedIntentsRequired as e:",
            "try:\n    from dotenv import load_dotenv\n    # Load main .env file first\n    load_dotenv()\n    # Then load Discord-specific configuration\n    load_dotenv('.env.discord')\n    print(\" Environment variables loaded from .env and .env.discord\")\nexcept ImportError:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\consolidated_discord_bot.py",
          "count": 1,
          "lines": [
            "    try:\n        from src.discord_commander.unified_discord_bot import main as discord_main\n        discord_main()\n    except ImportError as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\discord_channel_messenger.py",
          "count": 1,
          "lines": [
            "            try:\n                response = requests.post(\n                    webhook_url,\n                    json=payload,\n                    timeout=self.timeout,\n                    headers={'Content-Type': 'application/json'}\n                )\n\n                if response.status_code == 204:  # Discord success response\n                    logger.info(f\"Message posted successfully to webhook\")\n                    return True\n                elif response.status_code == 429:  # Rate limited\n                    retry_after = int(response.headers.get('Retry-After', 5))\n                    logger.warning(f\"Rate limited, retrying in {retry_after}s\")\n                    import time\n                    time.sleep(retry_after)\n                    continue\n                else:\n                    logger.error(f\"Webhook failed: {response.status_code} - {response.text}\")\n                    return False\n\n            except requests.exceptions.RequestException as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\broadcast_controller_view.py",
          "count": 5,
          "lines": [
            "        try:\n            from ...discord_commander.discord_gui_modals import BroadcastMessageModal\n\n            modal = BroadcastMessageModal(self.messaging_service)\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "        try:\n            from ...discord_commander.discord_gui_modals import JetFuelBroadcastModal\n\n            modal = JetFuelBroadcastModal(self.messaging_service)\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "        try:\n            from ...discord_commander.discord_gui_modals import SelectiveBroadcastModal\n\n            modal = SelectiveBroadcastModal(self.messaging_service)\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "        try:\n            from .broadcast_templates_view import BroadcastTemplatesView\n            \n            view = BroadcastTemplatesView(self.messaging_service)\n            embed = view.create_templates_embed()\n            \n            await interaction.response.send_message(embed=embed, view=view, ephemeral=True)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\messaging_controller_view.py",
          "count": 10,
          "lines": [
            "                try:\n                    await interaction.followup.send(\n                        f\" Interaction already processed. Please try selecting an agent again.\",\n                        ephemeral=True\n                    )\n                except Exception as followup_error:",
            "            try:\n                return int(points_value.replace(\",\", \"\").replace(\"pts\", \"\").strip())\n            except:",
            "        try:\n            # Use async version to avoid blocking event loop\n            self.agents = await self._load_agents_async()\n            self.agent_select.options = self._create_agent_options()\n\n            await interaction.response.send_message(\" Agent list refreshed with latest status!\", ephemeral=True)\n        except Exception as e:",
            "        try:\n            from ...discord_commander.controllers.status_controller_view import StatusControllerView\n\n            view = StatusControllerView(self.messaging_service)\n            embed = view._create_status_embed()\n\n            await interaction.response.send_message(embed=embed, view=view, ephemeral=True)\n        except Exception as e:",
            "        try:\n            from ...discord_commander.discord_gui_modals import AgentMessageModal\n\n            agent_id = self.agent_select.values[0]\n            modal = AgentMessageModal(agent_id, self.messaging_service)\n            \n            await interaction.response.send_modal(modal)\n        except discord.errors.HTTPException as e:",
            "        try:\n            from ...discord_commander.discord_gui_modals import BroadcastMessageModal\n\n            modal = BroadcastMessageModal(self.messaging_service)\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "        try:\n            from ...discord_commander.discord_gui_modals import JetFuelMessageModal\n\n            modal = JetFuelMessageModal(self.messaging_service)\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "        try:\n            status_reader = StatusReader()\n            all_statuses = await status_reader.read_all_statuses_async()\n            agents = []\n\n            for i in range(1, 9):\n                agent_id = f\"Agent-{i}\"\n                status_data = all_statuses.get(agent_id, {})\n\n                agents.append(\n                    {\n                        \"id\": agent_id,\n                        \"name\": status_data.get(\"agent_name\", f\"Agent-{i}\"),\n                        \"status\": status_data.get(\"status\", \"unknown\"),\n                        \"points\": self._extract_points(status_data.get(\"points_earned\", 0)),\n                        \"mission\": status_data.get(\"current_mission\", \"No mission\")[:50],\n                    }\n                )\n\n            return agents\n        except Exception as e:",
            "        try:\n            status_reader = StatusReader()\n            all_statuses = status_reader.read_all_statuses()\n            agents = []\n\n            for i in range(1, 9):\n                agent_id = f\"Agent-{i}\"\n                status_data = all_statuses.get(agent_id, {})\n\n                agents.append(\n                    {\n                        \"id\": agent_id,\n                        \"name\": status_data.get(\"agent_name\", f\"Agent-{i}\"),\n                        \"status\": status_data.get(\"status\", \"unknown\"),\n                        \"points\": self._extract_points(status_data.get(\"points_earned\", 0)),\n                        \"mission\": status_data.get(\"current_mission\", \"No mission\")[:50],\n                    }\n                )\n\n            return agents\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\status_controller_view.py",
          "count": 12,
          "lines": [
            "            try:\n                if not interaction.response.is_done():\n                    await interaction.response.send_message(\n                        f\" Error refreshing status: {e}\", ephemeral=True\n                    )\n                else:\n                    await interaction.followup.send(\n                        f\" Error refreshing status: {e}\", ephemeral=True\n                    )\n            except Exception as followup_error:",
            "            try:\n                if not interaction.response.is_done():\n                    await interaction.response.send_message(\n                        f\" Error: {e}\", ephemeral=True\n                    )\n                else:\n                    await interaction.followup.send(\n                        f\" Error: {e}\", ephemeral=True\n                    )\n            except Exception as followup_error:",
            "            try:\n                return int(points_value.replace(\",\", \"\").replace(\"pts\", \"\").strip())\n            except:",
            "        try:\n            all_statuses = self.status_reader.read_all_statuses()\n            agents = []\n\n            for i in range(1, 9):\n                agent_id = f\"Agent-{i}\"\n                status_data = all_statuses.get(agent_id, {})\n\n                agent_status = status_data.get(\"status\", \"unknown\").lower()\n                \n                # Apply filter\n                if filter_status and agent_status != filter_status:\n                    continue\n\n                agents.append(\n                    {\n                        \"id\": agent_id,\n                        \"name\": status_data.get(\"agent_name\", agent_id),\n                        \"status\": agent_status,\n                        \"points\": self._extract_points(status_data.get(\"points_earned\", 0)),\n                        \"mission\": status_data.get(\"current_mission\", \"No mission\")[:50],\n                        \"task\": status_data.get(\"current_tasks\", [\"\"])[0][:40] if status_data.get(\"current_tasks\") else \"None\",\n                    }\n                )\n\n            # Create embed\n            title = \" STATUS CONTROLLER - WOW FACTOR\"\n            if filter_status:\n                title += f\" ({filter_status.upper()} only)\"\n            \n            embed = discord.Embed(\n                title=title,\n                description=\"**Real-Time Swarm Status Monitoring**\\n\\nLive agent status with points and mission tracking.\",\n                color=discord.Color.green(),\n                timestamp=discord.utils.utcnow(),\n            )\n\n            # Add agent status fields\n            for agent in agents:\n                emoji = self._get_status_emoji(agent[\"status\"])\n                embed.add_field(\n                    name=f\"{emoji} {agent['id']}\",\n                    value=(\n                        f\"**{agent['name']}**\\n\"\n                        f\" {agent['points']} pts\\n\"\n                        f\" {agent['mission']}\\n\"\n                        f\" {agent['task']}\"\n                    ),\n                    inline=True,\n                )\n\n            # Add summary\n            active_count = sum(1 for a in agents if a[\"status\"] == \"active\")\n            idle_count = sum(1 for a in agents if a[\"status\"] == \"idle\")\n            total_points = sum(a[\"points\"] for a in agents)\n\n            embed.add_field(\n                name=\" Summary\",\n                value=(\n                    f\"**Active**: {active_count} | **Idle**: {idle_count} | **Total**: {len(agents)}/8\\n\"\n                    f\"**Total Points**: {total_points:,}\"\n                ),\n                inline=False,\n            )\n\n            embed.set_footer(text=\" WE. ARE. SWARM.  Live Status Monitoring\")\n            return embed\n\n        except Exception as e:",
            "        try:\n            all_statuses = self.status_reader.read_all_statuses()\n            idle_agents = []\n\n            for agent_id in [f\"Agent-{i}\" for i in range(1, 9)]:\n                status_data = all_statuses.get(agent_id, {})\n                status = status_data.get(\"status\", \"\").lower()\n                \n                if status == \"idle\" or not status_data.get(\"current_task\"):\n                    idle_agents.append(agent_id)\n\n            if not idle_agents:\n                await interaction.response.send_message(\n                    \" No idle agents found! All agents are active!\", ephemeral=True\n                )\n                return\n\n            # Open broadcast modal for idle agents\n            from ...discord_commander.discord_gui_modals import SelectiveBroadcastModal\n\n            modal = SelectiveBroadcastModal(self.messaging_service, default_agents=idle_agents)\n            await interaction.response.send_modal(modal)\n\n        except Exception as e:",
            "        try:\n            bot = interaction.client\n            if hasattr(bot, \"status_monitor\"):\n                bot.status_monitor.start_monitoring()\n                await interaction.response.send_message(\n                    \" Status monitor started (15s interval).\", ephemeral=True\n                )\n            else:\n                await interaction.response.send_message(\n                    \" Status monitor not initialized yet.\", ephemeral=True\n                )\n        except Exception as e:",
            "        try:\n            bot = interaction.client\n            if hasattr(bot, \"status_monitor\"):\n                bot.status_monitor.stop_monitoring()\n                await interaction.response.send_message(\n                    \" Status monitor stopped.\", ephemeral=True\n                )\n            else:\n                await interaction.response.send_message(\n                    \" Status monitor not initialized.\", ephemeral=True\n                )\n        except Exception as e:",
            "        try:\n            embed = self._create_status_embed()\n            await interaction.response.edit_message(embed=embed, view=self)\n        except Exception as e:",
            "        try:\n            embed = self._create_status_embed(filter_status=\"active\")\n            await interaction.response.edit_message(embed=embed, view=self)\n        except Exception as e:",
            "        try:\n            embed = self._create_status_embed(filter_status=\"idle\")\n            await interaction.response.edit_message(embed=embed, view=self)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\swarm_tasks_controller_view.py",
          "count": 10,
          "lines": [
            "            try:\n                with open(status_file, \"r\", encoding=\"utf-8\") as f:\n                    status = json.load(f)\n                    agents.append(status)\n            except Exception as e:",
            "        try:\n            # Update button states\n            self.prev_btn.disabled = self.current_page == 0\n            self.next_btn.disabled = self.current_page >= len(embeds) - 1\n\n            # Send or edit message\n            if not interaction.response.is_done():\n                await interaction.response.send_message(\n                    embed=embeds[self.current_page], view=self, ephemeral=True\n                )\n            else:\n                await interaction.edit_original_response(\n                    embed=embeds[self.current_page], view=self\n                )\n        except Exception as e:",
            "        try:\n            embeds = self._create_tasks_embeds()\n            if self.current_page < len(embeds) - 1:\n                self.current_page += 1\n                await self._update_message(interaction, embeds)\n        except Exception as e:",
            "        try:\n            if not interaction.response.is_done():\n                await interaction.response.send_message(message, ephemeral=True)\n            else:\n                await interaction.followup.send(message, ephemeral=True)\n        except Exception as followup_error:",
            "        try:\n            if self.current_page > 0:\n                self.current_page -= 1\n                embeds = self._create_tasks_embeds()\n                await self._update_message(interaction, embeds)\n        except Exception as e:",
            "        try:\n            self.current_page = 0\n            embeds = self._create_tasks_embeds()\n            await self._update_message(interaction, embeds)\n        except Exception as e:",
            "        try:\n            self.filter_priority = \"CRITICAL\"\n            self.current_page = 0\n            embeds = self._create_tasks_embeds()\n            await self._update_message(interaction, embeds)\n        except Exception as e:",
            "        try:\n            self.filter_priority = \"HIGH\"\n            self.current_page = 0\n            embeds = self._create_tasks_embeds()\n            await self._update_message(interaction, embeds)\n        except Exception as e:",
            "        try:\n            self.filter_priority = None\n            self.current_page = 0\n            embeds = self._create_tasks_embeds()\n            await self._update_message(interaction, embeds)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\controllers\\broadcast_templates_view.py",
          "count": 7,
          "lines": [
            "            try:\n                btn = discord.ui.Button(\n                    label=template.get(\"name\", f\"Template {idx+1}\"),\n                    style=discord.ButtonStyle.primary,\n                    emoji=template.get(\"emoji\"),\n                    custom_id=f\"template_{self.current_mode}_{idx}\",\n                    row=target_row,\n                )\n                btn.callback = lambda i, t=template: self.on_template_select(i, t)\n                self.add_item(btn)\n            except Exception as e:",
            "            try:\n                if not interaction.response.is_done():\n                    await interaction.response.send_message(\n                        f\" Error: {e}\", ephemeral=True\n                    )\n                else:\n                    await interaction.followup.send(\n                        f\" Error: {e}\", ephemeral=True\n                    )\n            except Exception as followup_error:",
            "        try:\n            from ..discord_gui_modals import TemplateBroadcastModal\n\n            # Create modal with template pre-filled\n            modal = TemplateBroadcastModal(\n                self.messaging_service,\n                template_message=template[\"message\"],\n                template_priority=template[\"priority\"],\n            )\n\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "        try:\n            self.current_mode = mode\n            self._create_template_buttons()\n\n            embed = self.create_templates_embed()\n            await interaction.response.edit_message(embed=embed, view=self)\n        except Exception as e:",
            "    try:\n        from ..discord_template_collection import (\n            ENHANCED_BROADCAST_TEMPLATES,\n        )\n        USE_ENHANCED_TEMPLATES = True\n    except ImportError:",
            "try:\n    from ..templates.broadcast_templates import (\n        ENHANCED_BROADCAST_TEMPLATES,\n    )\n    USE_ENHANCED_TEMPLATES = True\nexcept ImportError:",
            "try:\n    import discord\n    from discord.ext import commands\n\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\utils\\carmyn_preferences.py",
          "count": 4,
          "lines": [
            "    try:\n        PREFERENCES_FILE.parent.mkdir(parents=True, exist_ok=True)\n        prefs[\"last_updated\"] = datetime.now().isoformat()\n        with open(PREFERENCES_FILE, 'w', encoding='utf-8') as f:\n            json.dump(prefs, f, indent=2, ensure_ascii=False)\n        logger.info(f\" Preferences saved to {PREFERENCES_FILE}\")\n        return True\n    except Exception as e:",
            "    try:\n        if PREFERENCES_FILE.exists():\n            with open(PREFERENCES_FILE, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        else:\n            logger.warning(f\"Preferences file not found: {PREFERENCES_FILE}\")\n            return {}\n    except Exception as e:",
            "    try:\n        prefs = load_preferences()\n        if not prefs:\n            return False\n        \n        # Navigate to the key path\n        keys = key_path.split(\".\")\n        current = prefs\n        for key in keys[:-1]:\n            if key not in current:\n                current[key] = {}\n            current = current[key]\n        \n        # Update the value\n        current[keys[-1]] = value\n        prefs[\"preferences_updated_at\"] = datetime.now().isoformat()\n        \n        return save_preferences(prefs)\n    except Exception as e:",
            "    try:\n        prefs = load_preferences()\n        if not prefs:\n            return False\n        \n        # Update interaction history\n        history = prefs.get(\"interaction_history\", {})\n        history[\"total_interactions\"] = history.get(\"total_interactions\", 0) + 1\n        history[\"last_interaction\"] = datetime.now().isoformat()\n        \n        # Add note if provided\n        if interaction_note:\n            notes = history.get(\"notes\", [])\n            notes.append({\n                \"timestamp\": datetime.now().isoformat(),\n                \"note\": interaction_note\n            })\n            # Keep only last 50 notes\n            history[\"notes\"] = notes[-50:]\n        \n        prefs[\"interaction_history\"] = history\n        return save_preferences(prefs)\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\showcase_handlers.py",
          "count": 5,
          "lines": [
            "    try:\n        from ..github_book_viewer import GitHubBookData, GitHubBookNavigator\n        \n        book_data = GitHubBookData()\n        navigator = GitHubBookNavigator(book_data)\n        embed = navigator._create_goldmines_embed()\n        \n        return embed, navigator\n    except Exception as e:",
            "    try:\n        from ..swarm_showcase_commands import SwarmShowcaseCommands\n        \n        showcase = SwarmShowcaseCommands(bot=None)\n        return await showcase._create_excellence_embed()\n    except Exception as e:",
            "    try:\n        from ..swarm_showcase_commands import SwarmShowcaseCommands\n        \n        showcase = SwarmShowcaseCommands(bot=None)\n        return await showcase._create_overview_embed()\n    except Exception as e:",
            "    try:\n        from ..swarm_showcase_commands import SwarmShowcaseCommands\n        \n        showcase = SwarmShowcaseCommands(bot=None)\n        return await showcase._create_roadmap_embed()\n    except Exception as e:",
            "try:\n    import discord\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\agent_messaging_view.py",
          "count": 7,
          "lines": [
            "        try:\n            all_statuses = status_reader.read_all_statuses()\n            agents = []\n\n            for i in range(1, 9):\n                agent_id = f\"Agent-{i}\"\n                status_data = all_statuses.get(agent_id, {})\n                agents.append(\n                    {\n                        \"id\": agent_id,\n                        \"name\": status_data.get(\"agent_name\", agent_id),\n                        \"status\": status_data.get(\"status\", \"unknown\"),\n                        \"points\": self._extract_points(status_data.get(\"points_earned\", 0)),\n                    }\n                )\n\n            for agent in agents:\n                emoji = self._get_status_emoji(agent.get(\"status\", \"unknown\"))\n                embed.add_field(\n                    name=f\"{emoji} {agent['id']}\",\n                    value=f\"{agent.get('name', 'Unknown')}\\n{agent.get('points', 0)} points\",\n                    inline=True,\n                )\n\n            # CRITICAL FIX: Properly detect ACTIVE status\n            # Check for ACTIVE_AGENT_MODE, ACTIVE, JET_FUEL, etc.\n            active_count = sum(\n                1 for a in agents \n                if \"ACTIVE\" in str(a.get(\"status\", \"\")).upper() \n                or \"JET_FUEL\" in str(a.get(\"status\", \"\")).upper()\n                or \"ACTIVE_AGENT_MODE\" in str(a.get(\"status\", \"\")).upper()\n            )\n            embed.add_field(\n                name=\" Summary\",\n                value=f\"Active: {active_count}/{len(agents)} agents\",\n                inline=False,\n            )\n\n        except Exception as e:",
            "        try:\n            from ..discord_gui_modals import AgentMessageModal\n\n            agent_id = self.agent_select.values[0]\n            modal = AgentMessageModal(agent_id, self.messaging_service)\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "        try:\n            from ..discord_gui_modals import BroadcastMessageModal\n\n            modal = BroadcastMessageModal(self.messaging_service)\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "        try:\n            from .swarm_status_view import SwarmStatusGUIView\n\n            status_view = SwarmStatusGUIView(self.messaging_service)\n\n            embed = discord.Embed(\n                title=\" Swarm Status\",\n                description=\"Current agent status across the swarm\",\n                color=discord.Color.blue(),\n                timestamp=discord.utils.utcnow(),\n            )\n\n            for agent in self.agents:\n                emoji = self._get_status_emoji(agent.get(\"status\", \"unknown\"))\n                embed.add_field(\n                    name=f\"{emoji} {agent['id']}\",\n                    value=f\"{agent.get('name', 'Unknown')}\\n{agent.get('points', 0)} points\",\n                    inline=True,\n                )\n\n            await interaction.response.send_message(embed=embed, view=status_view, ephemeral=True)\n        except Exception as e:",
            "        try:\n            status_reader = StatusReader()\n            all_statuses = status_reader.read_all_statuses()\n            agents = []\n\n            # Always include all 8 agents (even if status not available)\n            for i in range(1, 9):\n                agent_id = f\"Agent-{i}\"\n                status_data = all_statuses.get(agent_id, {})\n\n                agents.append(\n                    {\n                        \"id\": agent_id,\n                        \"name\": status_data.get(\"agent_name\", f\"Agent-{i}\"),\n                        \"status\": status_data.get(\"status\", \"unknown\"),\n                        \"points\": self._extract_points(status_data.get(\"points_earned\", 0)),\n                    }\n                )\n\n            return agents\n        except Exception as e:",
            "        try:\n            status_reader = StatusReader()\n            status_reader.clear_cache()\n            \n            self.agents = self._load_agents()\n            self.agent_select.options = self._create_agent_options()\n\n            await interaction.response.send_message(\" Agent list refreshed!\", ephemeral=True)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\aria_message_agent8_modal.py",
          "count": 6,
          "lines": [
            "        try:\n            # Use inbox utility for reliable delivery\n            from src.utils.inbox_utility import create_inbox_message\n            \n            success = create_inbox_message(\n                recipient=\"Agent-8\",\n                sender=\"Aria\",\n                content=full_message,\n                priority=\"normal\",\n                message_type=\"text\",\n                tags=[\"aria\", \"preferences\", \"self-improving\"]\n            )\n            \n            if success:\n                logger.info(\" Message written to Agent-8 inbox via inbox utility\")\n            else:\n                logger.warning(\" Inbox utility failed, trying direct write...\")\n                # Fallback to direct write\n                return self._write_to_inbox_direct(full_message)\n            \n            return success\n        \n        except ImportError:",
            "        try:\n            from datetime import datetime\n            import uuid\n            \n            project_root = Path(__file__).parent.parent.parent.parent\n            inbox_dir = project_root / \"agent_workspaces\" / \"Agent-8\" / \"inbox\"\n            inbox_dir.mkdir(parents=True, exist_ok=True)\n            \n            # Generate message filename\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n            message_id = str(uuid.uuid4())[:8]\n            filename = f\"ARIA_MESSAGE_{timestamp}_{message_id}.md\"\n            inbox_file = inbox_dir / filename\n            \n            # Format message with headers\n            formatted_message = f\"\"\"#  ARIA MESSAGE - TEXT\n\n**From**: Aria\n**To**: Agent-8\n**Priority**: normal\n**Message ID**: {message_id}\n**Timestamp**: {datetime.now().isoformat()}\n\n---\n\n{full_message}\n\n---\n*Message delivered via Aria's personalized !aria command*\"\"\"\n            \n            # Write to inbox\n            inbox_file.write_text(formatted_message, encoding='utf-8')\n            logger.info(f\" Message written to Agent-8 inbox (direct): {inbox_file}\")\n            \n            return True\n        \n        except Exception as e:",
            "        try:\n            history = self.preferences.get(\"interaction_history\", {})\n            history[\"total_interactions\"] = history.get(\"total_interactions\", 0) + 1\n            history[\"last_interaction\"] = str(Path(__file__).parent.parent.parent.parent / \"agent_workspaces\" / \"Agent-8\" / \"aria_preferences.json\")\n            \n            # Extract topics from message (simple keyword detection)\n            topics = []\n            message_lower = message.lower()\n            if \"game\" in message_lower or \"gaming\" in message_lower:\n                topics.append(\"gaming\")\n            if \"wordpress\" in message_lower or \"theme\" in message_lower:\n                topics.append(\"wordpress\")\n            if \"code\" in message_lower or \"coding\" in message_lower:\n                topics.append(\"coding\")\n            if \"design\" in message_lower:\n                topics.append(\"design\")\n            \n            if topics:\n                existing_topics = history.get(\"topics_discussed\", [])\n                for topic in topics:\n                    if topic not in existing_topics:\n                        existing_topics.append(topic)\n                history[\"topics_discussed\"] = existing_topics\n            \n            self.preferences[\"interaction_history\"] = history\n            \n            # Save updated preferences\n            self.preferences_path.parent.mkdir(parents=True, exist_ok=True)\n            self.preferences_path.write_text(\n                json.dumps(self.preferences, indent=2),\n                encoding='utf-8'\n            )\n            \n            logger.info(f\"Updated Aria's interaction history: {history['total_interactions']} interactions\")\n        \n        except Exception as e:",
            "        try:\n            if self.preferences_path.exists():\n                return json.loads(self.preferences_path.read_text(encoding='utf-8'))\n        except Exception as e:",
            "        try:\n            user_message = self.message_input.value\n            \n            # Build message with preferences\n            full_message = self._build_message_template(user_message)\n            \n            # Write directly to Agent-8's inbox (file-based messaging)\n            success = self._write_to_inbox(full_message, user_message)\n            \n            if success:\n                # Update interaction history\n                self._update_interaction_history(user_message)\n                \n                # Send confirmation\n                embed = discord.Embed(\n                    title=\" Message Sent!\",\n                    description=f\"Your message has been sent to Agent-8's inbox!\",\n                    color=0x00FF00  # Green\n                )\n                embed.add_field(\n                    name=\" Message Preview\",\n                    value=user_message[:500] + (\"...\" if len(user_message) > 500 else \"\"),\n                    inline=False\n                )\n                embed.add_field(\n                    name=\" Note\",\n                    value=\"Agent-8 will see your message with your preferences included!\",\n                    inline=False\n                )\n                embed.set_footer(text=\"Preferences are being updated based on this interaction! \")\n                \n                await interaction.response.send_message(embed=embed, ephemeral=True)\n            else:\n                await interaction.response.send_message(\n                    f\" Error writing message to inbox. Check logs for details.\",\n                    ephemeral=True\n                )\n        \n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\aria_profile_view.py",
          "count": 3,
          "lines": [
            "        try:\n            await create_aria_profile_command(interaction)\n        except Exception as e:",
            "        try:\n            from .aria_message_agent8_modal import AriaMessageAgent8Modal\n            \n            modal = AriaMessageAgent8Modal()\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\bump_agent_view.py",
          "count": 3,
          "lines": [
            "        try:\n            from tools.agent_bump_script import bump_agents_by_number\n            \n            all_agents = list(range(1, 9))\n            results = bump_agents_by_number(all_agents)\n            \n            # Create result embed\n            success_count = sum(1 for r in results.values() if r)\n            total_count = len(results)\n            \n            if success_count == total_count:\n                color = discord.Color.green()\n                title = f\" Successfully Bumped All {success_count} Agents\"\n            elif success_count > 0:\n                color = discord.Color.orange()\n                title = f\" Partially Successful: {success_count}/{total_count}\"\n            else:\n                color = discord.Color.red()\n                title = f\" Failed to Bump Agents\"\n            \n            embed = discord.Embed(\n                title=title,\n                description=f\"Bumped {success_count}/{total_count} agent(s)\",\n                color=color,\n            )\n            \n            # Add results for each agent\n            for agent_id, success in results.items():\n                status = \"\" if success else \"\"\n                embed.add_field(\n                    name=f\"{status} {agent_id}\",\n                    value=\"Success\" if success else \"Failed\",\n                    inline=True,\n                )\n            \n            embed.set_footer(text=\"Agent Bump Script | Click + Shift+Backspace\")\n            await interaction.followup.send(embed=embed)\n            \n        except Exception as e:",
            "        try:\n            from tools.agent_bump_script import bump_agents_by_number\n            \n            results = bump_agents_by_number(self.selected_agents)\n            \n            # Create result embed\n            success_count = sum(1 for r in results.values() if r)\n            total_count = len(results)\n            \n            if success_count == total_count:\n                color = discord.Color.green()\n                title = f\" Successfully Bumped {success_count} Agent(s)\"\n            elif success_count > 0:\n                color = discord.Color.orange()\n                title = f\" Partially Successful: {success_count}/{total_count}\"\n            else:\n                color = discord.Color.red()\n                title = f\" Failed to Bump Agents\"\n            \n            embed = discord.Embed(\n                title=title,\n                description=f\"Bumped {success_count}/{total_count} agent(s)\",\n                color=color,\n            )\n            \n            # Add results for each agent\n            for agent_id, success in results.items():\n                status = \"\" if success else \"\"\n                embed.add_field(\n                    name=f\"{status} {agent_id}\",\n                    value=\"Success\" if success else \"Failed\",\n                    inline=True,\n                )\n            \n            embed.set_footer(text=\"Agent Bump Script | Click + Shift+Backspace\")\n            await interaction.followup.send(embed=embed)\n            \n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_message_agent7_modal.py",
          "count": 4,
          "lines": [
            "        try:\n            history = self.preferences.get(\"interaction_history\", {})\n            history[\"total_interactions\"] = history.get(\"total_interactions\", 0) + 1\n            history[\"last_interaction\"] = datetime.now().isoformat()\n            \n            # Extract topics from message (simple keyword detection)\n            topics = []\n            message_lower = message.lower()\n            if \"music\" in message_lower or \"dj\" in message_lower or \"mix\" in message_lower:\n                topics.append(\"music\")\n            if \"website\" in message_lower or \"site\" in message_lower or \"web\" in message_lower:\n                topics.append(\"website\")\n            if \"r&b\" in message_lower or \"rnb\" in message_lower:\n                topics.append(\"r&b\")\n            if \"dance\" in message_lower:\n                topics.append(\"dance\")\n            if \"jazz\" in message_lower:\n                topics.append(\"jazz\")\n            \n            if topics:\n                notes = history.get(\"notes\", [])\n                notes.append({\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"topics\": topics,\n                    \"note\": f\"Discussed: {', '.join(topics)}\"\n                })\n                # Keep only last 50 notes\n                history[\"notes\"] = notes[-50:]\n            \n            self.preferences[\"interaction_history\"] = history\n            \n            # Save updated preferences\n            self.preferences_path.parent.mkdir(parents=True, exist_ok=True)\n            self.preferences_path.write_text(\n                json.dumps(self.preferences, indent=2, ensure_ascii=False),\n                encoding='utf-8'\n            )\n            \n            logger.info(f\"Updated Carmyn's interaction history: {history['total_interactions']} interactions\")\n        \n        except Exception as e:",
            "        try:\n            if self.preferences_path.exists():\n                return json.loads(self.preferences_path.read_text(encoding='utf-8'))\n        except Exception as e:",
            "        try:\n            user_message = self.message_input.value\n            \n            # Build message with preferences\n            full_message = self._build_message_template(user_message)\n            \n            # Send to Agent-7 via inbox utility\n            try:\n                from src.utils.inbox_utility import create_inbox_message\n                \n                success = create_inbox_message(\n                    recipient=\"Agent-7\",\n                    sender=\"Carmyn (via !carmyn command)\",\n                    content=full_message,\n                    priority=\"normal\",\n                    message_type=\"carmyn_to_agent\",\n                    tags=[\"carmyn\", \"personalized\", \"preferences\"]\n                )\n                \n                if success:\n                    # Update interaction history\n                    self._update_interaction_history(user_message)\n                    \n                    # Send confirmation\n                    embed = discord.Embed(\n                        title=\" Message Sent!\",\n                        description=f\"Your message has been sent to Agent-7!\",\n                        color=0xFF69B4  # Hot pink\n                    )\n                    embed.add_field(\n                        name=\" Message Preview\",\n                        value=user_message[:500] + (\"...\" if len(user_message) > 500 else \"\"),\n                        inline=False\n                    )\n                    embed.add_field(\n                        name=\" Note\",\n                        value=\"Agent-7 will respond with your preferences in mind!\",\n                        inline=False\n                    )\n                    embed.add_field(\n                        name=\" Self-Improving System\",\n                        value=\"Your preferences are being updated based on this interaction! \",\n                        inline=False\n                    )\n                    embed.set_footer(text=\"Keep creating amazing music, Carmyn! \")\n                    \n                    await interaction.response.send_message(embed=embed, ephemeral=True)\n                else:\n                    await interaction.response.send_message(\n                        \" Error sending message. Please try again later.\",\n                        ephemeral=True\n                    )\n            except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_message_agent8_modal.py",
          "count": 6,
          "lines": [
            "        try:\n            # Use inbox utility for reliable delivery\n            from src.utils.inbox_utility import create_inbox_message\n            \n            success = create_inbox_message(\n                recipient=\"Agent-8\",\n                sender=\"Carmyn\",\n                content=full_message,\n                priority=\"normal\",\n                message_type=\"text\",\n                tags=[\"carmyn\", \"preferences\", \"self-improving\"]\n            )\n            \n            if success:\n                logger.info(\" Message written to Agent-8 inbox via inbox utility\")\n            else:\n                logger.warning(\" Inbox utility failed, trying direct write...\")\n                # Fallback to direct write\n                return self._write_to_inbox_direct(full_message)\n            \n            return success\n        \n        except ImportError:",
            "        try:\n            from datetime import datetime\n            import uuid\n            \n            project_root = Path(__file__).parent.parent.parent.parent\n            inbox_dir = project_root / \"agent_workspaces\" / \"Agent-8\" / \"inbox\"\n            inbox_dir.mkdir(parents=True, exist_ok=True)\n            \n            # Generate message filename\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n            message_id = str(uuid.uuid4())[:8]\n            filename = f\"CARMYN_MESSAGE_{timestamp}_{message_id}.md\"\n            inbox_file = inbox_dir / filename\n            \n            # Format message with headers\n            formatted_message = f\"\"\"#  CARMYN MESSAGE - TEXT\n\n**From**: Carmyn\n**To**: Agent-8\n**Priority**: normal\n**Message ID**: {message_id}\n**Timestamp**: {datetime.now().isoformat()}\n\n---\n\n{full_message}\n\n---\n*Message delivered via Carmyn's personalized !carmyn command*\"\"\"\n            \n            # Write to inbox\n            inbox_file.write_text(formatted_message, encoding='utf-8')\n            logger.info(f\" Message written to Agent-8 inbox (direct): {inbox_file}\")\n            \n            return True\n        \n        except Exception as e:",
            "        try:\n            history = self.preferences.get(\"interaction_history\", {})\n            history[\"total_interactions\"] = history.get(\"total_interactions\", 0) + 1\n            history[\"last_interaction\"] = str(Path(__file__).parent.parent.parent.parent / \"agent_workspaces\" / \"Agent-8\" / \"carmyn_preferences.json\")\n            \n            # Extract topics from message (simple keyword detection)\n            topics = []\n            message_lower = message.lower()\n            if \"music\" in message_lower or \"dj\" in message_lower:\n                topics.append(\"music\")\n            if \"r&b\" in message_lower or \"rb\" in message_lower:\n                topics.append(\"r&b\")\n            if \"dance\" in message_lower:\n                topics.append(\"dance\")\n            if \"jazz\" in message_lower:\n                topics.append(\"jazz\")\n            if \"website\" in message_lower or \"site\" in message_lower:\n                topics.append(\"website\")\n            if \"code\" in message_lower or \"coding\" in message_lower:\n                topics.append(\"coding\")\n            \n            if topics:\n                existing_topics = history.get(\"topics_discussed\", [])\n                for topic in topics:\n                    if topic not in existing_topics:\n                        existing_topics.append(topic)\n                history[\"topics_discussed\"] = existing_topics\n            \n            self.preferences[\"interaction_history\"] = history\n            \n            # Save updated preferences\n            self.preferences_path.parent.mkdir(parents=True, exist_ok=True)\n            self.preferences_path.write_text(\n                json.dumps(self.preferences, indent=2),\n                encoding='utf-8'\n            )\n            \n            logger.info(f\"Updated Carmyn's interaction history: {history['total_interactions']} interactions\")\n        \n        except Exception as e:",
            "        try:\n            if self.preferences_path.exists():\n                return json.loads(self.preferences_path.read_text(encoding='utf-8'))\n        except Exception as e:",
            "        try:\n            user_message = self.message_input.value\n            \n            # Build message with preferences\n            full_message = self._build_message_template(user_message)\n            \n            # Write directly to Agent-8's inbox (file-based messaging)\n            success = self._write_to_inbox(full_message, user_message)\n            \n            if success:\n                # Update interaction history\n                self._update_interaction_history(user_message)\n                \n                # Send confirmation\n                embed = discord.Embed(\n                    title=\" Message Sent!\",\n                    description=f\"Your message has been sent to Agent-8's inbox!\",\n                    color=0x00FF00  # Green\n                )\n                embed.add_field(\n                    name=\" Message Preview\",\n                    value=user_message[:500] + (\"...\" if len(user_message) > 500 else \"\"),\n                    inline=False\n                )\n                embed.add_field(\n                    name=\" Note\",\n                    value=\"Agent-8 will see your message with your preferences included!\",\n                    inline=False\n                )\n                embed.set_footer(text=\"Preferences are being updated based on this interaction! \")\n                \n                await interaction.response.send_message(embed=embed, ephemeral=True)\n            else:\n                await interaction.response.send_message(\n                    f\" Error writing message to inbox. Check logs for details.\",\n                    ephemeral=True\n                )\n        \n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\carmyn_profile_view.py",
          "count": 2,
          "lines": [
            "        try:\n            from .carmyn_message_agent8_modal import CarmynMessageAgent8Modal\n            \n            modal = CarmynMessageAgent8Modal()\n            await interaction.response.send_modal(modal)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\confirm_restart_view.py",
          "count": 2,
          "lines": [
            "        try:\n            self.confirmed = False\n            await interaction.response.send_message(\" Cancelled\", ephemeral=True)\n            self.stop()\n        except Exception as e:",
            "        try:\n            self.confirmed = True\n            await interaction.response.send_message(\" Restart confirmed\", ephemeral=True)\n            self.stop()\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\confirm_shutdown_view.py",
          "count": 2,
          "lines": [
            "        try:\n            self.confirmed = False\n            await interaction.response.send_message(\" Cancelled\", ephemeral=True)\n            self.stop()\n        except Exception as e:",
            "        try:\n            self.confirmed = True\n            await interaction.response.send_message(\" Shutdown confirmed\", ephemeral=True)\n            self.stop()\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\help_view.py",
          "count": 7,
          "lines": [
            "        try:\n            embed = self._create_github_embed()\n            await interaction.response.edit_message(embed=embed, view=self)\n            self.current_page = \"github\"\n        except Exception as e:",
            "        try:\n            embed = self._create_gui_embed()\n            await interaction.response.edit_message(embed=embed, view=self)\n            self.current_page = \"gui\"\n        except Exception as e:",
            "        try:\n            embed = self._create_main_embed()\n            await interaction.response.edit_message(embed=embed, view=self)\n            self.current_page = \"main\"\n        except Exception as e:",
            "        try:\n            embed = self._create_messaging_embed()\n            await interaction.response.edit_message(embed=embed, view=self)\n            self.current_page = \"messaging\"\n        except Exception as e:",
            "        try:\n            embed = self._create_swarm_embed()\n            await interaction.response.edit_message(embed=embed, view=self)\n            self.current_page = \"swarm\"\n        except Exception as e:",
            "        try:\n            if not interaction.response.is_done():\n                await interaction.response.send_message(\n                    f\" Error: {error}\", ephemeral=True\n                )\n            else:\n                await interaction.followup.send(\n                    f\" Error: {error}\", ephemeral=True\n                )\n        except Exception as followup_error:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\swarm_snapshot_view.py",
          "count": 4,
          "lines": [
            "        try:\n            active_agents = self.snapshot.get(\"active_agents\", [])\n            \n            if not active_agents:\n                await interaction.response.send_message(\n                    \" No active agents at this time.\", ephemeral=True\n                )\n                return\n            \n            # Create detailed embed\n            embed = discord.Embed(\n                title=\" Detailed Swarm Status\",\n                description=f\"**{len(active_agents)} Active Agents**\",\n                color=0x3498DB,\n            )\n            \n            for agent in active_agents:\n                priority_emoji = (\n                    \"\" if agent[\"priority\"] == \"HIGH\"\n                    else \"\" if agent[\"priority\"] == \"MEDIUM\"\n                    else \"\"\n                )\n                embed.add_field(\n                    name=f\"{priority_emoji} {agent['id']}\",\n                    value=(\n                        f\"**Phase:** {agent['phase']}\\n\"\n                        f\"**Priority:** {agent['priority']}\\n\"\n                        f\"**Mission:** {agent['mission']}\"\n                    ),\n                    inline=True,\n                )\n            \n            await interaction.response.send_message(embed=embed, ephemeral=True)\n        except Exception as e:",
            "        try:\n            await interaction.response.defer()\n            \n            # Re-read snapshot\n            snapshot = self._get_swarm_snapshot()\n            self.snapshot = snapshot\n            \n            # Update embed\n            embed = self.create_snapshot_embed()\n            await interaction.followup.send(embed=embed, view=self, ephemeral=True)\n        except Exception as e:",
            "        try:\n            workspace_root = Path(\"agent_workspaces\")\n            active_count = 0\n            total_agents = 8\n            \n            for i in range(1, 9):\n                agent_id = f\"Agent-{i}\"\n                status_file = workspace_root / agent_id / \"status.json\"\n                \n                if not status_file.exists():\n                    continue\n                \n                try:\n                    with open(status_file, 'r', encoding='utf-8') as f:\n                        status = json.load(f)\n                    \n                    agent_status = status.get(\"status\", \"\")\n                    if \"ACTIVE\" in agent_status.upper():\n                        active_count += 1\n                        mission = status.get(\"current_mission\", \"No active mission\")[:80]\n                        phase = status.get(\"current_phase\", \"Unknown\")\n                        priority = status.get(\"mission_priority\", \"MEDIUM\")\n                        \n                        snapshot[\"active_agents\"].append({\n                            \"id\": agent_id,\n                            \"mission\": mission,\n                            \"phase\": phase,\n                            \"priority\": priority,\n                        })\n                        \n                        completed = status.get(\"completed_tasks\", [])\n                        if completed:\n                            recent = completed[0][:100] if isinstance(completed[0], str) else str(completed[0])[:100]\n                            snapshot[\"recent_activity\"].append(f\"{agent_id}: {recent}\")\n                        \n                        current_tasks = status.get(\"current_tasks\", [])\n                        if current_tasks:\n                            focus = current_tasks[0][:80] if isinstance(current_tasks[0], str) else str(current_tasks[0])[:80]\n                            snapshot[\"current_focus\"].append(f\"{agent_id}: {focus}\")\n                \n                except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\swarm_status_view.py",
          "count": 3,
          "lines": [
            "            try:\n                if not interaction.response.is_done():\n                    await interaction.response.send_message(\n                        f\" Error refreshing status: {e}\", ephemeral=True\n                    )\n                else:\n                    await interaction.followup.send(\n                        f\" Error refreshing status: {e}\", ephemeral=True\n                    )\n            except Exception as followup_error:",
            "        try:\n            status_reader = StatusReader()\n            # Clear cache before refreshing to get latest status\n            status_reader.clear_cache()\n            agents = []\n\n            for i in range(1, 9):\n                agent_id = f\"Agent-{i}\"\n                status = status_reader.read_agent_status(agent_id)\n                if status:\n                    # Extract points properly - StatusReader normalizes to \"points\" field\n                    points = status.get(\"points\", 0)\n                    if not isinstance(points, (int, float)):\n                        points = 0\n\n                    agents.append(\n                        {\n                            \"id\": agent_id,\n                            \"name\": status.get(\"agent_name\", agent_id),\n                            \"status\": status.get(\"status\", \"unknown\"),\n                            \"points\": int(points),\n                        }\n                    )\n\n            embed = discord.Embed(\n                title=\" Swarm Status (Refreshed)\",\n                description=\"Updated agent status\",\n                color=discord.Color.green(),\n                timestamp=discord.utils.utcnow(),\n            )\n\n            for agent in agents:\n                # Check if status contains \"active\" (case-insensitive)\n                status_upper = agent[\"status\"].upper()\n                if \"ACTIVE\" in status_upper or \"JET_FUEL\" in status_upper:\n                    emoji = \"\"\n                elif \"COMPLETE\" in status_upper or \"COMPLETED\" in status_upper:\n                    emoji = \"\"\n                elif \"REST\" in status_upper or \"STANDBY\" in status_upper:\n                    emoji = \"\"\n                elif \"ERROR\" in status_upper or \"FAILED\" in status_upper:\n                    emoji = \"\"\n                else:\n                    emoji = \"\"\n                embed.add_field(\n                    name=f\"{emoji} {agent['id']}\",\n                    value=f\"{agent['name']}\\n{agent['points']} points\",\n                    inline=True,\n                )\n\n            await interaction.response.edit_message(embed=embed, view=self)\n\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\unstall_agent_view.py",
          "count": 2,
          "lines": [
            "        try:\n            status_file = Path(f\"agent_workspaces/{agent_id}/status.json\")\n            last_state = \"Unknown\"\n            if status_file.exists():\n                try:\n                    status_data = json.loads(status_file.read_text(encoding=\"utf-8\"))\n                    last_state = status_data.get(\"current_mission\", \"Unknown\")\n                except Exception:",
            "try:\n    import discord\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\views\\main_control_panel_view.py",
          "count": 2,
          "lines": [
            "        try:\n            await interaction.response.send_message(embed=embed, ephemeral=True)\n        except discord.InteractionResponded:",
            "try:\n    import discord\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\core_messaging_commands.py",
          "count": 5,
          "lines": [
            "        try:\n            # Force refresh if requested\n            if args.lower() == \"refresh\":\n                from ..status_reader import StatusReader\n                status_reader = StatusReader()\n                status_reader.clear_cache()\n                await ctx.send(\" Status cache cleared - refreshing...\", delete_after=3)\n\n            view = self.gui_controller.create_status_gui()\n\n            # Import status reader to create embed\n            from src.discord_commander.status_reader import StatusReader\n\n            status_reader = StatusReader()\n\n            # Create status embed\n            main_view = self.gui_controller.create_main_gui()\n            embed = await main_view._create_status_embed(status_reader)\n\n            await ctx.send(embed=embed, view=view)\n\n        except Exception as e:",
            "        try:\n            action = action.lower()\n\n            if not hasattr(self, 'status_monitor'):\n                await ctx.send(\" Status monitor not initialized. Bot may not be fully ready.\")\n                return\n\n            if action == \"start\":\n                if hasattr(self.status_monitor, 'monitor_status_changes'):\n                    if self.status_monitor.monitor_status_changes.is_running():\n                        await ctx.send(\" Status monitor is already running!\")\n                    else:\n                        self.status_monitor.start_monitoring()\n                        await ctx.send(\" Status monitor started! Checking every 15 seconds.\")\n                else:\n                    self.status_monitor.start_monitoring()\n                    await ctx.send(\" Status monitor started! Checking every 15 seconds.\")\n\n            elif action == \"stop\":\n                if hasattr(self.status_monitor, 'monitor_status_changes'):\n                    if self.status_monitor.monitor_status_changes.is_running():\n                        self.status_monitor.stop_monitoring()\n                        await ctx.send(\" Status monitor stopped.\")\n                    else:\n                        await ctx.send(\" Status monitor is not running.\")\n                else:\n                    await ctx.send(\" Status monitor is not running.\")\n\n            elif action == \"status\":\n                if hasattr(self.status_monitor, 'monitor_status_changes'):\n                    is_running = self.status_monitor.monitor_status_changes.is_running()\n                    status_text = \" RUNNING\" if is_running else \" STOPPED\"\n                    interval = self.status_monitor.check_interval\n\n                    # Add manual-start note to description\n                    description = f\"**Status:** {status_text}\"\n                    description += \"\\n**Start/stop via Control Panel button or !monitor start/stop**\"\n                    description += f\"\\n**Check Interval:** {interval} seconds\"\n\n                    embed = discord.Embed(\n                        title=\" Status Change Monitor\",\n                        description=description,\n                        color=0x27AE60 if is_running else 0xE74C3C,\n                        timestamp=discord.utils.utcnow()\n                    )\n\n                    # Show tracking info\n                    if hasattr(self.status_monitor, 'last_modified'):\n                        tracked_agents = len(self.status_monitor.last_modified)\n                        embed.add_field(\n                            name=\"Tracked Agents\",\n                            value=f\"{tracked_agents}/8 agents\",\n                            inline=True\n                        )\n\n                    embed.set_footer(\n                        text=\"Use Control Panel button or !monitor stop/start to control the monitor\")\n                    await ctx.send(embed=embed)\n                else:\n                    await ctx.send(\" Status monitor not initialized.\")\n\n            else:\n                await ctx.send(\" Invalid action. Use: `!monitor [stop|status]` (monitor auto-starts with bot)\")\n\n        except Exception as e:",
            "        try:\n            embed = discord.Embed(\n                title=\" Agent Messaging Control Panel\",\n                description=\"Use the controls below to interact with the swarm\",\n                color=discord.Color.blue(),\n                timestamp=discord.utils.utcnow(),\n            )\n\n            embed.add_field(\n                name=\" Instructions\",\n                value=(\n                    \"1. Select an agent from dropdown to send message\\n\"\n                    \"2. Click 'Broadcast' to message all agents\\n\"\n                    \"3. Click 'Status' to view swarm status\\n\"\n                    \"4. Click 'Refresh' to reload agent list\"\n                ),\n                inline=False,\n            )\n\n            view = self.gui_controller.create_main_gui()\n            await ctx.send(embed=embed, view=view)\n\n        except discord.Forbidden as e:",
            "        try:\n            success = await self.gui_controller.broadcast_message(\n                message=message,\n                priority=\"regular\",\n                discord_user=ctx.author,\n            )\n\n            if success:\n                embed = discord.Embed(\n                    title=\" Broadcast Sent\",\n                    description=\"Delivered to all agents\",\n                    color=discord.Color.green(),\n                )\n                # Use chunking utility to avoid truncation\n                from src.discord_commander.utils.message_chunking import chunk_field_value\n                message_chunks = chunk_field_value(message)\n                embed.add_field(\n                    name=\"Message\", value=message_chunks[0], inline=False)\n                # If message was chunked, send additional parts\n                if len(message_chunks) > 1:\n                    for i, chunk in enumerate(message_chunks[1:], 2):\n                        embed.add_field(\n                            name=f\"Message (continued {i}/{len(message_chunks)})\",\n                            value=chunk,\n                            inline=False\n                        )\n                await ctx.send(embed=embed)\n            else:\n                await ctx.send(\" Failed to broadcast message\")\n\n        except Exception as e:",
            "        try:\n            success = await self.gui_controller.send_message(\n                agent_id=agent_id,\n                message=message,\n                priority=\"regular\",\n                discord_user=ctx.author,\n            )\n\n            if success:\n                embed = discord.Embed(\n                    title=\" Message Sent\",\n                    description=f\"Delivered to **{agent_id}**\",\n                    color=discord.Color.green(),\n                )\n                # Use chunking utility to avoid truncation\n                from src.discord_commander.utils.message_chunking import chunk_field_value\n                message_chunks = chunk_field_value(message)\n                embed.add_field(\n                    name=\"Message\", value=message_chunks[0], inline=False)\n                # If message was chunked, send additional parts\n                if len(message_chunks) > 1:\n                    for i, chunk in enumerate(message_chunks[1:], 2):\n                        embed.add_field(\n                            name=f\"Message (continued {i}/{len(message_chunks)})\",\n                            value=chunk,\n                            inline=False\n                        )\n                await ctx.send(embed=embed)\n            else:\n                await ctx.send(f\" Failed to send message to {agent_id}\")\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\onboarding_commands.py",
          "count": 2,
          "lines": [
            "        try:\n            # If no agents specified, default to all\n            if not agent_ids or agent_ids.strip().lower() == \"all\":\n                agent_ids = \"Agent-1,Agent-2,Agent-3,Agent-4,Agent-5,Agent-6,Agent-7,Agent-8\"\n                agent_list = [f\"Agent-{i}\" for i in range(1, 9)]\n            else:\n                # Parse comma-separated agent IDs\n                raw_agent_list = [aid.strip()\n                                  for aid in agent_ids.split(\",\") if aid.strip()]\n                # Convert numeric IDs to Agent-X format\n                agent_list = []\n                for aid in raw_agent_list:\n                    if aid.isdigit():\n                        agent_list.append(f\"Agent-{aid}\")\n                    elif aid.lower().startswith(\"agent-\"):\n                        agent_list.append(aid)  # Already in correct format\n                    else:\n                        agent_list.append(aid)  # Keep as-is (might be valid)\n\n            if not agent_list:\n                await ctx.send(\" No valid agents specified. Use: `!hard_onboard 1` or `!hard_onboard Agent-1` or `!hard_onboard 1,2,3`\")\n                return\n\n            embed = discord.Embed(\n                title=\" HARD ONBOARD INITIATED\",\n                description=f\"Hard onboarding **{len(agent_list)} agent(s)**...\\n\\n**Agents:** {', '.join(agent_list)}\",\n                color=discord.Color.orange(),\n            )\n            await ctx.send(embed=embed)\n\n            # Hard onboard each agent using hard onboarding service\n            successful = []\n            failed = []\n\n            # Get project root (use module-level or calculate)\n            project_root = Path(__file__).parent.parent.parent\n\n            # Import hard onboarding service\n            from src.services.hard_onboarding_service import hard_onboard_agent\n\n            for agent_id in agent_list:\n                try:\n                    # Load onboarding message from agent's workspace\n                    onboarding_file = project_root / \"agent_workspaces\" / \\\n                        agent_id / \"HARD_ONBOARDING_MESSAGE.md\"\n\n                    if onboarding_file.exists():\n                        onboarding_message = onboarding_file.read_text(\n                            encoding=\"utf-8\")\n                    else:\n                        # Use default onboarding message if file doesn't exist\n                        onboarding_message = f\"\"\" HARD ONBOARD - {agent_id}\n\n**Status**: RESET & ACTIVATE\n**Protocol**: Complete session reset\n\n**YOUR MISSION**: Resume autonomous operations immediately.\n\n**NEXT ACTIONS**:\n1. Check your inbox for assignments\n2. Update your status.json\n3. Resume autonomous execution\n4. Post devlog when work complete\n\n**WE. ARE. SWARM. AUTONOMOUS. POWERFUL. **\"\"\"\n\n                    # Execute hard onboarding\n                    success = hard_onboard_agent(\n                        agent_id=agent_id,\n                        onboarding_message=onboarding_message,\n                        role=None\n                    )\n\n                    if success:\n                        successful.append(agent_id)\n                    else:\n                        failed.append(\n                            (agent_id, \"Hard onboarding service returned False\"))\n                except Exception as e:",
            "        try:\n            # If no agents specified, default to all\n            if not agent_ids or agent_ids.strip().lower() == \"all\":\n                agent_ids = \"Agent-1,Agent-2,Agent-3,Agent-4,Agent-5,Agent-6,Agent-7,Agent-8\"\n                agent_list = [f\"Agent-{i}\" for i in range(1, 9)]\n            else:\n                # Parse comma-separated agent IDs\n                raw_agent_list = [aid.strip()\n                                  for aid in agent_ids.split(\",\") if aid.strip()]\n                # Convert numeric IDs to Agent-X format\n                agent_list = []\n                for aid in raw_agent_list:\n                    if aid.isdigit():\n                        agent_list.append(f\"Agent-{aid}\")\n                    elif aid.lower().startswith(\"agent-\"):\n                        agent_list.append(aid)  # Already in correct format\n                    else:\n                        agent_list.append(aid)  # Keep as-is (might be valid)\n\n            if not agent_list:\n                await ctx.send(\" No valid agents specified. Use: `!soft 1` or `!soft Agent-1` or `!soft 1,2,3`\")\n                return\n\n            # Default message\n            message = \" SOFT ONBOARD - Agent activation initiated. Check your inbox and begin autonomous operations.\"\n\n            embed = discord.Embed(\n                title=\" SOFT ONBOARD INITIATED\",\n                description=f\"Soft onboarding **{len(agent_list)} agent(s)**...\\n\\n**Agents:** {', '.join(agent_list)}\",\n                color=discord.Color.orange(),\n            )\n            await ctx.send(embed=embed)\n\n            # Soft onboard agents (use --agents for multiple, --agent for single)\n            successful = []\n            failed = []\n\n            # Get project root (use module-level or calculate)\n            project_root = Path(__file__).parent.parent.parent\n            cli_path = project_root / 'tools' / 'soft_onboard_cli.py'\n\n            try:\n                # Use --agents for multiple agents (more efficient, uses soft_onboard_multiple_agents)\n                if len(agent_list) == 1:\n                    # Single agent - use --agent\n                    cmd = ['python', str(cli_path), '--agent',\n                           agent_list[0], '--message', message]\n                else:\n                    # Multiple agents - use --agents with comma-separated list\n                    agents_str = ','.join(agent_list)\n                    cmd = ['python', str(\n                        cli_path), '--agents', agents_str, '--message', message, '--generate-cycle-report']\n\n                result = subprocess.run(\n                    cmd, capture_output=True, text=True, timeout=TimeoutConstants.HTTP_EXTENDED, cwd=str(project_root))\n\n                if result.returncode == 0:\n                    # All agents successful\n                    successful = agent_list.copy()\n                    # Parse output to check individual results if needed\n                    if result.stdout:\n                        # Check for any failures in output\n                        if \"Failed:\" in result.stdout or \"\" in result.stdout:\n                            # Parse individual results from output\n                            lines = result.stdout.split('\\n')\n                            for line in lines:\n                                if \"\" in line and any(agent in line for agent in agent_list):\n                                    # Agent succeeded\n                                    pass\n                                elif \"\" in line and any(agent in line for agent in agent_list):\n                                    # Agent failed - extract agent ID\n                                    for agent in agent_list:\n                                        if agent in line and agent not in [s for s in successful]:\n                                            failed.append(\n                                                (agent, \"Failed during onboarding\"))\n                                            if agent in successful:\n                                                successful.remove(agent)\n                else:\n                    # Command failed - try to parse which agents failed\n                    error_msg = result.stderr[:500] if result.stderr else result.stdout[:\n                                                                                        500] if result.stdout else \"Unknown error\"\n                    # If we can't determine individual failures, mark all as failed\n                    if len(agent_list) == 1:\n                        failed.append((agent_list[0], error_msg))\n                    else:\n                        # For multiple agents, mark all as failed if we can't parse individual results\n                        for agent_id in agent_list:\n                            failed.append((agent_id, error_msg))\n            except subprocess.TimeoutExpired:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\placeholder_commands.py",
          "count": 2,
          "lines": [
            "        try:\n            cycles_dir = Path(\"docs/archive/cycles\")\n            if not cycles_dir.exists():\n                await ctx.send(\" Cycles directory not found: `docs/archive/cycles/`\")\n                return\n\n            # Find cycle report files\n            cycle_files = sorted(cycles_dir.glob(\n                \"CYCLE_ACCOMPLISHMENTS_*.md\"), reverse=True)\n\n            if not cycle_files:\n                await ctx.send(\" No cycle accomplishment reports found in `docs/archive/cycles/`\")\n                return\n\n            # Select file based on date parameter\n            selected_file = None\n            if date:\n                if date.lower() == \"latest\":\n                    selected_file = cycle_files[0]\n                else:\n                    # Try to match date in filename\n                    date_pattern = date.replace(\"-\", \"_\")\n                    for f in cycle_files:\n                        if date_pattern in f.name:\n                            selected_file = f\n                            break\n                    if not selected_file:\n                        await ctx.send(f\" No report found for date: {date}\\n**Available dates:** Use `!session latest` to see most recent\")\n                        return\n            else:\n                # Default to most recent\n                selected_file = cycle_files[0]\n\n            # Read and parse report\n            report_content = selected_file.read_text(encoding=\"utf-8\")\n\n            # Extract date from filename\n            date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', selected_file.name)\n            report_date = date_match.group(1) if date_match else \"Unknown\"\n\n            # Parse report sections\n            lines = report_content.split('\\n')\n\n            # Extract summary\n            summary = {}\n            in_summary = False\n            for line in lines:\n                if \"##  SWARM SUMMARY\" in line:\n                    in_summary = True\n                    continue\n                if in_summary and line.startswith(\"##\"):\n                    break\n                if in_summary and \"- **\" in line:\n                    match = re.search(r'\\*\\*(.+?)\\*\\*: (.+)', line)\n                    if match:\n                        summary[match.group(1)] = match.group(2).strip()\n\n            # Extract agent accomplishments\n            agents_data = {}\n            current_agent = None\n            current_section = None\n            current_content = []\n\n            for line in lines:\n                if line.startswith(\"### Agent-\"):\n                    # Save previous agent\n                    if current_agent:\n                        agents_data[current_agent][current_section] = '\\n'.join(\n                            current_content).strip()\n\n                    # Start new agent\n                    match = re.match(r'### (Agent-\\d+) - (.+)', line)\n                    if match:\n                        current_agent = match.group(1)\n                        agents_data[current_agent] = {\n                            'name': match.group(2),\n                            'completed_tasks': [],\n                            'achievements': [],\n                            'current_tasks': []\n                        }\n                        current_content = []\n                        current_section = None\n                elif current_agent and line.startswith(\"####\"):\n                    # Save previous section\n                    if current_section and current_content:\n                        if current_section == 'completed_tasks':\n                            agents_data[current_agent]['completed_tasks'] = [c.strip(\n                                '- ').strip() for c in current_content if c.strip() and c.strip().startswith('-')]\n                        elif current_section == 'achievements':\n                            agents_data[current_agent]['achievements'] = [c.strip(\n                                '- ').strip() for c in current_content if c.strip() and c.strip().startswith('-')]\n                        elif current_section == 'current_tasks':\n                            agents_data[current_agent]['current_tasks'] = [c.strip(\n                                '- ').strip() for c in current_content if c.strip() and c.strip().startswith('-')]\n\n                    # Start new section\n                    if \"Completed Tasks\" in line:\n                        current_section = 'completed_tasks'\n                    elif \"Achievements\" in line:\n                        current_section = 'achievements'\n                    elif \"Current Tasks\" in line:\n                        current_section = 'current_tasks'\n                    else:\n                        current_section = None\n                    current_content = []\n                elif current_agent and current_section and line.strip():\n                    current_content.append(line)\n\n            # Save last agent\n            if current_agent and current_section and current_content:\n                if current_section == 'completed_tasks':\n                    agents_data[current_agent]['completed_tasks'] = [c.strip(\n                        '- ').strip() for c in current_content if c.strip() and c.strip().startswith('-')]\n                elif current_section == 'achievements':\n                    agents_data[current_agent]['achievements'] = [c.strip(\n                        '- ').strip() for c in current_content if c.strip() and c.strip().startswith('-')]\n\n            # Create beautiful embed\n            embed = discord.Embed(\n                title=\" SESSION ACCOMPLISHMENTS REPORT\",\n                description=f\"**Date**: {report_date}\\n**Report**: `{selected_file.name}`\",\n                color=discord.Color.blue(),\n                timestamp=datetime.now()\n            )\n\n            # Add summary fields\n            if summary:\n                summary_text = \"\\n\".join(\n                    [f\"**{k}**: {v}\" for k, v in summary.items()])\n                embed.add_field(\n                    name=\" Swarm Summary\",\n                    value=summary_text[:1024],\n                    inline=False\n                )\n\n            # Add agent accomplishments (limit to fit Discord limits)\n            agent_texts = []\n            for agent_id in sorted(agents_data.keys()):\n                data = agents_data[agent_id]\n                agent_text = f\"**{agent_id}** - {data['name']}\\n\"\n\n                if data['completed_tasks']:\n                    task_count = len(data['completed_tasks'])\n                    agent_text += f\" **{task_count}** completed tasks\\n\"\n                    # Show first 3 tasks\n                    for task in data['completed_tasks'][:3]:\n                        task_short = task[:80] + \\\n                            \"...\" if len(task) > 80 else task\n                        agent_text += f\"   {task_short}\\n\"\n                    if task_count > 3:\n                        agent_text += f\"   *... and {task_count - 3} more*\\n\"\n\n                if data['achievements']:\n                    achievement_count = len(data['achievements'])\n                    agent_text += f\" **{achievement_count}** achievements\\n\"\n                    # Show first 2 achievements\n                    for achievement in data['achievements'][:2]:\n                        achievement_short = achievement[:80] + \\\n                            \"...\" if len(achievement) > 80 else achievement\n                        agent_text += f\"   {achievement_short}\\n\"\n                    if achievement_count > 2:\n                        agent_text += f\"   *... and {achievement_count - 2} more*\\n\"\n\n                agent_texts.append(agent_text)\n\n            # Split agents into chunks to fit Discord limits\n            chunk_size = 3  # 3 agents per embed field\n            for i in range(0, len(agent_texts), chunk_size):\n                chunk = agent_texts[i:i+chunk_size]\n                field_value = \"\\n\".join(chunk)\n                if len(field_value) > 1024:\n                    field_value = field_value[:1021] + \"...\"\n\n                field_name = f\" Agents {i+1}-{min(i+chunk_size, len(agent_texts))}\" if len(\n                    agent_texts) > chunk_size else \" Agent Accomplishments\"\n                embed.add_field(\n                    name=field_name,\n                    value=field_value,\n                    inline=False\n                )\n\n            # Add footer\n            embed.set_footer(text=\" WE. ARE. SWARM. \")\n\n            # Send embed\n            await ctx.send(embed=embed)\n\n            # If report is very long, also send a link to the full report\n            if len(report_content) > 4000:\n                await ctx.send(\n                    f\" **Full Report Available**: `{selected_file.name}`\\n\"\n                    f\" Use `!session {report_date}` to view this report again\"\n                )\n\n        except Exception as e:",
            "        try:\n            embed = discord.Embed(\n                title=\" How to Get SFTP Credentials (30 seconds)\",\n                description=\"**Quick guide to get your SFTP credentials from Hostinger**\",\n                color=discord.Color.green(),\n            )\n\n            embed.add_field(\n                name=\"Step 1: Log into Hostinger\",\n                value=\" https://hpanel.hostinger.com/\",\n                inline=False,\n            )\n\n            embed.add_field(\n                name=\"Step 2: Get Credentials\",\n                value=(\n                    \"1. Click **Files**  **FTP Accounts**\\n\"\n                    \"2. Find your domain\\n\"\n                    \"3. Copy these 4 values:\\n\"\n                    \"    **FTP Username** (not your email!)\\n\"\n                    \"    **FTP Password** (click 'Show' or reset if needed)\\n\"\n                    \"    **FTP Host** (IP address like `157.173.214.121`)\\n\"\n                    \"    **FTP Port** (should be `65002`)\"\n                ),\n                inline=False,\n            )\n\n            embed.add_field(\n                name=\"Step 3: Add to .env File\",\n                value=(\n                    \"Open `.env` in repository root, add:\\n\"\n                    \"```env\\n\"\n                    \"HOSTINGER_HOST=157.173.214.121\\n\"\n                    \"HOSTINGER_USER=your_username_here\\n\"\n                    \"HOSTINGER_PASS=your_password_here\\n\"\n                    \"HOSTINGER_PORT=65002\\n\"\n                    \"```\"\n                ),\n                inline=False,\n            )\n\n            embed.add_field(\n                name=\"Step 4: Test\",\n                value=\"```bash\\npython tools/sftp_credential_troubleshooter.py\\n```\",\n                inline=False,\n            )\n\n            embed.add_field(\n                name=\" Tip\",\n                value=\"Username might be different from your email (check Hostinger exactly as shown)\",\n                inline=False,\n            )\n\n            embed.set_footer(text=\" WE. ARE. SWARM. \")\n\n            await ctx.send(embed=embed)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\profile_commands.py",
          "count": 2,
          "lines": [
            "        try:\n            from src.discord_commander.views.aria_profile_view import AriaProfileView\n            view = AriaProfileView()\n            embed = view._create_main_embed()\n            await ctx.send(embed=embed, view=view)\n        except Exception as e:",
            "        try:\n            from src.discord_commander.views.carmyn_profile_view import CarmynProfileView\n            view = CarmynProfileView()\n            embed = view._create_main_embed()\n            await ctx.send(embed=embed, view=view)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\utility_commands.py",
          "count": 3,
          "lines": [
            "        try:\n            control_view = self.gui_controller.create_control_panel()\n            embed = discord.Embed(\n                title=\" All Commands - Use Control Panel Buttons!\",\n                description=(\n                    \"** All commands are accessible via buttons in the Control Panel!**\\n\\n\"\n                    \"**Click the buttons below to access all features:**\\n\"\n                    \" **Tasks** button = `!swarm_tasks`\\n\"\n                    \" **Swarm Status** button = `!status`\\n\"\n                    \" **GitHub Book** button = `!github_book`\\n\"\n                    \" **Roadmap** button = `!swarm_roadmap`\\n\"\n                    \" **Excellence** button = `!swarm_excellence`\\n\"\n                    \" **Overview** button = `!swarm_overview`\\n\"\n                    \" **Goldmines** button = `!goldmines`\\n\"\n                    \" **Templates** button = `!templates`\\n\"\n                    \" **Mermaid** button = `!mermaid`\\n\"\n                    \" **Monitor** button = `!monitor`\\n\"\n                    \" **Help** button = `!help`\\n\"\n                    \" **All Commands** button = This view\\n\\n\"\n                    \"**No need to type commands - just click buttons!**\"\n                ),\n                color=discord.Color.blue(),\n            )\n            embed.add_field(\n                name=\" Quick Access\",\n                value=\"Type `!control` (or `!panel`, `!menu`) to open Control Panel anytime!\",\n                inline=False,\n            )\n            embed.set_footer(text=\" WE. ARE. SWARM.  Buttons > Commands!\")\n            await ctx.send(embed=embed, view=control_view)\n        except Exception as e:",
            "        try:\n            diagram_code = self._clean_mermaid_code(diagram_code)\n            embed = discord.Embed(\n                title=\" Mermaid Diagram\",\n                description=\"Mermaid diagram code:\",\n                color=discord.Color.blue(),\n            )\n            mermaid_block = f\"```mermaid\\n{diagram_code}\\n```\"\n\n            if len(mermaid_block) > 1900:\n                await ctx.send(\" Mermaid diagram too long. Please shorten it.\")\n                return\n\n            embed.add_field(name=\"Diagram Code\", value=mermaid_block, inline=False)\n            embed.set_footer(text=\" Tip: Copy this code to a Mermaid editor or use Discord's code block rendering\")\n            await ctx.send(embed=embed)\n        except Exception as e:",
            "        try:\n            from src.discord_commander.views import HelpGUIView\n            view = HelpGUIView()\n            embed = view._create_main_embed()\n            await ctx.send(embed=embed, view=view)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\technical_debt_base.py",
          "count": 4,
          "lines": [
            "        try:\n            debate_id = debate_data[\"debate_id\"]\n            debate_file = self.debates_dir / f\"{debate_id}.json\"\n\n            import json\n            with open(debate_file, 'w', encoding='utf-8') as f:\n                json.dump(debate_data, f, indent=2, ensure_ascii=False)\n\n        except Exception as e:",
            "        try:\n            for debate_file in self.debates_dir.glob(\"*.json\"):\n                try:\n                    import json\n                    from datetime import datetime\n\n                    with open(debate_file, 'r', encoding='utf-8') as f:\n                        debate_data = json.load(f)\n\n                    debate_id = debate_data.get(\"debate_id\")\n                    deadline = debate_data.get(\"deadline\")\n\n                    # Check if debate is still active\n                    if deadline:\n                        from datetime import datetime\n                        deadline_dt = datetime.fromisoformat(deadline.replace('Z', '+00:00'))\n                        if datetime.now().replace(tzinfo=deadline_dt.tzinfo) < deadline_dt:\n                            self.active_debates[debate_id] = debate_data\n\n                except Exception as e:",
            "        try:\n            from systems.technical_debt.integration.orchestrator import TechnicalDebtIntegrationOrchestrator\n            self.debt_orchestrator = TechnicalDebtIntegrationOrchestrator()\n            logger.info(\" Technical Debt Integration Orchestrator initialized\")\n        except ImportError as e:",
            "        try:\n            import sys\n            from pathlib import Path as PathLib\n            project_root = PathLib(__file__).resolve().parents[4]\n            self.debates_dir = project_root / \"debates\"\n            self.debates_dir.mkdir(exist_ok=True)\n            self._load_active_debates()\n            logger.info(\" Debate system initialized\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\technical_debt_core.py",
          "count": 3,
          "lines": [
            "        try:\n            # Get comprehensive system status\n            system_status = self.debt_orchestrator.get_system_status()\n\n            if system_status.get(\"status\") == \"error\":\n                return await ctx.send(f\" Error retrieving debt status: {system_status.get('error')}\")\n\n            debt_summary = system_status.get(\"debt_summary\", {})\n            agent_availability = system_status.get(\"agent_availability\", {})\n            audit_compliance = system_status.get(\"audit_compliance\", {})\n            recommendations = system_status.get(\"assignment_recommendations\", {})\n\n            # Create embed\n            embed = discord.Embed(\n                title=\" Technical Debt Status - Integrated System\",\n                color=0xFFA500,  # Orange\n                timestamp=ctx.message.created_at\n            )\n\n            # Debt Summary\n            total_pending = debt_summary.get(\"total_pending_tasks\", 0)\n            total_resolved = debt_summary.get(\"total_resolved_tasks\", 0)\n\n            embed.add_field(\n                name=\" Overall Status\",\n                value=f\"**Pending:** {total_pending}\\n**Resolved:** {total_resolved}\\n**Available Agents:** {agent_availability.get('total_available', 0)}\",\n                inline=True\n            )\n\n            # Categories breakdown\n            categories = debt_summary.get(\"categories\", {})\n            if categories:\n                category_text = \"\"\n                for cat_name, cat_data in list(categories.items())[:6]:  # Limit to 6 for Discord\n                    pending = cat_data.get(\"pending\", 0)\n                    resolved = cat_data.get(\"resolved\", 0)\n                    if pending > 0:\n                        category_text += f\" {cat_name.replace('_', ' ').title()}: {pending} pending\\n\"\n\n                if category_text:\n                    embed.add_field(\n                        name=\" Active Categories\",\n                        value=category_text[:1024],  # Discord field limit\n                        inline=True\n                    )\n\n            # System Health\n            compliance = \" Compliant\" if audit_compliance.get(\"overall_compliance\") else \" Needs Attention\"\n            embed.add_field(\n                name=\" System Health\",\n                value=f\"**Audit Status:** {compliance}\\n**Last Updated:** {debt_summary.get('last_updated', 'Unknown')[:19]}\",\n                inline=False\n            )\n\n            # Recommendations\n            rec_count = recommendations.get(\"recommended_count\", 0)\n            if rec_count > 0:\n                embed.add_field(\n                    name=\" Recommendations\",\n                    value=f\"{rec_count} task assignments available\\nUse `!debt_recommendations` for details\",\n                    inline=False\n                )\n\n            embed.set_footer(text=\" Connected to Agent Status Monitor, Master Task Log & Audit Trail\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            recommendations = self.debt_orchestrator.agent_integration.get_assignment_recommendations()\n\n            if recommendations.get(\"status\") != \"success\":\n                return await ctx.send(f\" Error getting recommendations: {recommendations.get('message')}\")\n\n            recs = recommendations.get(\"recommendations\", [])\n\n            embed = discord.Embed(\n                title=\" Debt Task Assignment Recommendations\",\n                description=f\"**{len(recs)} intelligent recommendations available**\",\n                color=0x3498DB,  # Blue\n                timestamp=ctx.message.created_at\n            )\n\n            if recs:\n                rec_text = \"\"\n                for i, rec in enumerate(recs[:5], 1):  # Limit to 5 for readability\n                    task = rec.get(\"task\", {})\n                    agent = rec.get(\"recommended_agent\", \"Unknown\")\n                    reason = rec.get(\"reasoning\", \"Based on capabilities match\")\n\n                    rec_text += f\"**{i}.** {task.get('title', 'Unknown Task')}\\n\"\n                    rec_text += f\"    Assign to: `{agent}`\\n\"\n                    rec_text += f\"    Reason: {reason}\\n\\n\"\n\n                embed.add_field(\n                    name=\" Top Recommendations\",\n                    value=rec_text[:1024],\n                    inline=False\n                )\n\n                embed.add_field(\n                    name=\" Quick Assign\",\n                    value=\"Use `!debt_assign <category> <agent>` to manually assign tasks\",\n                    inline=False\n                )\n            else:\n                embed.add_field(\n                    name=\" No Recommendations\",\n                    value=\"All pending tasks are either assigned or no suitable agents are available.\",\n                    inline=False\n                )\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            result = self.debt_orchestrator.assign_specific_debt_task(category, agent_id)\n\n            if result.get(\"status\") == \"assigned\":\n                assignment = result.get(\"assignment\", {})\n                task = assignment.get(\"task\", {})\n\n                embed = discord.Embed(\n                    title=\" Debt Task Assigned\",\n                    color=0x00FF00,  # Green\n                    timestamp=ctx.message.created_at\n                )\n\n                embed.add_field(\n                    name=\" Task Details\",\n                    value=f\"**Category:** {category}\\n**Priority:** {task.get('priority', 'Unknown')}\\n**Items:** {task.get('pending_count', 0)}\",\n                    inline=True\n                )\n\n                embed.add_field(\n                    name=\" Assignment\",\n                    value=f\"**Agent:** {agent_id}\\n**Timestamp:** {assignment.get('timestamp', 'Unknown')[:19]}\\n**Audit Logged:** {'' if result.get('audit_logged') else ''}\",\n                    inline=True\n                )\n\n                embed.set_footer(text=f\"Assigned by {ctx.author}\")\n\n                await ctx.send(embed=embed)\n\n            elif result.get(\"status\") == \"agent_unavailable\":\n                await ctx.send(f\" Agent {agent_id} is not available for task assignment. Check `!technical_debt` for available agents.\")\n\n            else:\n                await ctx.send(f\" Assignment failed: {result.get('message')}\")\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\debate_commands.py",
          "count": 4,
          "lines": [
            "        try:\n            # Parse topic and create structured debate\n            debate_id = f\"debate_{int(datetime.now().timestamp())}\"\n\n            # Create basic debate structure\n            debate_data = {\n                \"debate_id\": debate_id,\n                \"topic\": topic,\n                \"description\": f\"Democratic debate on: {topic}\",\n                \"created_by\": str(ctx.author),\n                \"created\": datetime.now().isoformat(),\n                \"deadline\": (datetime.now() + timedelta(hours=duration_hours)).isoformat(),\n                \"duration_hours\": duration_hours,\n                \"status\": \"active\",\n                \"votes\": {},\n                \"arguments\": []\n            }\n\n            # Save debate\n            self._save_debate(debate_data)\n            self.active_debates[debate_id] = debate_data\n\n            embed = discord.Embed(\n                title=\" Debate Created\",\n                description=f\"**Topic:** {topic}\",\n                color=0x3498DB,  # Blue\n                timestamp=ctx.message.created_at\n            )\n\n            embed.add_field(\n                name=\" Duration\",\n                value=f\"{duration_hours} hours\",\n                inline=True\n            )\n\n            embed.add_field(\n                name=\" Status\",\n                value=\"ACTIVE - Open for arguments and voting\",\n                inline=True\n            )\n\n            embed.add_field(\n                name=\" How to Participate\",\n                value=\" `!debate_vote <debate_id> <option>` - Cast your vote\\n `!debate_argue <debate_id> <argument>` - Add argument\\n `!debate_status <debate_id>` - View current status\",\n                inline=False\n            )\n\n            embed.set_footer(text=f\"Created by {ctx.author}  Debate ID: {debate_id}\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            debate_id = debate.get(\"debate_id\", \"unknown\")\n            topic = debate.get(\"topic\", \"Unknown Topic\")\n\n            # Calculate time remaining\n            deadline = datetime.fromisoformat(debate[\"deadline\"].replace('Z', '+00:00'))\n            time_left = deadline - datetime.now().replace(tzinfo=deadline.tzinfo)\n            hours_left = max(0, int(time_left.total_seconds() / 3600))\n\n            embed = discord.Embed(\n                title=f\" Debate: {topic}\",\n                description=f\"**ID:** {debate_id}\",\n                color=0x3498DB,  # Blue\n                timestamp=ctx.message.created_at\n            )\n\n            embed.add_field(\n                name=\" Time Remaining\",\n                value=f\"{hours_left} hours\",\n                inline=True\n            )\n\n            # Vote breakdown\n            votes = debate.get(\"votes\", {})\n            vote_counts = {}\n            for vote_data in votes.values():\n                option = vote_data.get(\"option\", \"unknown\")\n                vote_counts[option] = vote_counts.get(option, 0) + 1\n\n            vote_summary = \"\\n\".join([f\" **{option}**: {count} votes\" for option, count in vote_counts.items()])\n            if not vote_summary:\n                vote_summary = \"No votes yet\"\n\n            embed.add_field(\n                name=f\" Votes ({len(votes)} total)\",\n                value=vote_summary,\n                inline=True\n            )\n\n            embed.set_footer(text=f\"Created by {debate.get('created_by', 'Unknown')}  Use !debate_vote {debate_id} <option> to vote\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            if debate_id not in self.active_debates:\n                return await ctx.send(f\" Debate '{debate_id}' not found or expired.\")\n\n            debate = self.active_debates[debate_id]\n\n            # Check if debate has expired\n            deadline = datetime.fromisoformat(debate[\"deadline\"].replace('Z', '+00:00'))\n            if datetime.now().replace(tzinfo=deadline.tzinfo) > deadline:\n                return await ctx.send(f\" Debate '{debate_id}' has expired.\")\n\n            # Record vote\n            voter_id = str(ctx.author)\n            if \"votes\" not in debate:\n                debate[\"votes\"] = {}\n\n            debate[\"votes\"][voter_id] = {\n                \"option\": option,\n                \"timestamp\": datetime.now().isoformat(),\n                \"voter\": str(ctx.author)\n            }\n\n            # Save updated debate\n            self._save_debate(debate)\n\n            embed = discord.Embed(\n                title=\" Vote Recorded\",\n                color=0x00FF00,  # Green\n                timestamp=ctx.message.created_at\n            )\n\n            embed.add_field(\n                name=\" Debate\",\n                value=debate.get(\"topic\", debate_id),\n                inline=True\n            )\n\n            embed.add_field(\n                name=\" Your Vote\",\n                value=f\"**{option}**\",\n                inline=True\n            )\n\n            embed.set_footer(text=f\"Voted by {ctx.author}\")\n\n            await ctx.send(embed=embed)\n\n        except Exception as e:",
            "        try:\n            if debate_id:\n                # Show specific debate\n                if debate_id not in self.active_debates:\n                    return await ctx.send(f\" Debate '{debate_id}' not found.\")\n\n                debate = self.active_debates[debate_id]\n                await self._show_debate_details(ctx, debate)\n            else:\n                # Show all active debates\n                if not self.active_debates:\n                    return await ctx.send(\" No active debates currently.\")\n\n                embed = discord.Embed(\n                    title=\" Active Debates\",\n                    description=f\"**{len(self.active_debates)} active debates**\",\n                    color=0x3498DB,  # Blue\n                    timestamp=ctx.message.created_at\n                )\n\n                for debate_id, debate in list(self.active_debates.items())[:5]:  # Limit to 5\n                    deadline = datetime.fromisoformat(debate[\"deadline\"].replace('Z', '+00:00'))\n                    time_left = deadline - datetime.now().replace(tzinfo=deadline.tzinfo)\n                    hours_left = max(0, int(time_left.total_seconds() / 3600))\n\n                    votes_count = len(debate.get(\"votes\", {}))\n                    args_count = len(debate.get(\"arguments\", []))\n\n                    embed.add_field(\n                        name=f\" {debate.get('topic', debate_id)[:50]}\",\n                        value=f\" {hours_left}h left   {votes_count} votes   {args_count} arguments\\n`!debate_status {debate_id}`\",\n                        inline=False\n                    )\n\n                embed.set_footer(text=\"Use !debate_status <debate_id> for detailed view\")\n\n                await ctx.send(embed=embed)\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\messaging_monitor_commands.py",
          "count": 1,
          "lines": [
            "        try:\n            action = action.lower()\n            if not hasattr(self.bot, 'status_monitor'):\n                await ctx.send(\" Status monitor not initialized. Bot may not be fully ready.\")\n                return\n\n            if action == \"start\":\n                await self._handle_monitor_start(ctx)\n            elif action == \"stop\":\n                await self._handle_monitor_stop(ctx)\n            elif action == \"status\":\n                await self._handle_monitor_status(ctx)\n            else:\n                await ctx.send(\" Invalid action. Use: `!monitor [stop|status]`\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\messaging_core_commands.py",
          "count": 2,
          "lines": [
            "        try:\n            success = await self.gui_controller.broadcast_message(\n                message=message,\n                priority=\"regular\",\n                discord_user=ctx.author,\n            )\n\n            if success:\n                embed = discord.Embed(\n                    title=\" Broadcast Sent\",\n                    description=\"Delivered to all agents\",\n                    color=discord.Color.green(),\n                )\n                from src.discord_commander.utils.message_chunking import chunk_field_value\n                message_chunks = chunk_field_value(message)\n                embed.add_field(name=\"Message\", value=message_chunks[0], inline=False)\n                if len(message_chunks) > 1:\n                    for i, chunk in enumerate(message_chunks[1:], 2):\n                        embed.add_field(\n                            name=f\"Message (continued {i}/{len(message_chunks)})\",\n                            value=chunk,\n                            inline=False\n                        )\n                await ctx.send(embed=embed)\n            else:\n                await ctx.send(\" Failed to broadcast message\")\n        except Exception as e:",
            "        try:\n            success = await self.gui_controller.send_message(\n                agent_id=agent_id,\n                message=message,\n                priority=\"regular\",\n                discord_user=ctx.author,\n            )\n\n            if success:\n                embed = discord.Embed(\n                    title=\" Message Sent\",\n                    description=f\"Delivered to **{agent_id}**\",\n                    color=discord.Color.green(),\n                )\n                from src.discord_commander.utils.message_chunking import chunk_field_value\n                message_chunks = chunk_field_value(message)\n                embed.add_field(name=\"Message\", value=message_chunks[0], inline=False)\n                if len(message_chunks) > 1:\n                    for i, chunk in enumerate(message_chunks[1:], 2):\n                        embed.add_field(\n                            name=f\"Message (continued {i}/{len(message_chunks)})\",\n                            value=chunk,\n                            inline=False\n                        )\n                await ctx.send(embed=embed)\n            else:\n                await ctx.send(f\" Failed to send message to {agent_id}\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\system_control_commands.py",
          "count": 3,
          "lines": [
            "        try:\n            embed = discord.Embed(\n                title=\" True Restart Requested\",\n                description=(\n                    \"Bot will perform a TRUE restart:\\n\"\n                    \" Current process will be terminated\\n\"\n                    \" Fresh bot + queue processor will start\\n\"\n                    \" All modules reloaded from disk\\n\"\n                    \" Message delivery enabled (queue processor)\\n\\n\"\n                    \"Continue?\"\n                ),\n                color=discord.Color.blue(),\n            )\n            view = ConfirmRestartView()\n            message = await ctx.send(embed=embed, view=view)\n            await view.wait()\n\n            if view.confirmed:\n                restart_embed = discord.Embed(\n                    title=\" Bot Restarting (True Restart)\",\n                    description=(\n                        \"Performing true restart...\\n\"\n                        \" Terminating current process\\n\"\n                        \" Starting fresh bot + queue processor\\n\"\n                        \" All modules reloaded from disk\\n\"\n                        \" Will be back in 5-10 seconds!\"\n                    ),\n                    color=discord.Color.blue(),\n                )\n                await ctx.send(embed=restart_embed)\n                self.logger.info(\" True restart command received - killing process and starting fresh\")\n                self._perform_true_restart()\n                await self.bot.close()\n            else:\n                await message.edit(content=\" Restart cancelled\", embed=None, view=None)\n        except Exception as e:",
            "        try:\n            embed = discord.Embed(\n                title=\" Shutdown Requested\",\n                description=\"Are you sure you want to shutdown the bot?\",\n                color=discord.Color.red(),\n            )\n            view = ConfirmShutdownView()\n            message = await ctx.send(embed=embed, view=view)\n            await view.wait()\n\n            if view.confirmed:\n                shutdown_embed = discord.Embed(\n                    title=\" Bot Shutting Down\",\n                    description=\"Gracefully closing connections...\",\n                    color=discord.Color.orange(),\n                )\n                await ctx.send(embed=shutdown_embed)\n                self.logger.info(\" Shutdown command received - closing bot\")\n                await self.bot.close()\n            else:\n                await message.edit(content=\" Shutdown cancelled\", embed=None, view=None)\n        except Exception as e:",
            "        try:\n            start_script = self._get_start_script_path()\n            if not start_script.exists():\n                self.logger.error(f\"Start script not found: {start_script}\")\n                return False\n\n            self._spawn_restart_process(start_script)\n            self._wait_and_log_restart()\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\control_panel_commands.py",
          "count": 1,
          "lines": [
            "        try:\n            # Check permissions using mixin\n            if not await self.check_permissions(ctx):\n                return\n\n            control_view = self.gui_controller.create_control_panel()\n            embed = self.create_info_embed(\n                title=\"Agent Cellphone V2 - Control Panel\",\n                description=(\n                    \"**Welcome to the Agent Cellphone V2 Control Center!**\\n\\n\"\n                    \"Use the buttons below to access all bot functions:\\n\"\n                    \" **Agent Status** - View current agent statuses\\n\"\n                    \" **Messaging** - Send messages to agents\\n\"\n                    \" **Swarm Tasks** - Manage swarm tasks\\n\"\n                    \" **Monitor** - Control status monitoring\\n\"\n                    \" **Templates** - Access message templates\\n\"\n                    \" **Help** - Interactive help system\\n\\n\"\n                    \"**All functions are accessible via buttons - no typing required!**\"\n                )\n            )\n            embed.add_field(\n                name=\" System Status\",\n                value=\" Bot Online |  Queue Processor Running |  PyAutoGUI Active\",\n                inline=False,\n            )\n            embed.set_footer(text=\" WE. ARE. SWARM.  Control at your fingertips!\")\n\n            await self.safe_send(ctx, embed=embed, view=control_view)\n            self.log_command_success(command_name)\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\command_base.py",
          "count": 2,
          "lines": [
            "class CommandRegistry:\n    \"\"\"Registry for managing Discord command cogs.\"\"\"\n\n    def __init__(self):\n        self._cog_classes = []\n        self._cog_instances = []\n\n    def register_cog_class(self, cog_class):\n        \"\"\"Register a cog class for automatic instantiation.\"\"\"\n        if cog_class not in self._cog_classes:\n            self._cog_classes.append(cog_class)\n        return cog_class\n\n    def get_registered_cogs(self):\n        \"\"\"Get all registered cog classes.\"\"\"\n        return self._cog_classes.copy()\n\n    def create_instances(self, bot: \"UnifiedDiscordBot\"):\n        \"\"\"Create instances of all registered cogs.\"\"\"\n        self._cog_instances = create_cog_instances(bot, self._cog_classes)\n        return self._cog_instances\n\n    def get_instances(self):\n        \"\"\"Get created cog instances.\"\"\"\n        return self._cog_instances.copy()\n\n\n# Global registry instance\ncommand_registry = CommandRegistry()\n\n\ndef register_cog(cog_class):\n    \"\"\"Decorator to register a cog class with the global registry.\"\"\"\n    command_registry.register_cog_class(cog_class)\n    return cog_class\n\n\n# Define command_template conditionally based on discord availability\nif DISCORD_AVAILABLE:\n    def command_template(error_handling: bool = True, log_command: bool = True):\n        \"\"\"\n        Decorator template for Discord commands that eliminates repetitive code.\n\n        Args:\n            error_handling: Whether to wrap command in try/catch\n            log_command: Whether to log command execution\n        \"\"\"\n        def decorator(func):\n            @functools.wraps(func)\n            async def wrapper(self, ctx: commands.Context, *args, **kwargs):\n                command_name = func.__name__\n\n                # Log command if enabled\n                if log_command:\n                    extra_info = \"\"\n                    if args:\n                        extra_info += f\"args={args}\"\n                    if kwargs:\n                        if extra_info:\n                            extra_info += \", \"\n                        extra_info += f\"kwargs={kwargs}\"\n                    self.log_command(ctx, command_name, extra_info)\n\n                # Execute with error handling if enabled\n                if error_handling:\n                    try:\n                        return await func(self, ctx, *args, **kwargs)\n                    except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\commands\\thea_commands.py",
          "count": 3,
          "lines": [
            "        try:\n            embed = self.create_base_embed(\n                \" Thea Manager Authentication\",\n                \"*Authenticating with Thea Manager...*\"\n            )\n            self.add_user_footer(embed, ctx)\n\n            response_msg = await ctx.send(embed=embed)\n\n            # Get Thea service and attempt authentication\n            thea_service = self.bot._get_thea_service(headless=True)\n            success = thea_service.ensure_thea_authenticated(\n                allow_manual=False)\n\n            if success:\n                embed.description = \" Successfully authenticated with Thea Manager\"\n                embed.color = discord.Color.green()\n            else:\n                embed.description = \" Authentication failed. Manual authentication may be required.\"\n                embed.color = discord.Color.red()\n\n            self.add_user_footer(embed, ctx)\n            await response_msg.edit(embed=embed)\n\n        except Exception as e:",
            "        try:\n            embed = self.create_base_embed(\n                \" Thea Manager Query\",\n                f\"**Query:** {message}\\n\\n*Processing...*\"\n            )\n            self.add_user_footer(embed, ctx)\n\n            # Send initial response\n            response_msg = await ctx.send(embed=embed)\n\n            # Get Thea service\n            thea_service = self.bot._get_thea_service(headless=True)\n\n            # Send message to Thea using correct API\n            try:\n                response_content = thea_service.send_prompt_and_get_response_text(\n                    message)\n\n                if response_content and len(response_content.strip()) > 0:\n                    # Success response\n                    embed.description = f\"**Query:** {message}\\n\\n**Response:** {str(response_content)[:1900]}\"\n                    embed.color = discord.Color.green()\n                    embed.set_footer(\n                        text=f\"Requested by {ctx.author.display_name} | Thea Manager\")\n\n                    if len(str(response_content)) > 1900:\n                        embed.description += \"...\"\n                else:\n                    # Empty response\n                    embed.description = f\"**Query:** {message}\\n\\n**Response:** *No response from Thea Manager*\"\n                    embed.color = discord.Color.yellow()\n\n            except Exception as service_error:",
            "        try:\n            embed = self.create_base_embed(\" Thea Manager Status\")\n\n            # Check if Thea service is available\n            try:\n                thea_service = self.bot._get_thea_service(headless=True)\n                embed.add_field(\n                    name=\" Service Status\",\n                    value=\" Thea service initialized\",\n                    inline=False\n                )\n\n                # Try to check authentication status\n                # Note: TheaBrowserService doesn't have check_authentication_status method\n                # We'll assume it's authenticated if service is available\n                auth_status = \" Service Available (authentication assumed)\"\n                embed.add_field(\n                    name=\" Authentication\",\n                    value=auth_status,\n                    inline=False\n                )\n\n            except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\handlers\\discord_event_handlers.py",
          "count": 7,
          "lines": [
            "                    try:\n                        await self.bot.ensure_thea_session(\n                            allow_interactive=False,\n                            min_interval_minutes=self.bot.thea_min_interval_minutes\n                        )\n                    except Exception as e:",
            "                try:\n                    await self.bot.change_presence(\n                        activity=discord.Activity(\n                            type=discord.ActivityType.watching, name=\"the swarm \")\n                    )\n                except Exception as e:",
            "                try:\n                    await self.bot.send_startup_message()\n                except Exception as e:",
            "                try:\n                    self.bot.status_monitor.start_monitoring()\n                    self.logger.info(\" Status change monitor started (auto)\")\n                except Exception as e:",
            "        try:\n            # Get developer prefix from Discord user ID\n            developer_prefix = self.bot._get_developer_prefix(\n                str(message.author.id))\n\n            # Parse message format\n            recipient, message_content, message_prefix = parse_message_format(\n                content, has_prefix, developer_prefix)\n\n            # Validate recipient\n            if not await validate_recipient(message, recipient, self.logger):\n                return\n\n            # Send message to agent\n            await self._send_message_to_agent(message, recipient, message_content, message_prefix)\n\n        except Exception as e:",
            "        try:\n            # Update connection health\n            self.bot.connection_healthy = True\n            self.bot.last_heartbeat = time.time()\n\n            # Prevent duplicate startup messages on reconnection\n            if not hasattr(self.bot, '_startup_sent'):\n                self.logger.info(\n                    f\" Discord Commander Bot ready: {self.bot.user}\")\n                self.logger.info(f\" Guilds: {len(self.bot.guilds)}\")\n\n                # Initialize status change monitor\n                try:\n                    await self._initialize_status_monitor()\n                except Exception as e:",
            "        try:\n            from src.discord_commander.status_change_monitor import setup_status_monitor\n\n            # Initialize scheduler for integration\n            scheduler = None\n            try:\n                from src.orchestrators.overnight.scheduler import TaskScheduler\n                scheduler = TaskScheduler()\n                self.logger.info(\" Task scheduler initialized\")\n            except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\lifecycle\\swarm_snapshot_helpers.py",
          "count": 3,
          "lines": [
            "        try:\n            agent_data = _load_agent_status(status_file, logger)\n            if agent_data and agent_data.get(\"is_active\"):\n                active_count += 1\n                snapshot[\"active_agents\"].append(agent_data[\"agent_info\"])\n                if agent_data.get(\"recent_activity\"):\n                    snapshot[\"recent_activity\"].append(agent_data[\"recent_activity\"])\n                if agent_data.get(\"current_focus\"):\n                    snapshot[\"current_focus\"].append(agent_data[\"current_focus\"])\n        except Exception as e:",
            "    try:\n        with open(status_file, 'r', encoding='utf-8') as f:\n            status = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError) as e:",
            "    try:\n        workspace_root = Path(\"agent_workspaces\")\n        # Force fresh read by processing all status files\n        active_count = _process_agent_statuses(workspace_root, snapshot, logger)\n        snapshot[\"engagement_rate\"] = (active_count / 8 * 100) if active_count > 0 else 0.0\n        logger.debug(f\"Fresh snapshot generated at {snapshot['snapshot_timestamp']}: {active_count} active agents\")\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\lifecycle\\bot_lifecycle.py",
          "count": 8,
          "lines": [
            "        try:\n            await self._load_approval_commands()\n            await self._load_messaging_commands()\n            await self._load_swarm_showcase_commands()\n            await self._load_github_book_commands()\n            await self._load_trading_commands()\n            await self._load_webhook_commands()\n            await self._load_tools_commands()\n            await self._load_file_share_commands()\n            await self._load_music_commands()\n            self._log_command_summary()\n        except Exception as e:",
            "        try:\n            channel = await self._get_startup_channel()\n            if not channel:\n                return\n\n            # Wait a moment to ensure all status.json files are written/flushed\n            # This ensures we get a fresh snapshot, not stale cached data\n            await asyncio.sleep(0.5)\n            self.logger.info(\" Generating fresh swarm snapshot...\")\n            \n            snapshot = get_swarm_snapshot(self.logger)\n            self.logger.info(f\" Fresh snapshot generated: {len(snapshot.get('active_agents', []))} active agents\")\n            \n            snapshot_view, snapshot_embed = self._create_snapshot_view(snapshot)\n\n            embed = self._create_startup_embed(snapshot, snapshot_embed)\n\n            # Send snapshot view if available\n            if snapshot_view and snapshot_embed:\n                await channel.send(embed=snapshot_embed, view=snapshot_view)\n\n            # Send control panel with startup message\n            control_view = self.bot.gui_controller.create_control_panel()\n            await channel.send(embed=embed, view=control_view)\n            self.logger.info(\" Startup message with control panel sent successfully\")\n\n        except Exception as e:",
            "        try:\n            from ..base import CommandRegistry\n\n            # Use command registry for automatic discovery and registration\n            registry = CommandRegistry(self.bot, self.bot.gui_controller)\n            registration_results = await registry.discover_and_register_all()\n\n            # Log registration results\n            successful_registrations = sum(1 for success in registration_results.values() if success)\n            total_registrations = len(registration_results)\n\n            self.logger.info(f\" Command registration complete: {successful_registrations}/{total_registrations} modules registered\")\n\n            # Verify gui and control commands are registered\n            gui_command = self.bot.get_command(\"gui\")\n            if gui_command:\n                self.logger.info(f\" GUI command registered: {gui_command.name}\")\n            else:\n                self.logger.warning(\" GUI command not found after loading messaging commands\")\n\n            control_command = self.bot.get_command(\"control\")\n            if control_command:\n                self.logger.info(f\" Control command registered: {control_command.name}\")\n            else:\n                self.logger.warning(\" Control command not found after loading messaging commands\")\n            \n            self.logger.info(\" All messaging command cogs loaded (V2 compliant modules)\")\n        except Exception as e:",
            "        try:\n            from src.discord_commander.approval_commands import ApprovalCommands\n            await self.bot.add_cog(ApprovalCommands(self.bot))\n            self.logger.info(\" Approval commands loaded\")\n        except Exception as e:",
            "        try:\n            from src.discord_commander.file_share_commands import setup as setup_file_share\n            await setup_file_share(self.bot)\n            self.logger.info(\" File share commands loaded\")\n        except Exception as e:",
            "        try:\n            from src.discord_commander.music_commands import setup\n            await setup(self.bot)\n        except Exception as e:",
            "        try:\n            from src.discord_commander.tools_commands import ToolsCommands\n            await self.bot.add_cog(ToolsCommands(self.bot))\n            self.logger.info(\" Tools commands loaded\")\n        except Exception as e:",
            "        try:\n            from src.discord_commander.views.swarm_snapshot_view import SwarmSnapshotView\n            snapshot_view = SwarmSnapshotView(snapshot)\n            snapshot_embed = snapshot_view.create_snapshot_embed()\n            return snapshot_view, snapshot_embed\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\integrations\\service_integration_manager.py",
          "count": 5,
          "lines": [
            "        try:\n            if self.thea_last_refresh_path.exists():\n                data = json.loads(\n                    self.thea_last_refresh_path.read_text(encoding=\"utf-8\"))\n                return float(data.get(\"ts\"))\n        except Exception:",
            "        try:\n            self.thea_last_refresh_path.parent.mkdir(\n                parents=True, exist_ok=True)\n            self.thea_last_refresh_path.write_text(\n                json.dumps({\"ts\": ts}, indent=2), encoding=\"utf-8\")\n        except Exception as e:",
            "        try:\n            self.thea_min_interval_minutes = int(\n                os.getenv(\"THEA_MIN_INTERVAL_MINUTES\", \"60\"))\n        except ValueError:",
            "        try:\n            svc = self.get_thea_service(headless=False)\n            if not svc.initialize():\n                self.logger.error(\"Thea interactive init failed\")\n                return False\n            ok = svc.ensure_thea_authenticated(allow_manual=True)\n            svc.close()\n            if ok:\n                self.logger.info(\" Thea session refreshed interactively\")\n                self._write_last_thea_refresh(now)\n                return True\n            self.logger.error(\" Thea interactive refresh failed\")\n        except Exception as e:",
            "        try:\n            svc = self.get_thea_service(headless=True)\n            if not svc.initialize():\n                self.logger.error(\"Thea refresh: initialize failed\")\n                return False\n            ok = svc.ensure_thea_authenticated(allow_manual=False)\n            svc.close()\n            if ok:\n                self.logger.info(\" Thea session refreshed headlessly\")\n                self._write_last_thea_refresh(now)\n                return True\n            self.logger.warning(\" Thea headless refresh failed\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\config\\bot_config.py",
          "count": 2,
          "lines": [
            "                try:\n                    profile_data = json.loads(\n                        profile_file.read_text(encoding=\"utf-8\"))\n                    discord_user_id = profile_data.get(\"discord_user_id\")\n                    developer_name = (\n                        profile_data.get(\"discord_username\") or\n                        profile_data.get(\"developer_name\")\n                    )\n\n                    if discord_user_id and developer_name:\n                        user_map[str(discord_user_id)] = developer_name.upper()\n                        logger.debug(\n                            f\"Loaded Discord mapping: {discord_user_id}  {developer_name}\")\n                except Exception as e:",
            "    try:\n        config_data = json.loads(config_file.read_text(encoding=\"utf-8\"))\n        valid_mappings = {\n            k: v for k, v in config_data.items()\n            if not k.startswith(\"_\") and isinstance(v, str)\n        }\n        user_map.update(valid_mappings)\n        if valid_mappings:\n            logger.info(f\"Loaded {len(valid_mappings)} Discord user mappings from config\")\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\monitor\\resumer_logic.py",
          "count": 7,
          "lines": [
            "            try:\n                msg_obj = UnifiedMessage(\n                    content=message_content,\n                    sender=\"SYSTEM\",\n                    recipient=agent_id,\n                    message_type=UnifiedMessageType.SYSTEM_TO_AGENT,\n                    priority=UnifiedMessagePriority.URGENT,\n                    tags=[],\n                    category=MessageCategory.S2A\n                )\n                \n                next_task = next_task_info.get(\"title\", \"No task assigned\") if next_task_info else \"No task assigned\"\n                \n                rendered = render_message(\n                    msg_obj,\n                    template_key=\"SWARM_PULSE\",\n                    context=f\"Inactivity Detected: {safe_minutes} minutes\",\n                    actions=\"Resume by producing an artifact.\",\n                    fallback=\"Escalate if blocked.\",\n                    fsm_state=fsm_state,\n                    current_mission=current_mission,\n                    time_since_update=f\"{safe_minutes} minutes\",\n                    next_task=next_task,\n                    task_priority=str(next_task_info.get(\"priority\", \"normal\")) if next_task_info else \"normal\",\n                    task_points=str(next_task_info.get(\"points\", \"0\")) if next_task_info else \"0\",\n                    task_status=\"assigned\" if next_task_info else \"unassigned\"\n                )\n            except Exception:",
            "        try:\n            # Attempt-based send mode\n            attempt = self.resume_attempts.get(agent_id, 0) + 1\n            send_mode = \"enter\" if attempt == 1 else \"ctrl_enter\"\n\n            safe_minutes = (\n                f\"{summary.inactivity_duration_minutes:.1f}\"\n                if summary.inactivity_duration_minutes and summary.inactivity_duration_minutes != float('inf')\n                else \"unknown\"\n            )\n\n            # Get next task assignment\n            task_assignment_text = \"\"\n            next_task_info = None\n            try:\n                from src.services.unified_service_managers import UnifiedContractManager\n                contract_manager = UnifiedContractManager()\n                task_result = contract_manager.get_next_task(agent_id)\n                \n                if task_result and task_result.get(\"status\") == \"assigned\" and task_result.get(\"task\"):\n                    next_task_info = task_result.get(\"task\")\n                    task_assignment_text = self._format_task_assignment(next_task_info, task_result.get(\"source\", \"contract_system\"), agent_id)\n                elif task_result and task_result.get(\"status\") == \"no_tasks\":\n                    task_assignment_text = \"\\n **NO TASKS AVAILABLE**\\nCheck inbox or continue current mission.\"\n            except Exception as e:",
            "        try:\n            # Find channel\n            channel = None\n            for guild in self.bot.guilds:\n                for ch in guild.channels:\n                    if ch.name in [\"agent-status\", \"captain-updates\", \"swarm-status\"] and isinstance(ch, discord.TextChannel):\n                        channel = ch\n                        break\n                if channel: break\n            \n            if channel:\n                embed = StatusEmbedFactory.create_resumer_embed(agent_id, prompt, summary)\n                await channel.send(embed=embed)\n        except Exception as e:",
            "        try:\n            from src.core.messaging_pyautogui import PyAutoGUIMessagingDelivery\n            \n            # Prepare render context\n            fsm_state = \"UNKNOWN\"\n            current_mission = \"Not specified\"\n            try:\n                status_file = self.workspace_path / agent_id / \"status.json\"\n                if status_file.exists():\n                    with open(status_file, \"r\", encoding=\"utf-8\") as f:\n                        status = json.load(f)\n                    fsm_state = status.get(\"fsm_state\") or status.get(\"status\") or fsm_state\n                    current_mission = status.get(\"current_mission\", current_mission)\n            except Exception:",
            "        try:\n            from src.core.optimized_stall_resume_prompt import generate_optimized_resume_prompt\n            \n            status_file = self.workspace_path / agent_id / \"status.json\"\n            if not await asyncio.to_thread(status_file.exists):\n                return None\n                \n            status = await asyncio.to_thread(lambda: json.loads(status_file.read_text(encoding='utf-8')))\n            \n            scheduled_section = \"\"\n            if pending_tasks:\n                # (Simplified) Just note tasks exist\n                scheduled_section = f\"You have {len(pending_tasks)} pending tasks.\"\n\n            return generate_optimized_resume_prompt(\n                agent_id=agent_id,\n                fsm_state=status.get(\"status\", \"active\"),\n                last_mission=status.get(\"current_mission\", \"Unknown\"),\n                stall_duration_minutes=summary.inactivity_duration_minutes,\n                scheduler=self.scheduler,\n                scheduled_tasks_section=scheduled_section\n            )\n        except Exception as e:",
            "        try:\n            inactivity_threshold_minutes = 5.0  # 5 minutes of inactivity\n\n            # EnhancedAgentActivityDetector.detect_agent_activity() returns a dict\n            activity_data = activity_detector.detect_agent_activity(agent_id)\n            \n            summary = ActivitySummary(activity_data)\n\n            # If agent is inactive for threshold duration\n            if not summary.is_active or summary.inactivity_duration_minutes >= inactivity_threshold_minutes:\n                # Check scheduler for pending tasks\n                pending_tasks = []\n                if self.scheduler:\n                    try:\n                        from src.orchestrators.overnight.scheduler_integration import SchedulerStatusMonitorIntegration\n                        integration = SchedulerStatusMonitorIntegration(\n                            scheduler=self.scheduler, status_monitor=None) # Pass None or mock if needed\n                        pending_tasks = integration.get_pending_tasks_for_agent(agent_id)\n                        \n                        # Mark agent as inactive in scheduler\n                        integration.mark_agent_inactive(\n                            agent_id, summary.inactivity_duration_minutes)\n                    except Exception as e:",
            "        try:\n            inbox_dir = self.workspace_path / \"Agent-4\" / \"inbox\"\n            if not await asyncio.to_thread(inbox_dir.exists):\n                return None\n                \n            def _find_pattern():\n                files = list(inbox_dir.glob(\"CAPTAIN_RESTART_PATTERN*.md\"))\n                if not files: return None\n                return max(files, key=lambda p: p.stat().st_mtime)\n            \n            pattern_file = await asyncio.to_thread(_find_pattern)\n            if not pattern_file:\n                return None\n                \n            content = await asyncio.to_thread(pattern_file.read_text, encoding='utf-8')\n            \n            # Prepend header\n            header = f\" RESUMER PROMPT - Captain Inactivity Detected\\n**Inactivity**: {inactivity_minutes:.1f} min\\n---\\n\"\n            return header + content\n            \n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\monitor\\status_embeds.py",
          "count": 1,
          "lines": [
            "            try:\n                points_int = int(points_val)\n                embed.add_field(\n                    name=\"Points Earned\",\n                    value=f\"+{points_int} points\",\n                    inline=True\n                )\n            except (ValueError, TypeError):"
          ]
        },
        {
          "file": "src\\discord_commander\\ui_components\\control_panel_embeds.py",
          "count": 1,
          "lines": [
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\ui_components\\control_panel_buttons.py",
          "count": 4,
          "lines": [
            "        try:\n            bot = interaction.client\n\n            if not hasattr(bot, 'status_monitor'):\n                embed = self._create_error_embed(\n                    \" Status Monitor\",\n                    \" Status monitor not initialized.\"\n                )\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n                return\n\n            # Get current status\n            status_monitor = bot.status_monitor\n            is_running = False\n            interval = 15\n\n            if hasattr(status_monitor, 'monitor_status_changes'):\n                is_running = status_monitor.monitor_status_changes.is_running()\n                if hasattr(status_monitor, 'check_interval'):\n                    interval = status_monitor.check_interval\n\n            status_text = \" RUNNING\" if is_running else \" STOPPED\"\n            status_color = discord.Color.green() if is_running else discord.Color.red()\n\n            embed = discord.Embed(\n                title=\" Status Change Monitor\",\n                description=f\"**Status:** {status_text}\\n**Check Interval:** {interval} seconds\\n\\nStatus refreshed successfully.\",\n                color=status_color,\n            )\n\n            await interaction.response.send_message(embed=embed, ephemeral=True)\n\n        except Exception as e:",
            "        try:\n            bot = interaction.client\n            if hasattr(bot, \"status_monitor\"):\n                bot.status_monitor.start_monitoring()\n                embed = self._create_success_embed(\n                    \" Monitor Started\",\n                    \" Status monitor started! Checking every 15 seconds.\"\n                )\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n            else:\n                embed = self._create_error_embed(\n                    \" Monitor Error\",\n                    \" Status monitor not initialized yet.\"\n                )\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n        except Exception as e:",
            "        try:\n            bot = interaction.client\n            if hasattr(bot, \"status_monitor\"):\n                bot.status_monitor.stop_monitoring()\n                embed = self._create_success_embed(\n                    \" Monitor Stopped\",\n                    \" Status monitor stopped.\"\n                )\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n            else:\n                embed = self._create_error_embed(\n                    \" Monitor Error\",\n                    \" Status monitor not available.\"\n                )\n                await interaction.response.send_message(embed=embed, ephemeral=True)\n        except Exception as e:",
            "try:\n    import discord\n    from discord.ext import commands\n    DISCORD_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\command_base.py",
          "count": 2,
          "lines": [
            "        try:\n            await ctx.send(embed=embed)\n        except Exception as send_error:",
            "        try:\n            return await ctx.send(content=content, embed=embed, **kwargs)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\command_registry.py",
          "count": 4,
          "lines": [
            "        try:\n            # Import the module\n            module = importlib.import_module(module_name)\n\n            # Find all command cog classes\n            cog_classes = self._find_command_cogs(module)\n\n            if not cog_classes:\n                self.logger.warning(f\"No command cogs found in {module_name}\")\n                return False\n\n            # Register each cog\n            for cog_class in cog_classes:\n                try:\n                    await self._register_cog(cog_class)\n                except Exception as e:",
            "        try:\n            cog_instance = cog_class(self.bot, self.gui_controller)\n            await self.bot.add_cog(cog_instance)\n            self.logger.debug(f\"Registered cog: {cog_class.__name__}\")\n        except Exception as e:",
            "        try:\n            import discord\n            from discord.ext import commands\n        except ImportError:",
            "class CommandRegistry:\n    \"\"\"\n    Registry for automatic command discovery and registration.\n\n    Eliminates repetitive command loading code by auto-discovering\n    all command cogs and registering them with proper dependencies.\n    \"\"\"\n\n    def __init__(self, bot: \"UnifiedDiscordBot\", gui_controller: \"DiscordGUIController\"):\n        \"\"\"Initialize command registry.\"\"\"\n        self.bot = bot\n        self.gui_controller = gui_controller\n        self.logger = logging.getLogger(__name__)\n\n        # Command modules to auto-discover\n        self.command_modules = [\n            \"src.discord_commander.commands.control_panel_commands\",\n            \"src.discord_commander.commands.core_messaging_commands\",\n            \"src.discord_commander.commands.messaging_core_commands\",\n            \"src.discord_commander.commands.utility_commands\",\n            \"src.discord_commander.commands.system_control_commands\",\n            \"src.discord_commander.commands.agent_management_commands\",\n            \"src.discord_commander.commands.onboarding_commands\",\n            \"src.discord_commander.commands.profile_commands\",\n            \"src.discord_commander.commands.placeholder_commands\",\n            \"src.discord_commander.commands.messaging_monitor_commands\",\n            \"src.discord_commander.commands.bot_messaging_commands\",\n            \"src.discord_commander.commands.thea_commands\",\n        ]\n\n    async def discover_and_register_all(self) -> Dict[str, bool]:\n        \"\"\"\n        Auto-discover and register all command cogs.\n\n        Returns dict mapping cog names to registration success.\n        \"\"\"\n        results = {}\n\n        for module_name in self.command_modules:\n            try:\n                success = await self._register_command_module(module_name)\n                results[module_name.split('.')[-1]] = success\n\n                if success:\n                    self.logger.info(f\" Registered command module: {module_name}\")\n                else:\n                    self.logger.warning(f\" Failed to register command module: {module_name}\")\n\n            except Exception as e:"
          ]
        },
        {
          "file": "src\\discord_commander\\base\\unified_command.py",
          "count": 1,
          "lines": [
            "try:\n    import discord\n    from discord.ext import commands\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\ai_automation\\utils\\filesystem.py",
          "count": 1,
          "lines": [
            "    try:\n        # Get current permissions\n        mode = path.stat().st_mode\n\n        # Add execute permissions for user, group, and others\n        new_mode = mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH\n        path.chmod(new_mode)\n\n        logger.info(f\"Made {path} executable\")\n\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\database.py",
          "count": 2,
          "lines": [
            "        try:\n            with self.get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"SELECT 1\")\n                result = cursor.fetchone()\n                return result is not None\n        except Exception as e:",
            "try:\n    import psycopg\n\n    PSYCOPG_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\runner.py",
          "count": 2,
          "lines": [
            "        try:\n            self.logger.info(f\"Processing conversation {conversation_id}\")\n\n            # Step 1: Fetch conversation\n            conversation = self._fetch_conversation(conversation_id)\n            if not conversation:\n                self.logger.error(f\"Failed to fetch conversation {conversation_id}\")\n                return False\n\n            # Step 2: Redact PII\n            redacted_conversation = self.redactor.redact_conversation(conversation)\n            self.logger.info(f\"Redacted conversation {conversation_id}\")\n\n            # Step 3: Generate summary\n            summary = self.summarizer.summarize_conversation(redacted_conversation, conversation_id)\n            if not summary:\n                self.logger.error(f\"Failed to summarize conversation {conversation_id}\")\n                return False\n\n            # Step 4: Validate summary\n            if not SummarySchema.validate(summary):\n                self.logger.error(f\"Summary validation failed for conversation {conversation_id}\")\n                return False\n\n            # Step 5: Save summary\n            summary_path = Path(self.config.get(\"paths.summaries\")) / f\"{conversation_id}.json\"\n            if not SummarySchema.save_summary(summary, str(summary_path)):\n                self.logger.error(f\"Failed to save summary for conversation {conversation_id}\")\n                return False\n\n            # Step 6: Generate embeddings\n            embeddings = self.embedding_builder.generate_summary_embeddings(summary)\n            self.embedding_builder.save_embeddings(conversation_id, embeddings)\n\n            # Step 7: Build index\n            self.index_builder.build_index_from_summary(summary)\n\n            self.logger.info(f\"Successfully processed conversation {conversation_id}\")\n            return True\n\n        except Exception as e:",
            "        try:\n            stats_dir = Path(\"ops/metrics\")\n            stats_dir.mkdir(parents=True, exist_ok=True)\n\n            stats_file = stats_dir / f\"batch_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n\n            with open(stats_file, \"w\") as f:\n                json.dump(self.stats, f, indent=2)\n\n            self.logger.info(f\"Statistics saved to {stats_file}\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\job_queue.py",
          "count": 2,
          "lines": [
            "                                    try:\n                                        callback(job)\n                                    except Exception as e:",
            "            try:\n                job = None\n                with self._lock:\n                    if self._pending_jobs:\n                        job = self._pending_jobs.pop(0)\n                        job.status = JobStatus.RUNNING\n                        job.started_at = time.time()\n                        self._running_jobs[job.id] = job\n\n                if job:\n                    try:\n                        result = job.task(*job.args, **job.kwargs)\n                        job.result = result\n                        job.status = JobStatus.COMPLETED\n                    except Exception as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\redactor.py",
          "count": 2,
          "lines": [
            "                try:\n                    self._patterns[pattern_name] = re.compile(pattern_str, re.IGNORECASE)\n                except re.error as e:",
            "        try:\n            compiled_pattern = re.compile(pattern, re.IGNORECASE)\n            self._patterns[name] = compiled_pattern\n            return True\n        except re.error as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\summarizer.py",
          "count": 5,
          "lines": [
            "            try:\n                return self._llm_summarize(text, schema)\n            except Exception as e:",
            "        try:\n            if provider == \"openai\":\n                return self._initialize_openai_client()\n            elif provider == \"anthropic\":\n                return self._initialize_anthropic_client()\n            elif provider == \"ollama\":\n                return self._initialize_ollama_client()\n            else:\n                self.logger.warning(f\"Unknown LLM provider: {provider}, falling back to extractive\")\n                return None\n        except Exception as e:",
            "        try:\n            import anthropic\n            api_key = self.llm_config.get(\"api_key\") or os.getenv(\"ANTHROPIC_API_KEY\")\n            if not api_key:\n                raise ValueError(\"Anthropic API key not found\")\n\n            client = anthropic.Anthropic(api_key=api_key)\n            self.logger.info(\" Anthropic LLM client initialized\")\n            return client\n        except ImportError:",
            "        try:\n            import ollama\n            model = self.llm_config.get(\"model\", \"llama2\")\n            client = ollama.Client()\n            # Test connection\n            client.list()\n            self.logger.info(f\" Ollama LLM client initialized with model: {model}\")\n            return {\"client\": client, \"model\": model}\n        except ImportError:",
            "        try:\n            import openai\n            api_key = self.llm_config.get(\"api_key\") or os.getenv(\"OPENAI_API_KEY\")\n            if not api_key:\n                raise ValueError(\"OpenAI API key not found\")\n\n            client = openai.OpenAI(api_key=api_key)\n            self.logger.info(\" OpenAI LLM client initialized\")\n            return client\n        except ImportError:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\embedding_builder.py",
          "count": 4,
          "lines": [
            "            try:\n                return self._real_embedding(text)\n            except Exception as e:",
            "        try:\n            from sentence_transformers import SentenceTransformer\n\n            model_name = self.model_name\n            if model_name == \"sentence-transformers\":\n                model_name = \"all-MiniLM-L6-v2\"  # Default model\n\n            model = SentenceTransformer(model_name)\n            self.dimension = model.get_sentence_embedding_dimension()\n            self.logger.info(f\" SentenceTransformers model initialized: {model_name} (dim: {self.dimension})\")\n            return model\n        except ImportError:",
            "        try:\n            if provider == \"sentence-transformers\" or provider.startswith(\"sentence-transformers/\"):\n                return self._initialize_sentence_transformers()\n            elif provider.startswith(\"openai\"):\n                return self._initialize_openai_embeddings()\n            elif provider == \"hash-based\" or provider == \"default\":\n                # Keep hash-based as fallback\n                return None\n            else:\n                self.logger.warning(f\"Unknown embedding provider: {provider}, using hash-based\")\n                return None\n        except Exception as e:",
            "        try:\n            import openai\n            api_key = self.config.get(\"api_key\") or os.getenv(\"OPENAI_API_KEY\")\n            if not api_key:\n                raise ValueError(\"OpenAI API key not found\")\n\n            client = openai.OpenAI(api_key=api_key)\n            self.dimension = 1536  # Ada-002 dimension\n            self.logger.info(\" OpenAI embeddings initialized\")\n            return client\n        except ImportError:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\advanced_reasoning.py",
          "count": 4,
          "lines": [
            "            try:\n                # Try to parse as JSON, wrap if not\n                json.loads(response)\n                return response\n            except json.JSONDecodeError:",
            "        try:\n            # Check cache first\n            cache_key = self._generate_cache_key(context)\n            if cache_key in self.response_cache:\n                cached_result, cache_time = self.response_cache[cache_key]\n                if time.time() - cache_time < self.cache_ttl:\n                    self.logger.debug(\"Returning cached reasoning result\")\n                    return cached_result\n\n            # Perform reasoning\n            result = self._perform_reasoning(context)\n\n            # Cache result\n            self.response_cache[cache_key] = (result, time.time())\n            self._cleanup_cache()\n\n            return result\n\n        except Exception as e:",
            "        try:\n            if provider == \"openai\":\n                import openai\n                api_key = self.config.get(\"api_key\") or __import__(\"os\").getenv(\"OPENAI_API_KEY\")\n                if api_key:\n                    return openai.OpenAI(api_key=api_key)\n                else:\n                    self.logger.warning(\"OpenAI API key not found\")\n                    return None\n            else:\n                self.logger.warning(f\"Unsupported LLM provider: {provider}\")\n                return None\n        except ImportError as e:",
            "        try:\n            response = self.llm_client.chat.completions.create(\n                model=self.default_model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                max_tokens=context.max_tokens,\n                temperature=context.temperature\n            )\n\n            return response.choices[0].message.content, response.usage.total_tokens\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\scrapers\\browser_manager.py",
          "count": 5,
          "lines": [
            "            try:\n                self.driver.quit()\n                logger.info(\" Driver closed successfully\")\n            except Exception as e:",
            "        try:\n            if self.use_undetected:\n                return self._create_undetected_driver()\n            else:\n                return self._create_standard_driver()\n        except Exception as e:",
            "        try:\n            options = Options()\n\n            if self.headless:\n                options.add_argument(\"--headless\")\n\n            # Add anti-detection arguments\n            options.add_argument(\"--no-sandbox\")\n            options.add_argument(\"--disable-dev-shm-usage\")\n            options.add_argument(\"--disable-blink-features=AutomationControlled\")\n            options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n            options.add_experimental_option(\"useAutomationExtension\", False)\n\n            # Create standard driver\n            driver = webdriver.Chrome(options=options)\n\n            # Execute script to remove webdriver property\n            driver.execute_script(\n                \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n            )\n\n            logger.info(\" Standard Chrome driver created successfully\")\n            return driver\n\n        except Exception as e:",
            "        try:\n            options = uc.ChromeOptions()\n            if self.headless:\n                options.add_argument(\"--headless\")\n\n            # Add basic anti-detection arguments (undetected-chromedriver handles the rest)\n            options.add_argument(\"--no-sandbox\")\n            options.add_argument(\"--disable-dev-shm-usage\")\n\n            # Create undetected driver\n            driver = uc.Chrome(options=options)\n\n            logger.info(\" Undetected Chrome driver created successfully\")\n            return driver\n\n        except ImportError:",
            "try:\n    import undetected_chromedriver as uc\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\scrapers\\scraper_conversation_methods.py",
          "count": 2,
          "lines": [
            "        try:\n            logger.info(\" Starting conversation extraction...\")\n            progress_stats = scraper.get_progress_stats()\n            if progress_stats[\"total_processed\"] > 0:\n                logger.info(\n                    f\" Resume mode: {progress_stats['successful']} conversations already processed\"\n                )\n            conversations = scraper.get_conversation_list(progress_callback)\n            if limit:\n                conversations = conversations[:limit]\n            if skip_processed:\n                original_count = len(conversations)\n                conversations = [\n                    conv for conv in conversations if not scraper._is_conversation_processed(conv)\n                ]\n                skipped_count = original_count - len(conversations)\n                if skipped_count > 0:\n                    logger.info(f\" Skipping {skipped_count} already processed conversations\")\n            logger.info(f\" Found {len(conversations)} conversations to extract\")\n            if len(conversations) == 0:\n                logger.info(\" All conversations already processed!\")\n                return {\n                    \"total\": 0,\n                    \"extracted\": 0,\n                    \"failed\": 0,\n                    \"skipped\": progress_stats[\"total_processed\"],\n                    \"errors\": [],\n                }\n            stats = {\n                \"total\": len(conversations),\n                \"extracted\": 0,\n                \"failed\": 0,\n                \"skipped\": progress_stats[\"total_processed\"],\n                \"errors\": [],\n            }\n            total_conversations = len(conversations)\n            for i, conversation in enumerate(conversations):\n                try:\n                    chronological_number = total_conversations - i\n                    logger.info(\n                        f\" Extracting conversation {i+1}/{total_conversations} (will be #{chronological_number} chronologically): {conversation.get('title', 'Unknown')}\"\n                    )\n                    if scraper.extract_conversation(\n                        conversation[\"url\"], output_dir, chronological_number\n                    ):\n                        stats[\"extracted\"] += 1\n                        scraper._mark_conversation_processed(conversation, success=True)\n                    else:\n                        stats[\"failed\"] += 1\n                        scraper._mark_conversation_processed(conversation, success=False)\n                        stats[\"errors\"].append(\n                            f\"Failed to extract {conversation.get('id', 'unknown')}\"\n                        )\n                    if progress_callback:\n                        progress_callback(i + 1, total_conversations)\n                except Exception as e:",
            "        try:\n            logger.info(\" Starting SMART conversation extraction...\")\n            Path(output_dir).mkdir(parents=True, exist_ok=True)\n            logger.info(\" Step 1: Counting total conversations...\")\n            total_count = scraper._count_total_conversations()\n            logger.info(f\" Found {total_count} total conversations\")\n            if limit:\n                total_count = min(total_count, limit)\n                logger.info(f\" Limited to {total_count} conversations\")\n            logger.info(\" Step 2: Extracting conversations as we discover them...\")\n            conversations = scraper._get_conversations_with_smart_extraction(\n                total_count, output_dir, progress_callback, skip_processed\n            )\n            stats = {\"total\": total_count, \"extracted\": 0, \"failed\": 0, \"skipped\": 0, \"errors\": []}\n            stats[\"extracted\"] = len(\n                [f for f in Path(output_dir).glob(\"conversation_*.json\") if f.exists()]\n            )\n            logger.info(\" Step 3: Reversing file numbering for chronological order...\")\n            scraper._reverse_file_numbering(output_dir, total_count)\n            logger.info(\n                f\" SMART extraction complete: {stats['extracted']}/{stats['total']} successful\"\n            )\n            return stats\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\scrapers\\scraper_extraction.py",
          "count": 1,
          "lines": [
            "        try:\n            output_path = Path(output_dir)\n            json_files = sorted(output_path.glob(\"*.json\"))\n            for idx, file in enumerate(json_files, 1):\n                new_number = total_count - idx + 1\n                new_name = f\"conversation_{new_number:04d}.json\"\n                new_path = output_path / new_name\n                temp_path = output_path / f\"temp_{new_number:04d}.json\"\n                file.rename(temp_path)\n            temp_files = sorted(output_path.glob(\"temp_*.json\"))\n            for temp_file in temp_files:\n                final_name = temp_file.name.replace(\"temp_\", \"\")\n                temp_file.rename(output_path / final_name)\n            logger.info(f\" Reversed numbering for {len(json_files)} files\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\scrapers\\scraper_progress.py",
          "count": 2,
          "lines": [
            "        try:\n            progress_path = Path(self.progress_file)\n            if progress_path.exists():\n                with open(progress_path) as f:\n                    return json.load(f)\n            return {}\n        except Exception as e:",
            "        try:\n            progress_path = Path(self.progress_file)\n            progress_path.parent.mkdir(parents=True, exist_ok=True)\n            with open(progress_path, \"w\") as f:\n                json.dump(self.processed_conversations, f, indent=2)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\scrapers\\login_handler.py",
          "count": 6,
          "lines": [
            "                try:\n                    element = driver.find_element(By.XPATH, indicator)\n                    if element.is_displayed():\n                        return False\n                except:",
            "        try:\n            # Look for 2FA input field\n            totp_field = driver.find_element(By.XPATH, \"//input[@name='totp']\")\n\n            # Generate TOTP code\n            totp = pyotp.TOTP(self.totp_secret)\n            code = totp.now()\n\n            # Enter code\n            totp_field.clear()\n            totp_field.send_keys(code)\n\n            # Submit\n            submit_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n            submit_button.click()\n            time.sleep(2)\n\n            return True\n\n        except Exception as e:",
            "        try:\n            # Look for logged-in indicators\n            logged_in_indicators = [\n                \"//button[contains(@data-testid, 'send-button')]\",\n                \"//div[contains(@class, 'conversation')]\",\n                \"//div[contains(@data-testid, 'conversation-turn')]\",\n                \"//div[contains(@class, 'markdown')]\",\n            ]\n\n            for indicator in logged_in_indicators:\n                try:\n                    element = driver.find_element(By.XPATH, indicator)\n                    if element.is_displayed():\n                        return True\n                except:",
            "        try:\n            # Navigate to ChatGPT\n            driver.get(\"https://chat.openai.com\")\n            time.sleep(3)\n\n            # Check if already logged in\n            if self._is_logged_in(driver):\n                logger.info(\" Already logged in to ChatGPT\")\n                return True\n\n            # Try automated login if credentials provided\n            if self.username and self.password:\n                if self._automated_login(driver):\n                    return True\n\n            # Try manual login if allowed\n            if allow_manual:\n                logger.info(\" Attempting manual login...\")\n                return self._manual_login(driver, manual_timeout)\n\n            logger.error(\" Login failed - no credentials and manual login not allowed\")\n            return False\n\n        except Exception as e:",
            "        try:\n            logger.info(\" Attempting automated login...\")\n\n            # Wait for login form\n            wait = WebDriverWait(driver, self.timeout)\n\n            # Find and fill username\n            username_field = wait.until(EC.presence_of_element_located((By.NAME, \"username\")))\n            username_field.clear()\n            username_field.send_keys(self.username)\n\n            # Click continue\n            continue_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n            continue_button.click()\n            time.sleep(2)\n\n            # Find and fill password\n            password_field = wait.until(EC.presence_of_element_located((By.NAME, \"password\")))\n            password_field.clear()\n            password_field.send_keys(self.password)\n\n            # Click login\n            login_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n            login_button.click()\n            time.sleep(3)\n\n            # Handle 2FA if needed\n            if self.totp_secret:\n                if self._handle_2fa(driver):\n                    logger.info(\" 2FA completed\")\n                else:\n                    logger.warning(\" 2FA failed, continuing...\")\n\n            # Check if login successful\n            if self._is_logged_in(driver):\n                logger.info(\" Automated login successful\")\n                return True\n            else:\n                logger.warning(\" Automated login failed\")\n                return False\n\n        except Exception as e:",
            "        try:\n            logger.info(f\" Manual login timeout: {timeout} seconds\")\n            logger.info(\" Please log in manually in the browser...\")\n\n            start_time = time.time()\n            while time.time() - start_time < timeout:\n                if self._is_logged_in(driver):\n                    logger.info(\" Manual login successful\")\n                    return True\n                time.sleep(2)\n\n            logger.error(\" Manual login timeout\")\n            return False\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\ai_training\\dreamvault\\scrapers\\scraper_login.py",
          "count": 2,
          "lines": [
            "        try:\n            # Ensure on ChatGPT domain before loading cookies\n            if not driver.current_url.startswith(\"https://chat.openai.com\"):\n                logger.info(\" Navigating to ChatGPT before loading cookies...\")\n                driver.get(\"https://chat.openai.com\")\n                time.sleep(3)\n            # Try to load cookies first\n            if cookie_manager.has_valid_cookies():\n                logger.info(\" Found saved cookies, attempting auto-login...\")\n                cookie_manager.load_cookies(driver)\n                time.sleep(3)\n                logger.info(\" Refreshing page to apply cookies...\")\n                driver.refresh()\n                time.sleep(3)\n                logger.info(\" Testing if cookies still work...\")\n                if login_handler._is_logged_in(driver):\n                    logger.info(\"  SUCCESS! Cookies work! Auto-login successful!\")\n                    if ScraperLoginHelper._handle_workspace_selection(driver):\n                        logger.info(\" Workspace selection handled\")\n                    logger.info(\" Skipping manual login - proceeding directly to scraping...\")\n                    return True\n                else:\n                    logger.warning(\" Cookies expired or invalid\")\n                    logger.info(\" Falling back to manual login...\")\n            else:\n                logger.info(\" No saved cookies found\")\n                logger.info(\" Will need manual login this time...\")\n            # Perform manual login\n            logger.info(\" Starting manual login process...\")\n            if login_handler.ensure_login(driver, allow_manual, manual_timeout):\n                cookie_manager.save_cookies(driver)\n                logger.info(\" Login successful! Cookies saved for next time.\")\n                logger.info(\" Next run will auto-login with these cookies!\")\n                return True\n            else:\n                logger.error(\" Login failed\")\n                return False\n        except Exception as e:",
            "        try:\n            from selenium.webdriver.common.by import By\n            from selenium.webdriver.support import expected_conditions as EC\n            from selenium.webdriver.support.ui import WebDriverWait\n            from src.core.config.timeout_constants import TimeoutConstants\n\n            wait = WebDriverWait(driver, 5)\n            modal = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"[role='dialog']\")))\n            logger.info(\" Workspace selection modal detected\")\n            buttons = modal.find_elements(By.TAG_NAME, \"button\")\n            for button in buttons:\n                if \"stay\" in button.text.lower() or \"personal\" in button.text.lower():\n                    button.click()\n                    logger.info(\" Selected personal workspace\")\n                    time.sleep(2)\n                    return True\n            logger.warning(\" Could not find workspace selection button\")\n            return False\n        except:"
          ]
        },
        {
          "file": "src\\application\\use_cases\\assign_task_uc.py",
          "count": 1,
          "lines": [
            "        try:\n            # Retrieve the task\n            task = self.tasks.get(TaskId(request.task_id))\n            if not task:\n                return AssignTaskResponse(\n                    success=False, error_message=f\"Task {request.task_id} not found\"\n                )\n\n            # Determine target agent\n            if request.agent_id:\n                # Manual assignment\n                agent = self.agents.get(AgentId(request.agent_id))\n                if not agent:\n                    return AssignTaskResponse(\n                        success=False, error_message=f\"Agent {request.agent_id} not found\"\n                    )\n\n                # Validate assignment\n                if not self.assignment_service.validate_assignment(task, agent):\n                    return AssignTaskResponse(\n                        success=False,\n                        error_message=f\"Agent {request.agent_id} cannot handle task {request.task_id}\",\n                    )\n            else:\n                # Auto-assignment - find best agent\n                available_agents = list(self.agents.get_available())\n                agent = self.assignment_service.find_best_agent_for_task(task, available_agents)\n\n                if not agent:\n                    return AssignTaskResponse(\n                        success=False, error_message=\"No suitable agent available for task\"\n                    )\n\n            # Perform assignment\n            task.assign_to(agent.id)\n            agent.assign_task(task.id)\n\n            # Save changes\n            self.tasks.save(task)\n            self.agents.save(agent)\n\n            # Publish domain event\n            event = TaskAssigned(\n                event_id=f\"task-assigned-{task.id}-{agent.id}\",\n                task_id=task.id,\n                agent_id=agent.id,\n                assigned_at=task.assigned_at,\n            )\n\n            self.message_bus.publish(\"task.assigned\", event.to_dict())\n\n            # Log success\n            self.logger.info(\n                \"Task assigned successfully\",\n                task_id=task.id,\n                agent_id=agent.id,\n                assignment_type=\"manual\" if request.agent_id else \"auto\",\n            )\n\n            return AssignTaskResponse(success=True, task=task, agent=agent)\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\application\\use_cases\\complete_task_uc.py",
          "count": 1,
          "lines": [
            "        try:\n            # Retrieve the task\n            task = self.tasks.get(TaskId(request.task_id))\n            if not task:\n                return CompleteTaskResponse(\n                    success=False, error_message=f\"Task {request.task_id} not found\"\n                )\n\n            # Verify task is assigned to the requesting agent\n            if task.assigned_agent_id != AgentId(request.agent_id):\n                return CompleteTaskResponse(\n                    success=False,\n                    error_message=f\"Task {request.task_id} is not assigned to agent {request.agent_id}\",\n                )\n\n            # Retrieve the agent\n            agent = self.agents.get(AgentId(request.agent_id))\n            if not agent:\n                return CompleteTaskResponse(\n                    success=False, error_message=f\"Agent {request.agent_id} not found\"\n                )\n\n            # Complete the task\n            task.complete()\n            agent.complete_task(task.id)\n\n            # Save changes\n            self.tasks.save(task)\n            self.agents.save(agent)\n\n            # Publish domain event\n            event = TaskCompleted(\n                event_id=f\"task-completed-{task.id}-{agent.id}\",\n                task_id=task.id,\n                agent_id=agent.id,\n                completed_at=task.completed_at,\n            )\n\n            self.message_bus.publish(\"task.completed\", event.to_dict())\n\n            # Log success\n            self.logger.info(\n                \"Task completed successfully\",\n                task_id=task.id,\n                agent_id=agent.id,\n                completion_time=task.completed_at.isoformat(),\n            )\n\n            return CompleteTaskResponse(success=True, task=task, agent=agent)\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\architecture\\design_patterns.py",
          "count": 3,
          "lines": [
            "            try:\n                observer.update(data)\n            except Exception as e:",
            "        try:\n            result = {'pattern': pattern.name, 'type': pattern.pattern_type\n                .value, 'applied': True, 'context': context, 'timestamp':\n                datetime.now().isoformat()}\n            self.logger.info(f' Applied pattern: {pattern.name}')\n            return result\n        except Exception as e:",
            "        try:\n            return self._creators[name]()\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\architecture\\system_integration.py",
          "count": 9,
          "lines": [
            "        try:\n            # Import message queue\n            from src.core.message_queue import MessageQueue\n            \n            # Get or create queue instance\n            if queue_instance is None:\n                queue_instance = MessageQueue()\n            \n            # Register as message queue endpoint\n            queue_url = f\"file://{queue_instance.config.queue_directory}\"\n            success = self.register_endpoint(\n                \"message_queue\",\n                IntegrationType.MESSAGE_QUEUE,\n                queue_url\n            )\n            if success:\n                self.logger.info(' Message queue registered in integration framework')\n            return success\n        except Exception as e:",
            "        try:\n            endpoint = IntegrationEndpoint(name=name, integration_type=\n                integration_type, endpoint_url=endpoint_url, status=\n                IntegrationStatus.PENDING, last_checked=datetime.now().\n                isoformat(), response_time=0.0, error_count=0)\n            self.endpoints[name] = endpoint\n            self.integration_count += 1\n            self.logger.info(\n                f' Registered endpoint: {name} ({integration_type.value})')\n            return True\n        except Exception as e:",
            "        try:\n            endpoint = self.endpoints[name]\n            endpoint.status = IntegrationStatus.CONNECTED\n            endpoint.last_checked = datetime.now().isoformat()\n            endpoint.response_time = 0.1\n            self.logger.info(f' Health check passed for endpoint: {name}')\n            return {'endpoint': name, 'status': endpoint.status.value,\n                'response_time': endpoint.response_time, 'last_checked':\n                endpoint.last_checked}\n        except Exception as e:",
            "        try:\n            from src.core.message_queue import MessageQueue\n            queue = MessageQueue()\n            \n            # Get queue statistics if available\n            try:\n                stats = queue.get_statistics()\n                queue_size = stats.get('queue_size', 0) if isinstance(stats, dict) else 0\n            except:",
            "        try:\n            from src.infrastructure.persistence.database_connection import DatabaseConnection\n            from src.infrastructure.persistence.persistence_models import PersistenceConfig\n            config = PersistenceConfig()\n            db_url = f\"sqlite://{config.db_path}\"\n            results['persistence_db'] = self.register_database('persistence', db_url)\n        except Exception as e:",
            "        try:\n            from src.services.metrics_exporter import MetricsExporter\n            # Register as API endpoint (metrics export service)\n            metrics_url = \"file://metrics_export.json\"  # Default export location\n            success = self.register_endpoint(\n                \"metrics_exporter\",\n                IntegrationType.API,\n                metrics_url\n            )\n            if success:\n                self.logger.info(' Metrics exporter registered in integration framework')\n            return success\n        except Exception as e:",
            "        try:\n            from src.services.metrics_exporter import MetricsExporter\n            exporter = MetricsExporter()\n            \n            # Test export\n            try:\n                metrics = exporter.export_unified_metrics()\n                metrics_available = bool(metrics.get(\"summary\"))\n            except:",
            "        try:\n            from src.shared_utils.api_client import APIClient\n            # Use default or config-based URL\n            base_url = \"https://api.example.com\"  # Update with actual URL from config\n            results['shared_api'] = self.register_api_client('shared', base_url)\n        except Exception as e:",
            "        try:\n            import os\n            db_url = os.getenv(\"DATABASE_URL\", \"sqlite:///data/dreamvault.db\")\n            results['dreamvault_db'] = self.register_database('dreamvault', db_url)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\architecture\\unified_architecture_core.py",
          "count": 8,
          "lines": [
            "        try:\n            # 1. Discover from EngineRegistry (SSOT)\n            from ..core.engines.registry import EngineRegistry\n            engine_registry = EngineRegistry()\n            for engine_name, engine_class in engine_registry._engines.items():\n                component = ArchitectureComponent(\n                    name=f\"engine.{engine_name}\",\n                    type=ArchitectureType.INTEGRATION,\n                    status=ComponentStatus.ACTIVE,\n                    version=\"1.0.0\",\n                    dependencies=[],\n                    metrics={},\n                    last_updated=datetime.now().isoformat()\n                )\n                discovered[component.name] = component\n                self.logger.info(f' Discovered: {component.name}')\n        except ImportError:",
            "        try:\n            # 2. Discover from Message Queue (SSOT)\n            from ..core.messaging_core import UnifiedMessagingCore\n            component = ArchitectureComponent(\n                name=\"messaging.queue\",\n                type=ArchitectureType.MESSAGING,\n                status=ComponentStatus.ACTIVE,\n                version=\"1.0.0\",\n                dependencies=[],\n                metrics={},\n                last_updated=datetime.now().isoformat()\n            )\n            discovered[component.name] = component\n            self.logger.info(f' Discovered: {component.name}')\n        except ImportError:",
            "        try:\n            # 3. Discover from Config SSOT\n            from ..core.config_ssot import get_unified_config\n            component = ArchitectureComponent(\n                name=\"config.ssot\",\n                type=ArchitectureType.INTEGRATION,\n                status=ComponentStatus.ACTIVE,\n                version=\"1.0.0\",\n                dependencies=[],\n                metrics={},\n                last_updated=datetime.now().isoformat()\n            )\n            discovered[component.name] = component\n            self.logger.info(f' Discovered: {component.name}')\n        except ImportError:",
            "        try:\n            from ...orchestrators.overnight.monitor import ProgressMonitor\n            monitor = ProgressMonitor()\n            if hasattr(monitor, 'get_health_status'):\n                orchestrator_health = monitor.get_health_status()\n                health['orchestrator'] = orchestrator_health\n            else:\n                health['orchestrator'] = {'status': 'monitoring_available', 'details': 'ProgressMonitor active'}\n        except ImportError:",
            "        try:\n            from ..core.messaging_core import UnifiedMessagingCore\n            from ..repositories.message_repository import MessageRepository\n            repo = MessageRepository()\n            queue_health = {\n                'status': 'active' if repo else 'inactive',\n                'pending_messages': len(repo.get_pending()) if repo else 0\n            }\n            health['message_queue'] = queue_health\n        except Exception as e:",
            "        try:\n            from ..core.performance.coordination_performance_monitor import CoordinationPerformanceMonitor\n            monitor = CoordinationPerformanceMonitor()\n            if hasattr(monitor, 'get_health_status'):\n                perf_health = monitor.get_health_status()\n                health['performance'] = perf_health\n            else:\n                health['performance'] = {'status': 'monitoring_available'}\n        except ImportError:",
            "        try:\n            if dependencies is None:\n                dependencies = []\n            component = ArchitectureComponent(name=name, type=\n                component_type, status=ComponentStatus.ACTIVE, version=\n                version, dependencies=dependencies, metrics={},\n                last_updated=datetime.now().isoformat())\n            self.components[name] = component\n            self.logger.info(\n                f' Registered component: {name} ({component_type.value})')\n            return True\n        except Exception as e:",
            "        try:\n            if name in self.components:\n                self.components[name].metrics.update(metrics)\n                self.components[name].last_updated = datetime.now().isoformat()\n                self.logger.info(f' Updated metrics for component: {name}')\n                return True\n            return False\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\automation\\ui_onboarding.py",
          "count": 3,
          "lines": [
            "        try:\n            # Get coordinates for agent\n            from ..core.coordinate_loader import get_coordinates_for_agent\n            coords = get_coordinates_for_agent(agent_id)\n            \n            if not coords:\n                logger.error(f\"No coordinates found for agent {agent_id}\")\n                return False\n            \n            x, y = coords\n            \n            # Move to coordinates and click\n            pyautogui.moveTo(x, y, duration=0.5)\n            pyautogui.click()\n            time.sleep(0.5)\n            \n            # Clear input field\n            pyautogui.hotkey('ctrl', 'a')\n            time.sleep(0.1)\n            pyautogui.press('delete')\n            time.sleep(0.1)\n            \n            # Paste message\n            if PYPERCLIP_AVAILABLE:\n                pyperclip.copy(message)\n                time.sleep(0.5)\n                pyautogui.hotkey('ctrl', 'v')\n                time.sleep(0.5)\n            else:\n                # Fallback to typing\n                pyautogui.write(message, interval=0.01)\n                time.sleep(0.5)\n            \n            # Send message\n            pyautogui.press('enter')\n            time.sleep(0.5)\n            \n            logger.info(f\" Successfully onboarded {agent_id}\")\n            return True\n            \n        except Exception as e:",
            "try:\n    import pyautogui\n    PYAUTOGUI_AVAILABLE = True\nexcept ImportError:",
            "try:\n    import pyperclip\n    PYPERCLIP_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\unified_data_processing_system.py",
          "count": 4,
          "lines": [
            "    try:\n        Path(dir_path).mkdir(parents=True, exist_ok=True)\n        return True\n    except Exception:",
            "    try:\n        path = Path(file_path)\n        if not path.exists():\n            return {}\n\n        with open(path, encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    except Exception:",
            "    try:\n        path = Path(file_path)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n\n        return True\n\n    except Exception:",
            "    try:\n        path = Path(file_path)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n\n        return True\n\n    except Exception:"
          ]
        },
        {
          "file": "src\\core\\stress_test_analysis_report.py",
          "count": 2,
          "lines": [
            "        try:\n            output_dir = output_dir or Path(\"stress_test_analysis_results\")\n            output_dir.mkdir(exist_ok=True)\n\n            # Generate all analysis components\n            report = {\n                \"report_metadata\": {\n                    \"generated_at\": datetime.now().isoformat(),\n                    \"analyzer_version\": \"1.0.0\",\n                    \"report_type\": \"full_analysis\",\n                },\n                \"executive_summary\": self._generate_executive_summary(),\n                \"latency_analysis\": self.analyzer.analyze_latency_patterns(),\n                \"bottleneck_analysis\": {\n                    \"bottlenecks\": self.analyzer.identify_bottlenecks(),\n                    \"severity_breakdown\": self._calculate_severity_breakdown(),\n                },\n                \"optimization_opportunities\": self.analyzer.generate_optimization_opportunities(),\n                \"performance_recommendations\": self.analyzer.generate_performance_recommendations(),\n                \"dashboard_visualization\": self.analyzer.generate_dashboard_visualization_data(),\n                \"key_findings\": self._generate_key_findings(),\n                \"action_items\": self._generate_action_items(),\n            }\n\n            # Save report to file\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            report_file = output_dir / f\"stress_test_analysis_report_{timestamp}.json\"\n            with open(report_file, \"w\") as f:\n                json.dump(report, f, indent=2)\n\n            # Generate markdown summary\n            markdown_file = output_dir / f\"stress_test_analysis_summary_{timestamp}.md\"\n            self._generate_markdown_summary(report, markdown_file)\n\n            self.logger.info(f\"Analysis report generated: {report_file}\")\n            self.logger.info(f\"Summary report generated: {markdown_file}\")\n\n            return report\n        except Exception as e:",
            "        try:\n            summary = report.get(\"executive_summary\", {})\n            bottlenecks = report.get(\"bottleneck_analysis\", {}).get(\"bottlenecks\", [])\n            findings = report.get(\"key_findings\", [])\n            actions = report.get(\"action_items\", [])\n\n            markdown_content = f\"\"\"# Stress Test Analysis Report\n\n**Generated**: {report.get('report_metadata', {}).get('generated_at', 'N/A')}\n\n---\n\n## Executive Summary\n\n**Overall Status**: {summary.get('test_performance', {}).get('overall_status', 'N/A').upper()}\n\n**Key Metrics:**\n- P99 Latency: {summary.get('test_performance', {}).get('p99_latency_ms', 0):.2f} ms\n- Throughput: {summary.get('test_performance', {}).get('throughput_msg_per_sec', 0):.2f} msg/sec\n- Failure Rate: {summary.get('test_performance', {}).get('failure_rate_percent', 0):.2f}%\n\n**Critical Issues**: {summary.get('critical_issues', 0)}\n\n**Top Priority**: {summary.get('top_priority', 'N/A')}\n\n---\n\n## Key Findings\n\n\"\"\"\n\n            for finding in findings:\n                markdown_content += f\"\"\"\n### {finding.get('category', 'Unknown').upper()}\n\n- **Finding**: {finding.get('finding', 'N/A')}\n- **Severity**: {finding.get('severity', 'N/A').upper()}\n- **Impact**: {finding.get('impact', 'N/A')}\n\n\"\"\"\n\n            markdown_content += \"\"\"\n---\n\n## Bottlenecks Identified\n\n\"\"\"\n\n            for bottleneck in bottlenecks[:5]:  # Top 5\n                markdown_content += f\"\"\"\n### {bottleneck.get('type', 'Unknown').replace('_', ' ').title()}\n\n- **Severity**: {bottleneck.get('severity', 'N/A').upper()}\n- **Description**: {bottleneck.get('description', 'N/A')}\n- **Metric**: {bottleneck.get('metric', 'N/A')}\n- **Value**: {bottleneck.get('value', 0)}\n\n\"\"\"\n\n            markdown_content += \"\"\"\n---\n\n## Recommended Action Items\n\n\"\"\"\n\n            for action in actions[:10]:  # Top 10\n                markdown_content += f\"\"\"\n### {action.get('action', 'N/A')} (Priority: {action.get('priority', 'N/A').upper()})\n\n- **Category**: {action.get('category', 'N/A')}\n- **Description**: {action.get('description', 'N/A')}\n- **Estimated Effort**: {action.get('estimated_effort', 'N/A')}\n- **Expected Impact**: {action.get('expected_impact', 'N/A')}\n\n\"\"\"\n\n            markdown_content += \"\"\"\n---\n\n*Report generated by Stress Test Metrics Analyzer*\n\"\"\"\n\n            with open(output_file, \"w\") as f:\n                f.write(markdown_content)\n\n            self.logger.info(f\"Markdown summary generated: {output_file}\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\stress_test_metrics.py",
          "count": 6,
          "lines": [
            "        try:\n            event = {\n                \"type\": event_type,\n                \"timestamp\": datetime.now().isoformat(),\n                \"details\": details,\n            }\n            self.chaos_events.append(event)\n            \n            if recovery_time_ms is not None:\n                self.crash_recovery_times.append(recovery_time_ms)\n            \n            # Update spike handling metrics\n            if event_type == \"spike\":\n                self.spike_handling_metrics[\"spikes_detected\"] += 1\n                if recovery_time_ms:\n                    self.spike_handling_metrics[\"spikes_handled\"] += 1\n                    \n        except Exception as e:",
            "        try:\n            self.failed_messages += 1\n            \n            if agent_id:\n                self.failures_by_agent[agent_id] += 1\n            \n            if message_type:\n                self.failures_by_type[message_type] += 1\n            \n            if reason:\n                self.failure_reasons[reason] += 1\n                \n        except Exception as e:",
            "        try:\n            self.latencies.append(latency_ms)\n            \n            if agent_id:\n                self.latencies_by_agent[agent_id].append(latency_ms)\n            \n            if message_type:\n                self.latencies_by_type[message_type].append(latency_ms)\n                \n        except Exception as e:",
            "        try:\n            self.queue_depths.append(depth)\n            self.queue_depth_timestamps.append(time.time())\n            \n            if depth > self.max_queue_depth:\n                self.max_queue_depth = depth\n                \n        except Exception as e:",
            "        try:\n            self.record_latency(latency_ms, agent_id, message_type)\n            \n            if delivery_mode == \"mock\":\n                self.mock_delivery_times.append(latency_ms)\n            else:\n                self.real_delivery_times.append(latency_ms)\n                \n        except Exception as e:",
            "        try:\n            self.total_messages += 1\n            self.message_timestamps.append(time.time())\n            \n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\stress_test_metrics_analyzer.py",
          "count": 6,
          "lines": [
            "        try:\n            bottlenecks = []\n\n            # Analyze overall metrics\n            overall = self.dashboard_data.get(\"overall_metrics\", {})\n            latency = overall.get(\"latency_percentiles\", {})\n\n            # High latency bottleneck\n            if latency.get(\"p99\", 0) > 500:  # 500ms threshold\n                bottlenecks.append({\n                    \"type\": \"high_latency\",\n                    \"severity\": \"high\" if latency.get(\"p99\", 0) > 1000 else \"medium\",\n                    \"metric\": \"p99_latency\",\n                    \"value\": latency.get(\"p99\", 0),\n                    \"threshold\": 500,\n                    \"description\": f\"P99 latency ({latency.get('p99', 0):.2f}ms) exceeds threshold\",\n                })\n\n            # Low throughput bottleneck\n            throughput = overall.get(\"throughput_msg_per_sec\", 0)\n            if throughput < 50:  # 50 msg/sec threshold\n                bottlenecks.append({\n                    \"type\": \"low_throughput\",\n                    \"severity\": \"high\" if throughput < 20 else \"medium\",\n                    \"metric\": \"throughput\",\n                    \"value\": throughput,\n                    \"threshold\": 50,\n                    \"description\": f\"Throughput ({throughput:.2f} msg/sec) below threshold\",\n                })\n\n            # High failure rate bottleneck\n            failure_rate = overall.get(\"failure_rate_percent\", 0)\n            if failure_rate > 1.0:  # 1% threshold\n                bottlenecks.append({\n                    \"type\": \"high_failure_rate\",\n                    \"severity\": \"high\" if failure_rate > 5.0 else \"medium\",\n                    \"metric\": \"failure_rate\",\n                    \"value\": failure_rate,\n                    \"threshold\": 1.0,\n                    \"description\": f\"Failure rate ({failure_rate:.2f}%) exceeds threshold\",\n                })\n\n            # Queue depth bottleneck\n            queue = overall.get(\"queue_depth\", {})\n            max_queue = queue.get(\"max\", 0)\n            if max_queue > 100:  # 100 items threshold\n                bottlenecks.append({\n                    \"type\": \"queue_depth\",\n                    \"severity\": \"high\" if max_queue > 500 else \"medium\",\n                    \"metric\": \"queue_depth_max\",\n                    \"value\": max_queue,\n                    \"threshold\": 100,\n                    \"description\": f\"Max queue depth ({max_queue}) exceeds threshold\",\n                })\n\n            # Per-agent bottlenecks\n            per_agent = self.dashboard_data.get(\"per_agent_metrics\", {})\n            for agent_id, agent_metrics in per_agent.items():\n                agent_latency = agent_metrics.get(\"latency_percentiles\", {})\n                if agent_latency.get(\"p99\", 0) > 500:\n                    bottlenecks.append({\n                        \"type\": \"agent_latency\",\n                        \"severity\": \"medium\",\n                        \"agent\": agent_id,\n                        \"metric\": \"p99_latency\",\n                        \"value\": agent_latency.get(\"p99\", 0),\n                        \"description\": f\"{agent_id}: High latency detected\",\n                    })\n\n            return bottlenecks\n        except Exception as e:",
            "        try:\n            latency_patterns = self.analyze_latency_patterns()\n            bottlenecks = self.identify_bottlenecks()\n            opportunities = self.generate_optimization_opportunities()\n\n            recommendations = {\n                \"summary\": {\n                    \"total_bottlenecks\": len(bottlenecks),\n                    \"high_severity\": sum(1 for b in bottlenecks if b[\"severity\"] == \"high\"),\n                    \"medium_severity\": sum(1 for b in bottlenecks if b[\"severity\"] == \"medium\"),\n                    \"optimization_opportunities\": len(opportunities),\n                },\n                \"critical_actions\": [\n                    opp for opp in opportunities if opp[\"priority\"] == \"high\"\n                ],\n                \"recommended_actions\": [\n                    opp for opp in opportunities if opp[\"priority\"] == \"medium\"\n                ],\n                \"latency_insights\": {\n                    \"current_state\": {\n                        \"p50\": latency_patterns.get(\"p50\", 0),\n                        \"p95\": latency_patterns.get(\"p95\", 0),\n                        \"p99\": latency_patterns.get(\"p99\", 0),\n                    },\n                    \"tail_latency_concern\": (\n                        latency_patterns.get(\"tail_latency_severity\", \"none\") != \"none\"\n                    ),\n                    \"recommendations\": self._generate_latency_recommendations(latency_patterns),\n                },\n                \"bottlenecks\": bottlenecks,\n                \"optimization_opportunities\": opportunities,\n            }\n\n            return recommendations\n        except Exception as e:",
            "        try:\n            opportunities = []\n            bottlenecks = self.identify_bottlenecks()\n\n            for bottleneck in bottlenecks:\n                if bottleneck[\"type\"] == \"high_latency\":\n                    opportunities.append({\n                        \"category\": \"latency_optimization\",\n                        \"priority\": \"high\" if bottleneck[\"severity\"] == \"high\" else \"medium\",\n                        \"recommendation\": \"Optimize message processing pipeline\",\n                        \"actions\": [\n                            \"Review message queue processing logic\",\n                            \"Consider batch processing for high-volume scenarios\",\n                            \"Evaluate database query optimization\",\n                            \"Check for unnecessary serialization/deserialization\",\n                        ],\n                        \"expected_impact\": \"20-40% latency reduction\",\n                    })\n\n                elif bottleneck[\"type\"] == \"low_throughput\":\n                    opportunities.append({\n                        \"category\": \"throughput_optimization\",\n                        \"priority\": \"high\" if bottleneck[\"severity\"] == \"high\" else \"medium\",\n                        \"recommendation\": \"Increase message processing throughput\",\n                        \"actions\": [\n                            \"Implement parallel processing\",\n                            \"Optimize I/O operations\",\n                            \"Consider connection pooling\",\n                            \"Review resource allocation\",\n                        ],\n                        \"expected_impact\": \"50-100% throughput improvement\",\n                    })\n\n                elif bottleneck[\"type\"] == \"high_failure_rate\":\n                    opportunities.append({\n                        \"category\": \"reliability_optimization\",\n                        \"priority\": \"high\",\n                        \"recommendation\": \"Reduce failure rate\",\n                        \"actions\": [\n                            \"Implement retry mechanisms with exponential backoff\",\n                            \"Add circuit breaker pattern\",\n                            \"Improve error handling and recovery\",\n                            \"Monitor and fix root causes\",\n                        ],\n                        \"expected_impact\": \"Failure rate reduction to <1%\",\n                    })\n\n                elif bottleneck[\"type\"] == \"queue_depth\":\n                    opportunities.append({\n                        \"category\": \"queue_optimization\",\n                        \"priority\": \"medium\",\n                        \"recommendation\": \"Optimize queue management\",\n                        \"actions\": [\n                            \"Increase consumer processing rate\",\n                            \"Implement queue prioritization\",\n                            \"Add queue depth alerts\",\n                            \"Consider horizontal scaling\",\n                        ],\n                        \"expected_impact\": \"Queue depth reduction by 50%\",\n                    })\n\n            return opportunities\n        except Exception as e:",
            "        try:\n            overall = self.dashboard_data.get(\"overall_metrics\", {})\n            latency = overall.get(\"latency_percentiles\", {})\n\n            patterns = {\n                \"p50\": latency.get(\"p50\", 0),\n                \"p95\": latency.get(\"p95\", 0),\n                \"p99\": latency.get(\"p99\", 0),\n                \"p95_to_p50_ratio\": (\n                    latency.get(\"p95\", 0) / latency.get(\"p50\", 1)\n                    if latency.get(\"p50\", 0) > 0\n                    else 0\n                ),\n                \"p99_to_p50_ratio\": (\n                    latency.get(\"p99\", 0) / latency.get(\"p50\", 1)\n                    if latency.get(\"p50\", 0) > 0\n                    else 0\n                ),\n                \"tail_latency_severity\": self._assess_tail_latency(latency),\n                \"latency_distribution\": self._analyze_latency_distribution(),\n            }\n\n            # Identify latency anomalies\n            patterns[\"anomalies\"] = self._detect_latency_anomalies(latency)\n            patterns[\"trends\"] = self._analyze_latency_trends()\n\n            return patterns\n        except Exception as e:",
            "        try:\n            visualization_data = {\n                \"charts\": {\n                    \"latency_distribution\": self._generate_latency_chart_data(),\n                    \"throughput_timeline\": self._generate_throughput_chart_data(),\n                    \"failure_rate_timeline\": self._generate_failure_rate_chart_data(),\n                    \"queue_depth_timeline\": self._generate_queue_depth_chart_data(),\n                    \"per_agent_comparison\": self._generate_agent_comparison_data(),\n                    \"message_type_analysis\": self._generate_message_type_analysis(),\n                },\n                \"summary_metrics\": self.dashboard_data.get(\"overall_metrics\", {}),\n                \"key_insights\": self._generate_key_insights(),\n            }\n\n            return visualization_data\n        except Exception as e:",
            "        try:\n            with open(file_path, \"r\") as f:\n                self.dashboard_data = json.load(f)\n            self.logger.info(f\"Loaded dashboard from {file_path}\")\n            return self.dashboard_data\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\task_completion_detector.py",
          "count": 1,
          "lines": [
            "            try:\n                callback = self.completion_callbacks[task_id]\n                callback(task_id, result, reason, self.active_tasks[task_id])\n            except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\activity_emitter.py",
          "count": 2,
          "lines": [
            "            try:\n                self.discord_sink.post_event(event)\n            except Exception:",
            "    try:\n        atype = ActivityType(event_type)\n    except Exception:"
          ]
        },
        {
          "file": "src\\core\\activity_source_checkers.py",
          "count": 7,
          "lines": [
            "                    try:\n                        commit_time = int(parts[1])\n                        commit_msg = parts[2] if len(parts) > 2 else \"\"\n\n                        # Collect files for this commit\n                        files = []\n                        i += 1\n                        while i < len(lines):\n                            file_line = lines[i].strip()\n                            if not file_line:\n                                i += 1\n                                break\n                            if \"|\" in file_line:\n                                # Next commit, go back one line\n                                i -= 1\n                                break\n                            files.append(file_line)\n                            i += 1\n\n                        commits.append({\n                            \"hash\": commit_hash,\n                            \"timestamp\": commit_time,\n                            \"message\": commit_msg,\n                            \"files\": files\n                        })\n                    except (ValueError, IndexError):",
            "            try:\n                mtime = coverage_file.stat().st_mtime\n                if mtime >= lookback_time.timestamp():\n                    signals.append(ActivitySignal(\n                        source=ActivitySource.TEST_EXECUTION,\n                        timestamp=mtime,\n                        confidence=ActivitySource.TEST_EXECUTION.base_confidence * 0.9,\n                        metadata={\"type\": \"coverage\"},\n                        agent_id=agent_id\n                    ))\n            except Exception:",
            "            try:\n                mtime = pytest_cache.stat().st_mtime\n                if mtime >= lookback_time.timestamp():\n                    signals.append(ActivitySignal(\n                        source=ActivitySource.TEST_EXECUTION,\n                        timestamp=mtime,\n                        confidence=ActivitySource.TEST_EXECUTION.base_confidence,\n                        metadata={\"type\": \"pytest_cache\"},\n                        agent_id=agent_id\n                    ))\n            except Exception:",
            "        try:\n            from src.core.config.timeout_constants import TimeoutConstants\n\n            # Agent-specific paths to check\n            agent_paths = [f\"agent_workspaces/{agent_id}/\"]\n\n            # Add captain reports for Agent-4\n            if agent_id == \"Agent-4\":\n                agent_paths.append(\"docs/captain_reports/\")\n\n            for path_pattern in agent_paths:\n                try:\n                    result = subprocess.run(\n                        [\"git\", \"log\", \"--since\", lookback_time.isoformat(),\n                         \"--format=%H|%ct|%s\", \"--name-only\", \"--\", path_pattern],\n                        capture_output=True,\n                        text=True,\n                        timeout=TimeoutConstants.HTTP_QUICK,\n                        cwd=self.workspace_root,\n                    )\n\n                    if result.returncode != 0 or not result.stdout.strip():\n                        continue\n\n                    # Parse git log output with file names\n                    commits = self._parse_git_log_with_files(result.stdout)\n\n                    for commit in commits:\n                        commit_time = commit['timestamp']\n                        commit_msg = commit['message']\n\n                        # Skip if before lookback time\n                        if commit_time < lookback_time.timestamp():\n                            continue\n\n                        # Filter out resume-related commits\n                        if any(noise in commit_msg.lower()\n                               for noise in self.noise_patterns):\n                            continue\n\n                        # Only add if we haven't already detected this commit\n                        commit_hash = commit['hash']\n                        if commit_hash[:8] in existing_hashes:\n                            continue\n\n                        signals.append(ActivitySignal(\n                            source=ActivitySource.GIT_COMMIT,\n                            timestamp=commit_time,\n                            confidence=ActivitySource.GIT_COMMIT.base_confidence,\n                            metadata={\n                                \"hash\": commit_hash[:8],\n                                \"message\": commit_msg[:100],\n                                # Limit file list\n                                \"files\": commit.get('files', [])[:5],\n                                \"detection_method\": \"file_path\"\n                            },\n                            agent_id=agent_id\n                        ))\n                except Exception as e:",
            "        try:\n            from src.core.config.timeout_constants import TimeoutConstants\n\n            result = subprocess.run(\n                [\"git\", \"log\", \"--since\", lookback_time.isoformat(),\n                 \"--format=%H|%ct|%s\", \"--grep\", agent_id, \"--all\"],\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_QUICK,\n                cwd=self.workspace_root,\n            )\n\n            if result.returncode == 0 and result.stdout.strip():\n                for line in result.stdout.strip().split(\"\\n\"):\n                    if not line:\n                        continue\n                    parts = line.split(\"|\", 2)\n                    if len(parts) >= 2:\n                        try:\n                            commit_time = int(parts[1])\n                            commit_msg = parts[2] if len(parts) > 2 else \"\"\n\n                            # Filter out resume-related commits\n                            if any(noise in commit_msg.lower() for noise in self.noise_patterns):\n                                continue\n\n                            signals.append(ActivitySignal(\n                                source=ActivitySource.GIT_COMMIT,\n                                timestamp=commit_time,\n                                confidence=ActivitySource.GIT_COMMIT.base_confidence,\n                                metadata={\n                                    \"hash\": parts[0][:8],\n                                    \"message\": commit_msg[:100]\n                                },\n                                agent_id=agent_id\n                            ))\n                        except (ValueError, IndexError):",
            "        try:\n            from src.services.unified_service_managers import UnifiedContractManager\n            manager = UnifiedContractManager()\n            agent_status = manager.get_agent_status(agent_id)\n\n            contracts = agent_status.get(\"contracts\", [])\n            active_contracts = agent_status.get(\"active_contracts\", 0)\n\n            if active_contracts > 0 and contracts:\n                latest_contract = max(\n                    contracts,\n                    key=lambda c: c.get(\"assigned_at\", 0) or c.get(\n                        \"updated_at\", 0) or 0\n                )\n\n                timestamp = latest_contract.get(\n                    \"assigned_at\") or latest_contract.get(\"updated_at\")\n                if timestamp:\n                    try:\n                        if isinstance(timestamp, str):\n                            ts = datetime.fromisoformat(\n                                timestamp.replace(\"Z\", \"+00:00\")\n                            ).replace(tzinfo=None)\n                        else:\n                            ts = datetime.fromtimestamp(timestamp)\n\n                        if ts >= lookback_time:\n                            signals.append(ActivitySignal(\n                                source=ActivitySource.CONTRACT_CLAIMED,\n                                timestamp=ts.timestamp(),\n                                confidence=ActivitySource.CONTRACT_CLAIMED.base_confidence,\n                                metadata={\n                                    \"contract_id\": latest_contract.get(\"contract_id\", \"\"),\n                                    \"title\": latest_contract.get(\"title\", \"\")[:50],\n                                    \"status\": latest_contract.get(\"status\", \"\")\n                                },\n                                agent_id=agent_id\n                            ))\n                    except Exception:",
            "        try:\n            with open(self.activity_event_file, 'r', encoding='utf-8') as f:\n                lines = f.readlines()\n                # Check last 200 lines for performance\n                recent_lines = lines[-200:] if len(lines) > 200 else lines\n\n                for line in recent_lines:\n                    if not line.strip():\n                        continue\n\n                    try:\n                        event = json.loads(line.strip())\n                        if event.get(\"agent\", \"\").lower() != agent_id.lower():\n                            continue\n\n                        ts_raw = event.get(\"ts\")\n                        if not ts_raw:\n                            continue\n\n                        try:\n                            ts = datetime.fromisoformat(\n                                str(ts_raw).replace(\"Z\", \"+00:00\")\n                            ).replace(tzinfo=None)\n                        except Exception:"
          ]
        },
        {
          "file": "src\\core\\activity_source_checkers_tier2.py",
          "count": 4,
          "lines": [
            "        try:\n            # Check for processed/read indicators\n            processed_files = list(inbox_dir.glob(\"*_processed.md\"))\n            read_files = list(inbox_dir.glob(\"*_read.md\"))\n\n            all_processed = processed_files + read_files\n            if all_processed:\n                latest = max(all_processed, key=lambda p: p.stat().st_mtime)\n                mtime = latest.stat().st_mtime\n\n                if mtime >= lookback_time.timestamp():\n                    signals.append(ActivitySignal(\n                        source=ActivitySource.INBOX_PROCESSING,\n                        timestamp=mtime,\n                        confidence=ActivitySource.INBOX_PROCESSING.base_confidence,\n                        metadata={\"file\": latest.name},\n                        agent_id=agent_id\n                    ))\n        except Exception as e:",
            "        try:\n            exclude_dirs = {\"__pycache__\", \".git\",\n                            \"node_modules\", \".venv\", \"inbox\"}\n            exclude_files = {\"status.json\"}  # Checked separately\n\n            latest_mtime = 0\n            file_count = 0\n\n            for file_path in agent_dir.rglob(\"*\"):\n                if file_path.is_file():\n                    if any(exclude in file_path.parts for exclude in exclude_dirs):\n                        continue\n                    if file_path.name in exclude_files:\n                        continue\n\n                    try:\n                        mtime = file_path.stat().st_mtime\n                        if mtime >= lookback_time.timestamp():\n                            latest_mtime = max(latest_mtime, mtime)\n                            file_count += 1\n                    except (OSError, PermissionError):",
            "        try:\n            mtime = status_file.stat().st_mtime\n            if mtime < lookback_time.timestamp():\n                return signals\n\n            # Read status to validate meaningful content\n            with open(status_file, 'r', encoding='utf-8') as f:\n                status = json.load(f)\n\n            # Check if status has meaningful content (not just noise)\n            last_updated_str = status.get(\"last_updated\", \"\")\n            mission = status.get(\"current_mission\", \"\")\n            current_tasks = status.get(\"current_tasks\", [])\n\n            # Require meaningful content to count as activity\n            if not (mission.strip() or current_tasks):\n                return signals\n\n            # Parse last_updated timestamp\n            try:\n                if 'T' in last_updated_str:\n                    last_updated = datetime.fromisoformat(\n                        last_updated_str.replace(\"Z\", \"+00:00\")\n                    ).replace(tzinfo=None)\n                else:\n                    last_updated = datetime.strptime(\n                        last_updated_str, \"%Y-%m-%d %H:%M:%S\"\n                    )\n\n                if last_updated >= lookback_time:\n                    signals.append(ActivitySignal(\n                        source=ActivitySource.STATUS_UPDATE,\n                        timestamp=last_updated.timestamp(),\n                        confidence=ActivitySource.STATUS_UPDATE.base_confidence,\n                        metadata={\n                            \"status\": status.get(\"status\", \"\"),\n                            \"mission\": mission[:50],\n                            \"task_count\": len(current_tasks)\n                        },\n                        agent_id=agent_id\n                    ))\n            except Exception:",
            "        try:\n            patterns = [\n                f\"*{agent_id.lower()}*\",\n                f\"*{agent_id.replace('-', '_').lower()}*\",\n            ]\n\n            for pattern in patterns:\n                for devlog_file in devlogs_dir.glob(f\"{pattern}.md\"):\n                    try:\n                        mtime = devlog_file.stat().st_mtime\n                        if mtime >= lookback_time.timestamp():\n                            signals.append(ActivitySignal(\n                                source=ActivitySource.DEVLOG_CREATED,\n                                timestamp=mtime,\n                                confidence=ActivitySource.DEVLOG_CREATED.base_confidence,\n                                metadata={\"file\": devlog_file.name},\n                                agent_id=agent_id\n                            ))\n                            break  # Only need one recent devlog\n                    except (OSError, PermissionError):"
          ]
        },
        {
          "file": "src\\core\\agent_activity_tracker.py",
          "count": 3,
          "lines": [
            "        try:\n            last_active = datetime.fromisoformat(last_active_str)\n            timeout = timedelta(minutes=timeout_minutes)\n            return datetime.now() - last_active < timeout\n        except (ValueError, AttributeError):",
            "        try:\n            with open(self.activity_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, indent=2, ensure_ascii=False)\n            return True\n        except OSError:",
            "        try:\n            with open(self.activity_file, encoding=\"utf-8\") as f:\n                return json.load(f)\n        except (OSError, json.JSONDecodeError):"
          ]
        },
        {
          "file": "src\\core\\agent_context_manager.py",
          "count": 3,
          "lines": [
            "        try:\n            if agent_id in self._contexts:\n                del self._contexts[agent_id]\n                logger.info(f\"Context removed for agent {agent_id}\")\n                return True\n            else:\n                logger.warning(f\"Agent {agent_id} not found, cannot remove context\")\n                return False\n        except Exception as e:",
            "        try:\n            if agent_id not in self._contexts:\n                logger.warning(f\"Agent {agent_id} not found, cannot update context\")\n                return False\n\n            self._contexts[agent_id].update(updates)\n            self._contexts[agent_id][\"updated_at\"] = datetime.now().isoformat()\n            logger.info(f\"Context updated for agent {agent_id}\")\n            return True\n        except Exception as e:",
            "        try:\n            self._contexts[agent_id] = {\n                **context,\n                \"updated_at\": datetime.now().isoformat(),\n                \"agent_id\": agent_id\n            }\n            logger.info(f\"Context set for agent {agent_id}\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\agent_documentation_service.py",
          "count": 5,
          "lines": [
            "                try:\n                    # Try to use vector_db.search if it has that method\n                    if hasattr(self.vector_db, 'search'):\n                        from src.web.vector_database.models import SearchRequest\n                        search_request = SearchRequest(\n                            query=query,\n                            collection=\"agent_cellphone_v2\",\n                            limit=n_results,\n                        )\n                        results = self.vector_db.search(search_request)\n                        return [\n                            {\n                                \"title\": result.title or f\"Document {result.id}\",\n                                \"content\": result.content,\n                                \"relevance\": result.relevance or result.score or 0.0,\n                                \"source\": result.collection or \"unknown\",\n                                \"id\": result.id,\n                            }\n                            for result in results\n                        ]\n                except Exception as e:",
            "                try:\n                    if hasattr(self.vector_db, 'get_documents'):\n                        from src.web.vector_database.models import PaginationRequest\n                        pagination_request = PaginationRequest(\n                            collection=\"agent_cellphone_v2\",\n                            page=1,\n                            page_size=100,\n                        )\n                        documents = self.vector_db.get_documents(pagination_request)\n                        if isinstance(documents, dict) and \"documents\" in documents:\n                            for doc in documents.get(\"documents\", []):\n                                if doc.id == doc_id or str(doc.id) == str(doc_id):\n                                    return {\n                                        \"id\": doc.id,\n                                        \"title\": doc.title or f\"Document {doc.id}\",\n                                        \"content\": doc.content,\n                                        \"last_updated\": datetime.now().isoformat(),\n                                    }\n                except Exception as e:",
            "        try:\n            if self.agent_id:\n                logger.info(f\"Initialized documentation access for agent {self.agent_id}\")\n            else:\n                logger.info(\"Initialized documentation service\")\n        except Exception as e:",
            "        try:\n            logger.info(f\"Getting document {doc_id} for agent {self.agent_id}\")\n            \n            # Use vector database service if available\n            try:\n                from src.services.vector_database_service_unified import get_vector_database_service\n                from src.web.vector_database.models import PaginationRequest\n                \n                vector_db = get_vector_database_service()\n                if vector_db:\n                    # Search for document by ID using pagination request\n                    # Note: Vector DB services typically don't have direct get-by-id,\n                    # so we search with a filter or use pagination\n                    pagination_request = PaginationRequest(\n                        collection=\"agent_cellphone_v2\",\n                        page=1,\n                        page_size=100,  # Get enough to find the doc\n                    )\n                    \n                    documents = vector_db.get_documents(pagination_request)\n                    \n                    # Find document by ID in results\n                    if isinstance(documents, dict) and \"documents\" in documents:\n                        for doc in documents.get(\"documents\", []):\n                            if doc.id == doc_id or str(doc.id) == str(doc_id):\n                                return {\n                                    \"id\": doc.id,\n                                    \"title\": doc.title or f\"Document {doc.id}\",\n                                    \"content\": doc.content,\n                                    \"collection\": doc.collection,\n                                    \"tags\": doc.tags or [],\n                                    \"metadata\": doc.metadata or {},\n                                    \"created_at\": doc.created_at.isoformat() if hasattr(doc.created_at, 'isoformat') else str(doc.created_at),\n                                    \"updated_at\": doc.updated_at.isoformat() if hasattr(doc.updated_at, 'isoformat') else str(doc.updated_at),\n                                    \"last_updated\": doc.updated_at.isoformat() if hasattr(doc.updated_at, 'isoformat') else str(doc.updated_at),\n                                }\n            except ImportError:",
            "        try:\n            logger.info(f\"Searching documentation for agent {target_agent_id}: {query}\")\n            \n            # Use vector database service if available\n            try:\n                from src.services.vector_database_service_unified import get_vector_database_service\n                from src.web.vector_database.models import SearchRequest\n                \n                vector_db = get_vector_database_service()\n                if vector_db:\n                    # Build search request with agent filter if provided\n                    filters = {}\n                    if target_agent_id:\n                        filters[\"agent_id\"] = target_agent_id\n                    \n                    search_request = SearchRequest(\n                        query=query,\n                        collection=\"agent_cellphone_v2\",  # Default collection\n                        limit=n_results,\n                        filters=filters if filters else None,\n                    )\n                    \n                    # Perform search\n                    results = vector_db.search(search_request)\n                    \n                    # Convert to expected format\n                    return [\n                        {\n                            \"title\": result.title or f\"Document {result.id}\",\n                            \"content\": result.content,\n                            \"relevance\": result.relevance or result.score or 0.0,\n                            \"source\": result.metadata.get(\"source\", result.collection or \"unknown\") if result.metadata else result.collection or \"unknown\",\n                            \"id\": result.id,\n                            \"collection\": result.collection,\n                            \"tags\": result.tags or [],\n                            \"created_at\": result.created_at.isoformat() if hasattr(result.created_at, 'isoformat') else str(result.created_at),\n                        }\n                        for result in results\n                    ]\n            except ImportError:"
          ]
        },
        {
          "file": "src\\core\\agent_lifecycle.py",
          "count": 2,
          "lines": [
            "        try:\n            cycle = self.status.get('cycle_count', 0)\n            commit_msg = message or f\"status: {self.agent_id} cycle {cycle} complete\"\n\n            subprocess.run(['git', 'add', str(self.status_file)], check=True)\n            subprocess.run(['git', 'commit', '-m', commit_msg], check=True)\n            return True\n        except subprocess.CalledProcessError:",
            "        try:\n            from src.discord_commander.status_change_monitor import StatusChangeMonitor\n            # Try to get bot instance and notify (if bot is running)\n            # This is optional - file watcher will catch it anyway\n            import sys\n            if 'bot' in sys.modules or hasattr(sys.modules.get('src.discord_commander.unified_discord_bot', None), 'status_monitor'):\n                # Bot might be running, but we can't reliably access it here\n                # File watcher will detect the change within 15 seconds\n                pass\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\agent_mode_manager.py",
          "count": 2,
          "lines": [
            "        try:\n            if not self.config_path.exists():\n                logger.error(f\" Agent mode config not found: {self.config_path}\")\n                return self._get_default_config()\n            \n            with open(self.config_path, 'r', encoding='utf-8') as f:\n                config = json.load(f)\n            \n            logger.info(f\" Loaded agent mode config: {config.get('current_mode', 'unknown')}\")\n            return config\n        except Exception as e:",
            "        try:\n            with open(self.config_path, 'w', encoding='utf-8') as f:\n                json.dump(self.config, f, indent=2)\n            logger.info(f\" Saved agent mode config: {self.config_path}\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\agent_self_healing_system.py",
          "count": 5,
          "lines": [
            "            try:\n                await self._check_and_heal_all_agents()\n                await asyncio.sleep(self.config.check_interval_seconds)\n            except asyncio.CancelledError:",
            "            try:\n                file_mtime = status_file.stat().st_mtime\n                age_seconds = current_time - file_mtime\n\n                if age_seconds > self.config.stall_threshold_seconds:\n                    stalled.append((agent_id, age_seconds))\n            except Exception as e:",
            "            try:\n                stale_agents = self.activity_detector.get_stale_agents(\n                    max_age_seconds=self.config.stall_threshold_seconds\n                )\n                return stale_agents\n            except Exception as e:",
            "        try:\n            from src.orchestrators.overnight.enhanced_agent_activity_detector import (\n                EnhancedAgentActivityDetector\n            )\n            self.activity_detector = EnhancedAgentActivityDetector()\n        except ImportError:",
            "        try:\n            import pyautogui as pg\n            pyautogui = pg\n        except ImportError:"
          ]
        },
        {
          "file": "src\\core\\auto_gas_pipeline_system.py",
          "count": 12,
          "lines": [
            "                        try:\n                            start, end = map(int, value.split(\"-\"))\n                            return end - start + 1\n                        except ValueError:",
            "            try:\n                agent_num = int(agent_id.split(\"-\")[1])\n                # Simple mapping: Agent-1 = 10 repos, Agent-2 = 15 repos, etc.\n                return max(10, agent_num * 5 + 5)\n            except (ValueError, IndexError):",
            "            try:\n                while True:\n                    time.sleep(1)\n            except KeyboardInterrupt:",
            "        try:\n            # Create gas message\n            gas_message = self._create_gas_message(agent_id, progress, gas_type)\n\n            # Log delivery\n            delivery_record = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"agent_id\": agent_id,\n                \"progress\": progress,\n                \"gas_type\": gas_type,\n                \"message\": gas_message,\n                \"jet_fuel\": self.jet_fuel_enabled\n            }\n            self.gas_delivery_log.append(delivery_record)\n\n            # Send via messaging system\n            success = self._send_gas_message(agent_id, gas_message)\n\n            # Log to Swarm Brain\n            self._log_to_swarm_brain(delivery_record, success)\n\n            status = \"\" if success else \"\"\n            logger.info(f\"{status} Auto-Gas delivered to {agent_id}: {gas_type} at {progress:.1f}%\")\n\n        except Exception as e:",
            "        try:\n            # Get completed and total tasks from agent data\n            completed_tasks = agent_data.get(\"completed_tasks\", [])\n            current_tasks = agent_data.get(\"current_tasks\", [])\n\n            # Parse repo completion patterns\n            completed_repos = self._count_repo_completions(completed_tasks)\n            total_assigned_repos = self._get_total_assigned_repos(agent_data)\n\n            if total_assigned_repos == 0:\n                return None\n\n            progress = (completed_repos / total_assigned_repos) * 100\n            return min(progress, 100.0)  # Cap at 100%\n\n        except Exception as e:",
            "        try:\n            # Get current swarm state\n            swarm_state = self.status_aggregator.aggregate_swarm_state()\n\n            if not swarm_state or \"agents\" not in swarm_state:\n                return\n\n            agents = swarm_state[\"agents\"]\n\n            for agent_id, agent_data in agents.items():\n                if agent_data.get(\"status\") != \"active\":\n                    continue\n\n                # Calculate progress for this agent\n                progress = self._calculate_agent_progress(agent_data)\n\n                if progress is not None:\n                    # Check if gas should be delivered\n                    gas_needed = self._check_gas_requirements(agent_id, progress)\n\n                    if gas_needed:\n                        self._deliver_gas(agent_id, progress, gas_needed)\n\n        except Exception as e:",
            "        try:\n            # Get recent swarm brain entries for this agent\n            context_items = []\n\n            swarm_brain_path = self.workspace_root / \"swarm_brain\"\n            if swarm_brain_path.exists():\n                # Look for recent learnings\n                learnings_path = swarm_brain_path / \"learnings\"\n                if learnings_path.exists():\n                    learning_files = list(learnings_path.glob(\"*.md\"))[:3]  # Recent 3 files\n                    for file_path in learning_files:\n                        try:\n                            content = file_path.read_text(encoding='utf-8')[:200]  # First 200 chars\n                            context_items.append(f\"- {file_path.stem}: {content}...\")\n                        except:",
            "        try:\n            # Use the messaging CLI to send gas\n            import subprocess\n            import sys\n\n            cmd = [\n                sys.executable, \"-m\", \"src.services.messaging_cli\",\n                \"--send-message\",\n                f\"--recipient={agent_id}\",\n                f\"--content={message}\",\n                \"--priority=urgent\",\n                \"--tags=auto_gas_pipeline,system\"\n            ]\n\n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                cwd=self.workspace_root,\n                timeout=30\n            )\n\n            return result.returncode == 0\n\n        except Exception as e:",
            "        try:\n            from swarm_brain import SwarmBrain\n\n            brain = SwarmBrain()\n            brain.share_learning(\n                agent_id=\"AutoGasPipeline\",\n                title=f\"Auto-Gas: {delivery_record['agent_id']}  {delivery_record['gas_type']}\",\n                content=f\"\"\"Progress: {delivery_record['progress']:.1f}%\nGas Type: {delivery_record['gas_type']}\nJet Fuel: {delivery_record['jet_fuel']}\nSuccess: {success}\nTimestamp: {delivery_record['timestamp']}\n\nMessage: {delivery_record['message'][:200]}...\"\"\",\n                tags=[\"auto_gas\", \"pipeline\", \"automation\", \"system\"]\n            )\n\n        except Exception as e:",
            "        try:\n            logger.info(f\" Force gas delivery requested: {agent_id} ({gas_type})\")\n\n            # Create emergency gas message\n            emergency_message = f\"\"\" EMERGENCY GAS DELIVERY!\n\nFORCED {gas_type.upper()} GAS - Execute immediately!\nThis is an emergency fuel delivery outside normal pipeline timing.\n\nContinue mission execution!\"\"\"\n\n            success = self._send_gas_message(agent_id, emergency_message)\n\n            if success:\n                # Log emergency delivery\n                delivery_record = {\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"agent_id\": agent_id,\n                    \"progress\": -1,  # Emergency delivery\n                    \"gas_type\": f\"emergency_{gas_type}\",\n                    \"message\": emergency_message,\n                    \"jet_fuel\": False\n                }\n                self.gas_delivery_log.append(delivery_record)\n\n                logger.info(f\" Emergency gas delivered to {agent_id}\")\n            else:\n                logger.error(f\" Emergency gas delivery failed for {agent_id}\")\n\n            return success\n\n        except Exception as e:",
            "        try:\n            self._monitor_agents()\n        except Exception as e:",
            "        try:\n            swarm_state = self.status_aggregator.aggregate_swarm_state()\n\n            status = {\n                \"running\": self.running,\n                \"monitoring_interval\": self.monitoring_interval,\n                \"jet_fuel_enabled\": getattr(self, 'jet_fuel_enabled', False),\n                \"agents_monitored\": len(swarm_state.get(\"agents\", {})),\n                \"gas_deliveries_today\": len([\n                    d for d in self.gas_delivery_log\n                    if d[\"timestamp\"].startswith(datetime.now().strftime(\"%Y-%m-%d\"))\n                ]),\n                \"total_gas_deliveries\": len(self.gas_delivery_log),\n                \"last_delivery\": self.gas_delivery_log[-1] if self.gas_delivery_log else None\n            }\n\n            return status\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\command_execution_wrapper.py",
          "count": 1,
          "lines": [
            "    try:\n        # Execute command\n        process = subprocess.Popen(\n            command,\n            shell=shell,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            bufsize=1,\n        )\n\n        stdout_lines = []\n        stderr_lines = []\n\n        # Read output line by line\n        while True:\n            # Check stdout\n            if process.stdout:\n                line = process.stdout.readline()\n                if line:\n                    stdout_lines.append(line)\n                    output_so_far = \"\".join(stdout_lines)\n\n                    # Check for completion if enabled\n                    if check_completion and task_id:\n                        is_complete, reason = detector.detect_output_completion(output_so_far)\n                        if is_complete:\n                            logger.debug(f\"Command completion detected: {reason}\")\n                            # Continue reading but mark as complete\n\n            # Check stderr\n            if process.stderr:\n                line = process.stderr.readline()\n                if line:\n                    stderr_lines.append(line)\n\n            # Check if process finished\n            if process.poll() is not None:\n                break\n\n        # Wait for process to complete\n        exit_code = process.wait(timeout=timeout)\n\n        # Get final output\n        stdout = \"\".join(stdout_lines)\n        stderr = \"\".join(stderr_lines)\n        full_output = stdout + stderr\n\n        # Detect completion\n        is_complete = False\n        completion_reason = None\n\n        if check_completion:\n            # Check exit code first\n            if exit_code == 0:\n                is_complete, completion_reason = detect_command_completion(full_output)\n                if not is_complete:\n                    # Exit code 0 but no completion pattern - assume success\n                    is_complete = True\n                    completion_reason = \"Exit code 0\"\n            else:\n                # Non-zero exit code - check for error patterns\n                is_complete, completion_reason = detect_command_completion(full_output)\n                if not is_complete:\n                    # Non-zero exit but no pattern - assume failed\n                    is_complete = True\n                    completion_reason = f\"Exit code {exit_code}\"\n\n        # Update task if registered\n        if task_id:\n            detector.update_task_output(task_id, full_output, exit_code)\n\n        result = CommandExecutionResult(\n            command=command,\n            exit_code=exit_code,\n            stdout=stdout,\n            stderr=stderr,\n            is_complete=is_complete,\n            completion_reason=completion_reason,\n        )\n\n        logger.debug(f\"Command executed: {result}\")\n        return result\n\n    except subprocess.TimeoutExpired:"
          ]
        },
        {
          "file": "src\\core\\coordinator_models.py",
          "count": 1,
          "lines": [
            "        try:\n            self.__post_init__()\n            return True\n        except ValueError:"
          ]
        },
        {
          "file": "src\\core\\coordinator_registry.py",
          "count": 4,
          "lines": [
            "        try:\n            if name not in self._coordinators:\n                self.logger.warning(f\"Coordinator '{name}' not found\")\n                return False\n\n            coordinator = self._coordinators[name]\n            if hasattr(coordinator, 'shutdown'):\n                coordinator.shutdown()\n\n            del self._coordinators[name]\n            self.logger.info(f\"Unregistered coordinator: {name}\")\n            return True\n\n        except Exception as e:",
            "        try:\n            if not hasattr(coordinator, 'name'):\n                self.logger.error(\"Coordinator must have a 'name' attribute\")\n                return False\n\n            coordinator_name = coordinator.name\n            if coordinator_name in self._coordinators:\n                self.logger.warning(f\"Coordinator '{coordinator_name}' already registered\")\n                return False\n\n            self._coordinators[coordinator_name] = coordinator\n            self.logger.info(f\"Registered coordinator: {coordinator_name}\")\n            return True\n\n        except Exception as e:",
            "        try:\n            self.logger.info(\"Shutting down all coordinators\")\n            for name, coordinator in list(self._coordinators.items()):\n                try:\n                    if hasattr(coordinator, 'shutdown'):\n                        coordinator.shutdown()\n                    self.logger.info(f\"Shutdown coordinator: {name}\")\n                except Exception as e:",
            "        try:\n            statuses = {}\n            for name, coordinator in self._coordinators.items():\n                try:\n                    if hasattr(coordinator, 'get_status'):\n                        statuses[name] = coordinator.get_status()\n                    else:\n                        statuses[name] = {\"status\": \"unknown\", \"error\": \"No get_status method\"}\n                except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\debate_to_gas_integration.py",
          "count": 13,
          "lines": [
            "                try:\n                    umsg = UnifiedMessage(\n                        content=task,\n                        sender=\"SYSTEM\",\n                        recipient=agent_id,\n                        message_type=UnifiedMessageType.SYSTEM_TO_AGENT if UnifiedMessageType else None,\n                        priority=UnifiedMessagePriority.URGENT if UnifiedMessagePriority else None,\n                        tags=[UnifiedMessageTag.SYSTEM] if UnifiedMessageTag else [],\n                        category=MessageCategory.S2A,\n                    )\n                    rendered = render_message(\n                        umsg,\n                        template_key=\"DEBATE_CYCLE\",\n                        topic=topic,\n                        role=\"Assignee\",\n                        context=f\"Debate decision: {decision}\",\n                        rules=rules,\n                        deliverable=deliverable,\n                        fallback=fallback,\n                    )\n                    try:\n                        Coordinator.send_to_agent(\n                            agent=agent_id,\n                            message=rendered,\n                            priority=UnifiedMessagePriority.URGENT if UnifiedMessagePriority else None,\n                            use_pyautogui=True,\n                            sender=\"SYSTEM\",\n                            message_category=MessageCategory.S2A,\n                        )\n                    except TypeError:",
            "                try:\n                    umsg = UnifiedMessage(\n                        content=task,\n                        sender=\"SYSTEM\",\n                        recipient=agent_id,\n                        message_type=UnifiedMessageType.SYSTEM_TO_AGENT,\n                        priority=UnifiedMessagePriority.URGENT,\n                        tags=[UnifiedMessageTag.SYSTEM],\n                        category=MessageCategory.S2A,\n                    )\n                    rendered = render_message(\n                        umsg,\n                        template_key=\"DEBATE_CYCLE\",\n                        topic=topic,\n                        role=\"Assignee\",\n                        context=f\"Debate decision: {decision}\",\n                        rules=rules,\n                        deliverable=deliverable,\n                        fallback=fallback,\n                    )\n                    MessageCoordinator.send_to_agent(\n                        agent=agent_id,\n                        message=rendered,\n                        priority=UnifiedMessagePriority.URGENT,\n                        use_pyautogui=True,\n                        sender=\"SYSTEM\",\n                        message_category=MessageCategory.S2A,\n                    )\n                    logger.info(f\" GAS delivered to {agent_id}\")\n                    continue\n                except Exception as e:",
            "            try:\n                inbox_dir = self.project_root / \"agent_workspaces\" / agent_id / \"inbox\"\n                inbox_dir.mkdir(parents=True, exist_ok=True)\n                msg_file = inbox_dir / \\\n                    f\"DEBATE_DECISION_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n                with open(msg_file, 'w', encoding='utf-8') as f:\n                    if rendered:\n                        f.write(rendered)\n                    else:\n                        f.write(task)\n                logger.info(f\" Inbox message created for {agent_id}\")\n            except Exception as e:",
            "            try:\n                inbox_dir = self.project_root / \"agent_workspaces\" / agent_id / \"inbox\"\n                inbox_dir.mkdir(parents=True, exist_ok=True)\n                msg_file = inbox_dir / \\\n                    f\"S2A_DEBATE_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n                with open(msg_file, \"w\", encoding=\"utf-8\") as f:\n                    f.write(rendered or task)\n                logger.info(f\" Inbox message created for {agent_id}\")\n            except Exception as e:",
            "        try:\n            # Step 1: Store in Swarm Brain\n            self._store_in_swarm_brain(topic, decision, execution_plan)\n\n            # Step 2: Generate activation messages\n            messages = self._generate_activation_messages(\n                topic, decision, execution_plan, agent_assignments\n            )\n\n            # Step 3: Deliver via gasline\n            self._deliver_via_gasline(messages)\n\n            # Step 4: Create execution tracking\n            self._create_execution_tracker(topic, agent_assignments)\n\n            logger.info(\n                f\" Debate decision '{decision}' activated via gasline\")\n            return True\n\n        except Exception as e:",
            "        try:\n            from src.core.messaging_models_core import (  # type: ignore\n                MessageCategory,\n                UnifiedMessagePriority,\n                UnifiedMessageType,\n                UnifiedMessageTag,\n                UnifiedMessage,\n            )\n            out.update(\n                {\n                    \"MessageCategory\": MessageCategory,\n                    \"UnifiedMessagePriority\": UnifiedMessagePriority,\n                    \"UnifiedMessageType\": UnifiedMessageType,\n                    \"UnifiedMessageTag\": UnifiedMessageTag,\n                    \"UnifiedMessage\": UnifiedMessage,\n                }\n            )\n        except Exception:",
            "        try:\n            from src.core.messaging_templates import render_message  # type: ignore\n            out[\"render_message\"] = render_message\n        except Exception:",
            "        try:\n            from src.services.messaging.message_coordinator import MessageCoordinator  # type: ignore\n            out[\"Coordinator\"] = MessageCoordinator\n        except Exception:",
            "        try:\n            from src.services.messaging_infrastructure import MessageCoordinator\n            from src.core.messaging_models_core import (\n                MessageCategory,\n                UnifiedMessagePriority,\n                UnifiedMessageType,\n                UnifiedMessageTag,\n            )\n            from src.core.messaging_templates import render_message\n            from src.core.messaging_models_core import UnifiedMessage\n        except ImportError:",
            "        try:\n            self._store_in_swarm_brain(\n                topic, decision, execution_plan, agent_assignments)\n            payloads = self._generate_activation_payloads(\n                topic, decision, execution_plan, agent_assignments)\n            self._deliver_via_gasline(payloads)\n            self._create_execution_tracker(\n                topic, decision, execution_plan, agent_assignments)\n            logger.info(\" Debate decision activated via gasline\")\n            return True\n        except Exception as e:",
            "    try:\n        from src.services.messaging.messaging_models import (  # type: ignore\n            MessageCategory,\n            UnifiedMessagePriority,\n            UnifiedMessageType,\n            UnifiedMessageTag,\n            UnifiedMessage,\n        )\n        out.update(\n            {\n                \"MessageCategory\": MessageCategory,\n                \"UnifiedMessagePriority\": UnifiedMessagePriority,\n                \"UnifiedMessageType\": UnifiedMessageType,\n                \"UnifiedMessageTag\": UnifiedMessageTag,\n                \"UnifiedMessage\": UnifiedMessage,\n            }\n        )\n    except Exception:",
            "    try:\n        from src.services.messaging.messaging_template_dispatcher import render_message  # type: ignore\n        out[\"render_message\"] = render_message\n    except Exception:",
            "    try:\n        from src.services.messaging_infrastructure import MessageCoordinator  # type: ignore\n        out[\"Coordinator\"] = MessageCoordinator\n    except Exception:"
          ]
        },
        {
          "file": "src\\core\\deferred_push_queue.py",
          "count": 3,
          "lines": [
            "            try:\n                with open(self.queue_file, 'r') as f:\n                    data = json.load(f)\n                    # Filter out completed entries older than 24 hours\n                    pending = [\n                        entry for entry in data.get(\"pending_pushes\", [])\n                        if entry.get(\"status\") != PushStatus.COMPLETED.value or\n                        self._is_recent(entry.get(\"timestamp\"))\n                    ]\n                    return pending\n            except Exception as e:",
            "        try:\n            data = {\n                \"pending_pushes\": self.pending_pushes,\n                \"last_updated\": datetime.now().isoformat()\n            }\n            with open(self.queue_file, 'w') as f:\n                json.dump(data, f, indent=2)\n        except Exception as e:",
            "        try:\n            timestamp = datetime.fromisoformat(timestamp_str)\n            age = (datetime.now() - timestamp).total_seconds() / 3600\n            return age < hours\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\end_of_cycle_push.py",
          "count": 6,
          "lines": [
            "            try:\n                subprocess.run(\n                    ['git', 'commit', '-m', commit_msg],\n                    check=True,\n                    capture_output=True\n                )\n                self.tracker.record_commit()\n            except subprocess.CalledProcessError as e:",
            "            try:\n                subprocess.run(\n                    ['git', 'push'],\n                    check=True,\n                    capture_output=True\n                )\n                self.tracker.mark_pushed()\n            except subprocess.CalledProcessError as e:",
            "        try:\n            result = subprocess.run(\n                ['git', 'diff', '--name-only'],\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            staged = subprocess.run(\n                ['git', 'diff', '--cached', '--name-only'],\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            return list(set(result.stdout.strip().split('\\n') + \n                          staged.stdout.strip().split('\\n'))) if result.stdout.strip() or staged.stdout.strip() else []\n        except subprocess.CalledProcessError:",
            "        try:\n            result = subprocess.run(\n                ['git', 'log', '--oneline', 'origin/main..HEAD'],\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            return [line for line in result.stdout.strip().split('\\n') if line]\n        except subprocess.CalledProcessError:",
            "        try:\n            result = subprocess.run(\n                ['git', 'status', '--porcelain'],\n                capture_output=True,\n                text=True,\n                check=True\n            )\n            return result.stdout.strip()\n        except subprocess.CalledProcessError:",
            "        try:\n            subprocess.run(['git', 'add', '-A'], check=True, capture_output=True)\n        except subprocess.CalledProcessError as e:"
          ]
        },
        {
          "file": "src\\core\\enhanced_activity_status_checker.py",
          "count": 3,
          "lines": [
            "            try:\n                from tools.agent_activity_detector import AgentActivityDetector\n                self.activity_detector = AgentActivityDetector(self.workspace_root)\n            except ImportError:",
            "            try:\n                summary = self.activity_detector.detect_agent_activity(\n                    agent_id,\n                    lookback_minutes=self.activity_lookback_minutes,\n                    use_events=True\n                )\n                \n                details[\"has_recent_activity\"] = summary.is_active\n                details[\"activity_sources\"] = summary.activity_sources\n                \n                if summary.last_activity:\n                    minutes_old = (\n                        now - summary.last_activity\n                    ).total_seconds() / 60\n                    details[\"minutes_since_activity\"] = round(minutes_old, 1)\n                    \n                    # Agent is active if:\n                    # 1. Activity detected within lookback window, OR\n                    # 2. Last activity is more recent than status timestamp\n                    if summary.is_active or (\n                        status_timestamp and\n                        summary.last_activity > status_timestamp\n                    ):\n                        details[\"has_recent_activity\"] = True\n                        details[\"is_actually_stalled\"] = False\n                        return False, details\n                \n            except Exception as e:",
            "        try:\n            import json\n            with open(status_file, 'r', encoding='utf-8') as f:\n                status = json.load(f)\n            \n            last_updated_str = status.get(\"last_updated\", \"\")\n            if not last_updated_str:\n                # Fallback to file mtime\n                return datetime.fromtimestamp(status_file.stat().st_mtime)\n            \n            # Try parsing timestamp\n            formats = [\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M:%S\"]\n            for fmt in formats:\n                try:\n                    return datetime.strptime(last_updated_str, fmt)\n                except ValueError:"
          ]
        },
        {
          "file": "src\\core\\gasline_integrations.py",
          "count": 5,
          "lines": [
            "        try:\n            # Import debate-to-gas integration\n            from src.core.debate_to_gas_integration import activate_debate_decision\n            \n            # Activate with full context\n            execution_plan = self._generate_execution_plan_from_brain(topic)\n            \n            activate_debate_decision(\n                topic=topic,\n                decision=decision,\n                execution_plan=execution_plan,\n                agent_assignments=agent_assignments\n            )\n            \n            logger.info(f\" Debate '{topic}' activated via gasline\")\n            return True\n            \n        except Exception as e:",
            "        try:\n            # Prioritize violations by ROI\n            prioritized = self._prioritize_violations_with_brain(violations)\n            \n            # Auto-assign to best agents\n            if auto_assign:\n                assignments = self._assign_violations_to_agents(prioritized)\n                \n                # Deliver via gasline\n                for agent_id, violation_tasks in assignments.items():\n                    self._send_violation_assignment(agent_id, violation_tasks)\n            \n            logger.info(f\" {len(violations)} violations activated via gasline\")\n            return True\n            \n        except Exception as e:",
            "        try:\n            # Use smart assignment with Swarm Brain + Markov optimizer\n            from src.core.smart_assignment_optimizer import SmartAssignmentOptimizer\n            \n            smart_assigner = SmartAssignmentOptimizer()\n            assignments = smart_assigner.assign_violations(violations)\n            logger.info(f\"Smart assignment completed for {len(violations)} violations\")\n            return assignments\n        except Exception as e:",
            "        try:\n            from src.services.messaging_cli_handlers import send_message_to_agent\n            send_message_to_agent(\n                agent_id=agent_id,\n                message=message,\n                sender=\"Swarm Brain (Auto-Assignment)\",\n                priority=\"high\",\n                use_pyautogui=True\n            )\n            logger.info(f\" Violation assignment sent to {agent_id}\")\n        except ImportError:",
            "        try:\n            from src.swarm_brain.swarm_memory import SwarmMemory\n            \n            memory = SwarmMemory(agent_id=agent_id)\n            results = memory.search_swarm_knowledge(query)\n            \n            if results:\n                # Send results via gasline\n                self._send_knowledge_results(agent_id, query, results)\n                return True\n            else:\n                # No results - send guidance\n                self._send_no_results_guidance(agent_id, query)\n                return False\n                \n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\keyboard_control_lock.py",
          "count": 2,
          "lines": [
            "    try:\n        # Force release - may raise RuntimeError if lock not held by this thread\n        # But we check locked() first, so this should be safe\n        _keyboard_control_lock.release()\n        logger.info(\" Keyboard lock force released\")\n        return True\n    except RuntimeError as e:",
            "    try:\n        # Try to acquire lock with timeout to prevent deadlocks\n        acquired = _keyboard_control_lock.acquire(timeout=_LOCK_TIMEOUT)\n        \n        if not acquired:\n            raise RuntimeError(\n                f\" TIMEOUT: Could not acquire keyboard lock within {_LOCK_TIMEOUT}s. \"\n                f\"Another source may be holding it: {_current_holder}\"\n            )\n        \n        _current_holder = source\n        logger.debug(f\" Keyboard lock acquired by: {source}\")\n        \n        yield\n        \n    except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\local_repo_layer.py",
          "count": 11,
          "lines": [
            "            try:\n                backup_file = self.metadata_file.with_suffix('.json.bak')\n                import shutil\n                shutil.copy2(self.metadata_file, backup_file)\n                logger.debug(f\" Backed up metadata to {backup_file}\")\n            except Exception as e:",
            "            try:\n                shutil.rmtree(repo_path)\n                logger.info(f\" Removed repository: {repo_name}\")\n            except Exception as e:",
            "            try:\n                temp_file = self.metadata_file.with_suffix('.json.tmp')\n                if temp_file.exists():\n                    temp_file.unlink()\n            except:",
            "        try:\n            # Checkout target branch\n            subprocess.run(\n                [\"git\", \"checkout\", target_branch],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_DEFAULT\n            )\n            \n            # Merge source branch\n            result = subprocess.run(\n                [\"git\", \"merge\", \"--no-commit\", \"--no-ff\", source_branch],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_MEDIUM\n            )\n            \n            if result.returncode != 0:\n                # Check for conflicts\n                if \"CONFLICT\" in result.stdout or \"CONFLICT\" in result.stderr:\n                    logger.warning(f\" Merge conflict detected: {source_branch}  {target_branch}\")\n                    return False, \"Merge conflict detected\"\n                \n                logger.error(f\" Merge failed: {result.stderr}\")\n                return False, result.stderr\n            \n            logger.info(f\" Merged {source_branch}  {target_branch} in {repo_name}\")\n            return True, None\n            \n        except Exception as e:",
            "        try:\n            # Ensure directory exists\n            self.metadata_file.parent.mkdir(parents=True, exist_ok=True)\n            \n            # Write to temporary file first, then rename (atomic write)\n            temp_file = self.metadata_file.with_suffix('.json.tmp')\n            with open(temp_file, 'w', encoding='utf-8') as f:\n                json.dump(self.repos, f, indent=2, ensure_ascii=False)\n            \n            # Atomic rename\n            if temp_file.exists():\n                if self.metadata_file.exists():\n                    self.metadata_file.unlink()\n                temp_file.rename(self.metadata_file)\n                \n        except PermissionError as e:",
            "        try:\n            # Get base branch (main or master)\n            result = subprocess.run(\n                [\"git\", \"branch\", \"--show-current\"],\n                cwd=repo_path,\n                capture_output=True,\n                text=True\n            )\n            base_branch = result.stdout.strip() if result.returncode == 0 else \"main\"\n            \n            # Generate patch\n            result = subprocess.run(\n                [\"git\", \"format-patch\", \"-1\", branch, \"--stdout\"],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_DEFAULT\n            )\n            \n            if result.returncode != 0:\n                logger.error(f\" Patch generation failed: {result.stderr}\")\n                return None\n            \n            # Write patch file\n            with open(output_path, 'w', encoding='utf-8') as f:\n                f.write(result.stdout)\n            \n            logger.info(f\" Patch generated: {output_path}\")\n            return output_path\n            \n        except Exception as e:",
            "        try:\n            logger.info(f\" Cloning {repo_name} from GitHub...\")\n            result = subprocess.run(\n                [\"git\", \"clone\", \"--depth\", \"1\", \"-b\", branch, github_url, str(repo_path)],\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_LONG\n            )\n            \n            if result.returncode != 0:\n                logger.error(f\" Clone failed: {result.stderr}\")\n                return False, None\n            \n            # Register in metadata\n            self.repos[repo_name] = {\n                \"github_url\": github_url,\n                \"github_user\": github_user,\n                \"local_path\": str(repo_path),\n                \"branch\": branch,\n                \"cloned_at\": datetime.now().isoformat(),\n                \"last_synced\": datetime.now().isoformat(),\n                \"status\": \"active\"\n            }\n            self._save_metadata()\n            \n            logger.info(f\" {repo_name}: Cloned to {repo_path}\")\n            return True, repo_path\n            \n        except subprocess.TimeoutExpired:",
            "        try:\n            logger.info(f\" Cloning {repo_name} from local source...\")\n            result = subprocess.run(\n                [\"git\", \"clone\", str(source_path), str(repo_path)],\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_LONG\n            )\n            \n            if result.returncode != 0:\n                logger.error(f\" Local clone failed: {result.stderr}\")\n                return False, None\n            \n            # Checkout branch\n            subprocess.run(\n                [\"git\", \"checkout\", branch],\n                cwd=repo_path,\n                capture_output=True,\n                text=True\n            )\n            \n            # Register in metadata\n            self.repos[repo_name] = {\n                \"source_path\": str(source_path),\n                \"local_path\": str(repo_path),\n                \"branch\": branch,\n                \"cloned_at\": datetime.now().isoformat(),\n                \"status\": \"local_only\"\n            }\n            self._save_metadata()\n            \n            logger.info(f\" {repo_name}: Local clone created\")\n            return True, repo_path\n            \n        except Exception as e:",
            "        try:\n            result = subprocess.run(\n                [\"git\", \"checkout\", \"-b\", branch_name],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_DEFAULT\n            )\n            \n            if result.returncode != 0:\n                logger.error(f\" Branch creation failed: {result.stderr}\")\n                return False\n            \n            logger.info(f\" Branch {branch_name} created in {repo_name}\")\n            return True\n            \n        except Exception as e:",
            "        try:\n            self.base_path.mkdir(parents=True, exist_ok=True)\n        except PermissionError as e:",
            "        try:\n            with open(self.metadata_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            # Self-healing: Validate structure\n            if not isinstance(data, dict):\n                logger.warning(f\" Invalid metadata structure, resetting\")\n                self._backup_metadata()\n                return {}\n            \n            # Validate each repo entry has required fields\n            validated_data = {}\n            for repo_name, repo_info in data.items():\n                if isinstance(repo_info, dict) and \"local_path\" in repo_info:\n                    # Verify path still exists, remove if not\n                    local_path = Path(repo_info.get(\"local_path\", \"\"))\n                    if local_path.exists():\n                        validated_data[repo_name] = repo_info\n                    else:\n                        logger.debug(f\" Removing stale repo entry: {repo_name} (path doesn't exist)\")\n                else:\n                    logger.debug(f\" Invalid repo entry format: {repo_name}\")\n            \n            # If we removed entries, save the cleaned version\n            if len(validated_data) != len(data):\n                logger.info(f\" Cleaned metadata: {len(data)}  {len(validated_data)} repos\")\n                # Save will happen after self.repos is set in __init__\n            \n            return validated_data\n            \n        except json.JSONDecodeError as e:"
          ]
        },
        {
          "file": "src\\core\\merge_conflict_resolver.py",
          "count": 3,
          "lines": [
            "        try:\n            # Checkout target branch\n            subprocess.run(\n                [\"git\", \"checkout\", target_branch],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_DEFAULT\n            )\n            \n            # Detect conflicts first\n            has_conflicts, conflict_files = self.detect_conflicts(\n                repo_path, source_branch, target_branch\n            )\n            \n            # Perform merge\n            result = subprocess.run(\n                [\"git\", \"merge\", \"--no-commit\", \"--no-ff\", source_branch],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_MEDIUM\n            )\n            \n            # Resolve conflicts if any\n            if has_conflicts and conflict_files:\n                logger.warning(f\" Conflicts detected: {len(conflict_files)} files\")\n                \n                # Auto-resolve\n                resolution_success = self.resolve_conflicts_auto(\n                    repo_path, conflict_files, resolution_strategy\n                )\n                \n                if not resolution_success:\n                    return False, conflict_files, \"Conflict resolution failed\"\n                \n                # Generate conflict report\n                report = self.generate_conflict_report(repo_path, conflict_files)\n                logger.info(f\" Conflict report: {report['conflict_count']} files resolved\")\n                \n                return True, conflict_files, None\n            \n            elif result.returncode == 0:\n                logger.info(f\" Merge successful: {source_branch}  {target_branch}\")\n                return True, None, None\n            else:\n                error_msg = result.stderr or result.stdout\n                logger.error(f\" Merge failed: {error_msg}\")\n                return False, None, error_msg\n                \n        except Exception as e:",
            "        try:\n            # Checkout target branch\n            subprocess.run(\n                [\"git\", \"checkout\", target_branch],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_DEFAULT\n            )\n            \n            # Try merge (dry run)\n            result = subprocess.run(\n                [\"git\", \"merge\", \"--no-commit\", \"--no-ff\", source_branch],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_MEDIUM\n            )\n            \n            # Check for conflict markers\n            conflict_files = []\n            if result.returncode != 0:\n                # Parse conflict output\n                conflict_pattern = re.compile(r\"CONFLICT \\(.*?\\): Merge conflict in (.+)\")\n                matches = conflict_pattern.findall(result.stdout + result.stderr)\n                conflict_files = list(set(matches))\n            \n            # Abort merge\n            subprocess.run(\n                [\"git\", \"merge\", \"--abort\"],\n                cwd=repo_path,\n                capture_output=True,\n                text=True,\n                timeout=TimeoutConstants.HTTP_DEFAULT\n            )\n            \n            has_conflicts = len(conflict_files) > 0\n            return has_conflicts, conflict_files\n            \n        except Exception as e:",
            "        try:\n            if strategy == \"theirs\":\n                # Use source branch version (theirs)\n                for file_path in conflict_files:\n                    result = subprocess.run(\n                        [\"git\", \"checkout\", \"--theirs\", file_path],\n                        cwd=repo_path,\n                        capture_output=True,\n                        text=True,\n                        timeout=TimeoutConstants.HTTP_DEFAULT\n                    )\n                    if result.returncode != 0:\n                        logger.error(f\" Failed to resolve {file_path}\")\n                        return False\n                \n                # Stage resolved files\n                for file_path in conflict_files:\n                    subprocess.run(\n                        [\"git\", \"add\", file_path],\n                        cwd=repo_path,\n                        capture_output=True,\n                        text=True\n                    )\n                \n                logger.info(f\" Auto-resolved {len(conflict_files)} conflicts using '{strategy}' strategy\")\n                return True\n            \n            elif strategy == \"ours\":\n                # Use target branch version (ours)\n                for file_path in conflict_files:\n                    result = subprocess.run(\n                        [\"git\", \"checkout\", \"--ours\", file_path],\n                        cwd=repo_path,\n                        capture_output=True,\n                        text=True,\n                        timeout=TimeoutConstants.HTTP_DEFAULT\n                    )\n                    if result.returncode != 0:\n                        logger.error(f\" Failed to resolve {file_path}\")\n                        return False\n                \n                # Stage resolved files\n                for file_path in conflict_files:\n                    subprocess.run(\n                        [\"git\", \"add\", file_path],\n                        cwd=repo_path,\n                        capture_output=True,\n                        text=True\n                    )\n                \n                logger.info(f\" Auto-resolved {len(conflict_files)} conflicts using '{strategy}' strategy\")\n                return True\n            \n            else:\n                logger.error(f\" Unknown resolution strategy: {strategy}\")\n                return False\n                \n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\message_queue_error_monitor.py",
          "count": 5,
          "lines": [
            "        try:\n            # Load existing alerts\n            existing = []\n            if self.alerts_file.exists():\n                with open(self.alerts_file, 'r', encoding='utf-8') as f:\n                    existing = json.load(f)\n            \n            # Append new alerts\n            existing.extend(alerts)\n            \n            # Keep only last 1000 alerts\n            if len(existing) > 1000:\n                existing = existing[-1000:]\n            \n            # Save\n            with open(self.alerts_file, 'w', encoding='utf-8') as f:\n                json.dump(existing, f, indent=2, default=str)\n                \n        except Exception as e:",
            "        try:\n            # Load queue entries\n            with open(self.queue_file, 'r', encoding='utf-8') as f:\n                entries = json.load(f)\n            \n            if not isinstance(entries, list):\n                return 0\n            \n            # Create lookup for stuck message IDs\n            stuck_ids = {alert.queue_id for alert in stuck_alerts}\n            \n            # Reset stuck messages\n            reset_count = 0\n            for entry in entries:\n                if entry.get('queue_id') in stuck_ids:\n                    entry['status'] = 'PENDING'\n                    entry['updated_at'] = datetime.now().isoformat()\n                    reset_count += 1\n                    logger.info(f\"Reset stuck message {entry.get('queue_id')} to PENDING\")\n            \n            # Save updated entries\n            if reset_count > 0:\n                # Create backup\n                backup_file = self.queue_file.with_suffix('.json.backup')\n                if self.queue_file.exists():\n                    import shutil\n                    shutil.copy2(self.queue_file, backup_file)\n                \n                # Save updated queue\n                with open(self.queue_file, 'w', encoding='utf-8') as f:\n                    json.dump(entries, f, indent=2, default=str)\n                \n                logger.info(f\"Reset {reset_count} stuck messages to PENDING\")\n            \n            return reset_count\n            \n        except Exception as e:",
            "        try:\n            # Load queue entries\n            with open(self.queue_file, 'r', encoding='utf-8') as f:\n                entries = json.load(f)\n            \n            if not isinstance(entries, list):\n                return None\n            \n            # Get recent entries (last N)\n            recent_entries = entries[-window_size:] if len(entries) > window_size else entries\n            \n            if not recent_entries:\n                return None\n            \n            # Count failures\n            total = len(recent_entries)\n            failed = sum(1 for e in recent_entries if e.get('status') == 'FAILED')\n            failure_rate = failed / total if total > 0 else 0.0\n            \n            # Check thresholds\n            if failure_rate >= 0.10:  # 10%\n                severity = 'critical'\n            elif failure_rate >= 0.05:  # 5%\n                severity = 'warning'\n            else:\n                return None  # Within acceptable range\n            \n            alert = ErrorAlert(\n                alert_type='high_failure_rate',\n                severity=severity,\n                message=f\"Failure rate {failure_rate:.1%} exceeds threshold\",\n                timestamp=datetime.now().isoformat(),\n                details={\n                    'failure_rate': failure_rate,\n                    'failed_messages': failed,\n                    'total_messages': total,\n                    'window_size': window_size\n                }\n            )\n            \n            return alert\n            \n        except Exception as e:",
            "        try:\n            # Load queue entries\n            with open(self.queue_file, 'r', encoding='utf-8') as f:\n                entries = json.load(f)\n            \n            if not isinstance(entries, list):\n                return None\n            \n            total = len(entries)\n            pending = sum(1 for e in entries if e.get('status') == 'PENDING')\n            processing = sum(1 for e in entries if e.get('status') == 'PROCESSING')\n            failed = sum(1 for e in entries if e.get('status') == 'FAILED')\n            \n            # Check queue size\n            if total > 1000:\n                severity = 'critical'\n                message = f\"Queue size critically high: {total} messages\"\n            elif total > 500:\n                severity = 'warning'\n                message = f\"Queue size elevated: {total} messages\"\n            else:\n                return None  # Queue size acceptable\n            \n            alert = ErrorAlert(\n                alert_type='queue_size',\n                severity=severity,\n                message=message,\n                timestamp=datetime.now().isoformat(),\n                details={\n                    'total_messages': total,\n                    'pending': pending,\n                    'processing': processing,\n                    'failed': failed\n                }\n            )\n            \n            return alert\n            \n        except Exception as e:",
            "        try:\n            # Load queue entries\n            with open(self.queue_file, 'r', encoding='utf-8') as f:\n                entries = json.load(f)\n            \n            if not isinstance(entries, list):\n                return alerts\n            \n            current_time = time.time()\n            \n            for entry in entries:\n                status = entry.get('status', 'UNKNOWN')\n                \n                # Only check PROCESSING status\n                if status != 'PROCESSING':\n                    continue\n                \n                # Get processing start time\n                updated_at = entry.get('updated_at')\n                if not updated_at:\n                    # Fall back to created_at if updated_at missing\n                    updated_at = entry.get('created_at')\n                \n                if not updated_at:\n                    # Can't determine age, skip\n                    continue\n                \n                # Parse timestamp\n                try:\n                    if isinstance(updated_at, str):\n                        # Try ISO format\n                        if 'T' in updated_at:\n                            dt = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))\n                        else:\n                            # Try other formats\n                            dt = datetime.strptime(updated_at, \"%Y-%m-%d %H:%M:%S\")\n                    else:\n                        # Assume Unix timestamp\n                        dt = datetime.fromtimestamp(updated_at)\n                    \n                    processing_start = dt.timestamp()\n                    stuck_duration = current_time - processing_start\n                    \n                    # Check thresholds\n                    if stuck_duration >= self.STUCK_CRITICAL_THRESHOLD:\n                        severity = 'critical'\n                    elif stuck_duration >= self.STUCK_WARNING_THRESHOLD:\n                        severity = 'warning'\n                    else:\n                        continue  # Not stuck yet\n                    \n                    alert = StuckMessageAlert(\n                        queue_id=entry.get('queue_id', 'unknown'),\n                        recipient=entry.get('message', {}).get('recipient', 'unknown'),\n                        status=status,\n                        stuck_duration_seconds=stuck_duration,\n                        first_processing_time=updated_at,\n                        current_time=datetime.now().isoformat(),\n                        severity=severity\n                    )\n                    alerts.append(alert)\n                    \n                except (ValueError, TypeError) as e:"
          ]
        },
        {
          "file": "src\\core\\message_queue_helpers.py",
          "count": 3,
          "lines": [
            "    try:\n        from .agent_activity_tracker import get_activity_tracker\n        tracker = get_activity_tracker()\n        msg_data = message if isinstance(message, dict) else {}\n        sender = msg_data.get(\"sender\", msg_data.get(\"from\", \"UNKNOWN\"))\n        if sender.startswith(\"Agent-\"):\n            tracker.mark_active(sender, \"message_queuing\")\n    except Exception as e:",
            "    try:\n        msg_data = message if isinstance(message, dict) else {}\n        message_repository.save_message({\n            \"from\": msg_data.get(\"sender\", msg_data.get(\"from\", \"UNKNOWN\")),\n            \"to\": msg_data.get(\"recipient\", msg_data.get(\"to\", \"UNKNOWN\")),\n            \"message_type\": msg_data.get(\"type\", \"text\"),\n            \"priority\": msg_data.get(\"priority\", \"normal\"),\n            \"content\": str(msg_data.get(\"content\", \"\"))[:500],\n            \"content_length\": len(str(msg_data.get(\"content\", \"\"))),\n            \"queue_id\": queue_id,\n            \"source\": msg_data.get(\"source\", \"queue\"),\n            \"status\": \"QUEUED\",\n            \"timestamp\": format_swarm_timestamp(now),\n        })\n        if logger:\n            logger.debug(f\" Queued message logged to history: {queue_id}\")\n    except Exception as e:",
            "    try:\n        msg_data = message if isinstance(message, dict) else {}\n        metrics_engine.increment_metric(\"queue.enqueued\")\n        sender = msg_data.get(\"sender\", \"UNKNOWN\")\n        recipient = msg_data.get(\"recipient\", \"UNKNOWN\")\n        metrics_engine.increment_metric(f\"queue.enqueued.by_sender.{sender}\")\n        metrics_engine.increment_metric(\n            f\"queue.enqueued.by_recipient.{recipient}\"\n        )\n        metrics_engine.record_metric(\"queue.size\", queue_size)\n    except Exception:"
          ]
        },
        {
          "file": "src\\core\\message_queue_performance_metrics.py",
          "count": 3,
          "lines": [
            "        try:\n            # Load existing metrics\n            existing = []\n            if self.metrics_file.exists():\n                with open(self.metrics_file, 'r', encoding='utf-8') as f:\n                    existing = json.load(f)\n            \n            # Append new metrics\n            new_metrics = [asdict(m) for m in self._delivery_metrics[-10:]]\n            existing.extend(new_metrics)\n            \n            # Keep only last 1000 metrics to prevent file bloat\n            if len(existing) > 1000:\n                existing = existing[-1000:]\n            \n            # Save\n            with open(self.metrics_file, 'w', encoding='utf-8') as f:\n                json.dump(existing, f, indent=2)\n        except Exception as e:",
            "        try:\n            with open(self.baseline_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            return PerformanceBaseline(**data)\n        except Exception as e:",
            "        try:\n            with open(self.baseline_file, 'w', encoding='utf-8') as f:\n                json.dump(asdict(baseline), f, indent=2)\n            logger.info(f\" Baseline metrics saved to {self.baseline_file}\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\messaging_process_lock.py",
          "count": 6,
          "lines": [
            "                    try:\n                        fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)\n                        logger.debug(\n                            f\" Cross-process lock acquired (POSIX) after {attempt} attempts\"\n                        )\n                        return True\n                    except BlockingIOError:",
            "                    try:\n                        fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)\n                    except:",
            "            try:\n                attempt += 1\n\n                # Thread-level lock first (within process)\n                if not self._lock.acquire(blocking=False):\n                    time.sleep(current_delay)\n                    if use_exponential_backoff:\n                        current_delay = min(current_delay * 1.5, max_delay)\n                    continue\n\n                # Create lock file\n                self.lock_file = open(self.lock_file_path, \"w\")\n\n                # Attempt file lock (cross-process)\n                if WINDOWS and LOCK_AVAILABLE:\n                    # Windows: Use msvcrt\n                    try:\n                        msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_NBLCK, 1)\n                        logger.debug(\n                            f\" Cross-process lock acquired (Windows) after {attempt} attempts\"\n                        )\n                        return True\n                    except OSError:",
            "        try:\n            if self.lock_file:\n                # Release file lock\n                if WINDOWS and LOCK_AVAILABLE:\n                    try:\n                        msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_UNLCK, 1)\n                    except:",
            "    try:\n        import fcntl  # POSIX file locking (Linux/Mac)\n\n        LOCK_AVAILABLE = True\n        msvcrt = None  # Not available on POSIX\n    except ImportError:",
            "    try:\n        import msvcrt  # Windows file locking\n\n        LOCK_AVAILABLE = True\n        fcntl = None  # Not available on Windows\n    except ImportError:"
          ]
        },
        {
          "file": "src\\core\\mock_unified_messaging_core.py",
          "count": 1,
          "lines": [
            "        try:\n            # Chaos mode: Random crash simulation\n            if self.config.chaos_mode:\n                if random.random() < self.config.chaos_crash_rate:\n                    latency_ms = (time.time() - start_time) * 1000\n                    self._record_delivery(False, latency_ms, \"CHAOS_CRASH\")\n                    return False\n            \n            # Calculate base latency (1-10ms)\n            base_latency_ms = random.uniform(\n                self.config.min_latency_ms,\n                self.config.max_latency_ms\n            )\n            \n            # Chaos mode: Random latency spike\n            chaos_spike = 0.0\n            chaos_event = None\n            if self.config.chaos_mode:\n                if random.random() < self.config.chaos_latency_spike_rate:\n                    chaos_spike = random.uniform(0, self.config.chaos_max_spike_ms)\n                    base_latency_ms += chaos_spike\n                    chaos_event = f\"LATENCY_SPIKE_{chaos_spike:.1f}ms\"\n            \n            # Simulate latency\n            time.sleep(base_latency_ms / 1000.0)\n            \n            # Determine success/failure based on success rate\n            is_success = random.random() < self.config.success_rate\n            \n            latency_ms = (time.time() - start_time) * 1000\n            \n            # Record delivery\n            self._record_delivery(\n                is_success,\n                latency_ms,\n                chaos_event=chaos_event\n            )\n            \n            if is_success:\n                self.logger.debug(\n                    f\" Mock delivered: {sender}  {recipient} \"\n                    f\"({latency_ms:.2f}ms)\"\n                )\n            else:\n                self.logger.debug(\n                    f\" Mock failed: {sender}  {recipient} \"\n                    f\"({latency_ms:.2f}ms)\"\n                )\n            \n            return is_success\n            \n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\multi_agent_request_validator.py",
          "count": 1,
          "lines": [
            "        try:\n            # Get all active collectors\n            with self.responder.lock:\n                collectors = self.responder.collectors.values()\n            \n            # Find collectors where this agent is a recipient and hasn't responded\n            for collector in collectors:\n                if collector.status in [ResponseStatus.PENDING, ResponseStatus.COLLECTING]:\n                    if agent_id in collector.recipients:\n                        # Check if agent has responded\n                        if agent_id not in collector.responses:\n                            # Agent has pending request!\n                            return {\n                                \"collector_id\": collector.collector_id,\n                                \"request_id\": collector.request_id,\n                                \"sender\": collector.sender,\n                                \"original_message\": collector.original_message,\n                                \"recipient_count\": len(collector.recipients),\n                                \"responses_received\": collector.get_response_count(),\n                                \"timeout_seconds\": collector.timeout_seconds,\n                                \"created_at\": collector.created_at,\n                                \"is_pending\": True\n                            }\n            \n            return None\n            \n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\multi_agent_responder.py",
          "count": 3,
          "lines": [
            "                try:\n                    self._check_timeouts()\n                    time.sleep(10)  # Check every 10 seconds\n                except Exception as e:",
            "        try:\n            file_path = self.storage_dir / f\"{collector.collector_id}.json\"\n            data = {\n                \"collector_id\": collector.collector_id,\n                \"request_id\": collector.request_id,\n                \"sender\": collector.sender,\n                \"recipients\": collector.recipients,\n                \"original_message\": collector.original_message,\n                \"status\": collector.status.value,\n                \"responses\": {\n                    agent_id: {\n                        \"response\": resp.response,\n                        \"timestamp\": resp.timestamp.isoformat()\n                    }\n                    for agent_id, resp in collector.responses.items()\n                },\n                \"combined_response\": combined,\n                \"created_at\": collector.created_at.isoformat(),\n                \"finalized_at\": datetime.now().isoformat()\n            }\n            \n            import json\n            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, indent=2)\n            \n        except Exception as e:",
            "        try:\n            from ..services.messaging_infrastructure import MessageCoordinator\n            from ..core.messaging_models_core import UnifiedMessageType, UnifiedMessagePriority, UnifiedMessageTag\n            \n            # Deliver combined message to original sender\n            delivery_result = MessageCoordinator.send_to_agent(\n                agent=collector.sender,\n                message=combined,\n                priority=UnifiedMessagePriority.REGULAR,\n                use_pyautogui=True,\n                stalled=False\n            )\n            \n            if isinstance(delivery_result, dict) and delivery_result.get(\"success\"):\n                logger.info(\n                    f\" Combined message delivered to {collector.sender} \"\n                    f\"(collector: {collector_id}, queue_id: {delivery_result.get('queue_id', 'unknown')})\"\n                )\n            else:\n                logger.warning(\n                    f\" Failed to deliver combined message to {collector.sender} \"\n                    f\"(collector: {collector_id})\"\n                )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\onboarding_service.py",
          "count": 2,
          "lines": [
            "            try:\n                from ..services.onboarding_template_loader import OnboardingTemplateLoader\n                self._template_loader = OnboardingTemplateLoader()\n            except ImportError:",
            "        try:\n            # Try to use template loader if available\n            if self.template_loader:\n                return self.template_loader.load_onboarding_template(agent_id, style)\n            \n            # Fallback to default message\n            return self._default_onboarding_message(agent_id, style)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\pydantic_config.py",
          "count": 1,
          "lines": [
            "try:\n    from pydantic import ConfigDict\n    PYDANTIC_V2_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\repository_merge_improvements.py",
          "count": 5,
          "lines": [
            "            try:\n                exists = github_client.repo_exists(repo_name)\n                status = RepoStatus.EXISTS if exists else RepoStatus.DELETED\n                self.update_repo_status(repo_name, status)\n                return exists, None if exists else f\"Repository {repo_name} does not exist\"\n            except Exception as e:",
            "        try:\n            ATTEMPT_TRACKING_FILE.parent.mkdir(parents=True, exist_ok=True)\n            data = {\n                pair: {\n                    **to_dict(attempt),\n                    'error_type': attempt.error_type.value if attempt.error_type else None\n                }\n                for pair, attempt in self.merge_attempts.items()\n            }\n            ATTEMPT_TRACKING_FILE.write_text(\n                json.dumps(data, indent=2, ensure_ascii=False),\n                encoding='utf-8'\n            )\n        except Exception as e:",
            "        try:\n            REPO_STATUS_FILE.parent.mkdir(parents=True, exist_ok=True)\n            data = {\n                name: {\n                    **to_dict(meta),\n                    'status': meta.status.value\n                }\n                for name, meta in self.repo_statuses.items()\n            }\n            REPO_STATUS_FILE.write_text(\n                json.dumps(data, indent=2, ensure_ascii=False),\n                encoding='utf-8'\n            )\n        except Exception as e:",
            "        try:\n            if ATTEMPT_TRACKING_FILE.exists():\n                data = json.loads(ATTEMPT_TRACKING_FILE.read_text(encoding='utf-8'))\n                for pair, attempt in data.items():\n                    self.merge_attempts[pair] = MergeAttempt(\n                        **{**attempt, 'error_type': ErrorType(attempt['error_type']) if attempt.get('error_type') else None}\n                    )\n        except Exception as e:",
            "        try:\n            if REPO_STATUS_FILE.exists():\n                data = json.loads(REPO_STATUS_FILE.read_text(encoding='utf-8'))\n                for name, meta in data.items():\n                    self.repo_statuses[name] = RepoMetadata(\n                        **{**meta, 'status': RepoStatus(meta['status'])}\n                    )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\resume_cycle_planner_integration.py",
          "count": 3,
          "lines": [
            "        try:\n            # Use ContractManager to get and claim next task\n            result = self.contract_manager.get_next_task(agent_id)\n            \n            if result.get(\"status\") == \"assigned\" and result.get(\"task\"):\n                task = result[\"task\"]\n                \n                logger.info(\n                    f\" Claimed cycle planner task for {agent_id}: \"\n                    f\"{task.get('title', 'Unknown')}\"\n                )\n                \n                # Return task with assignment context\n                return {\n                    \"task_id\": task.get(\"task_id\") or task.get(\"contract_id\", \"\"),\n                    \"title\": task.get(\"title\", \"Untitled Task\"),\n                    \"description\": task.get(\"description\", \"\"),\n                    \"priority\": task.get(\"priority\", \"MEDIUM\"),\n                    \"status\": \"assigned\",  # Task has been claimed\n                    \"assigned_at\": task.get(\"assigned_at\"),\n                    \"source\": result.get(\"source\", \"cycle_planner\"),\n                    \"estimated_time\": task.get(\"estimated_time\", \"\"),\n                    \"deliverables\": task.get(\"deliverables\", []),\n                }\n            \n            elif result.get(\"status\") == \"no_tasks\":\n                logger.debug(f\"No tasks available for {agent_id}\")\n                return None\n            \n            else:\n                logger.warning(\n                    f\"Unexpected result from get_next_task for {agent_id}: {result}\"\n                )\n                return None\n                \n        except Exception as e:",
            "        try:\n            from src.services.unified_service_managers import UnifiedContractManager\n            from src.services.contract_system.cycle_planner_integration import (\n                CyclePlannerIntegration\n            )\n            self.contract_manager = UnifiedContractManager()\n            self.cycle_planner = CyclePlannerIntegration()\n            self._initialized = True\n        except ImportError as e:",
            "        try:\n            task = self.cycle_planner.get_next_cycle_task(agent_id)\n            \n            if task:\n                return {\n                    \"task_id\": task.get(\"task_id\") or task.get(\"contract_id\", \"\"),\n                    \"title\": task.get(\"title\", \"Untitled Task\"),\n                    \"description\": task.get(\"description\", \"\"),\n                    \"priority\": task.get(\"priority\", \"MEDIUM\"),\n                    \"status\": \"pending\",  # Not yet claimed\n                    \"estimated_time\": task.get(\"estimated_time\", \"\"),\n                    \"deliverables\": task.get(\"deliverables\", []),\n                }\n            \n            return None\n            \n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\self_healing_helpers.py",
          "count": 3,
          "lines": [
            "    try:\n        coord_file = Path(\"cursor_agent_coords.json\")\n        if coord_file.exists():\n            data = json.loads(coord_file.read_text(encoding=\"utf-8\"))\n            for agent_id, info in data.get(\"agents\", {}).items():\n                coords = info.get(\"chat_input_coordinates\")\n                if coords and len(coords) == 2:\n                    coordinates[agent_id] = (coords[0], coords[1])\n            logger.info(f\"Loaded coordinates for {len(coordinates)} agents\")\n    except Exception as e:",
            "    try:\n        if tracking_file.exists():\n            with open(tracking_file, 'r') as f:\n                data = json.load(f)\n                today = date.today().isoformat()\n                return {\n                    agent_id: {today: counts.get(today, 0)}\n                    for agent_id, counts in data.items()\n                    if today in counts\n                }\n        return {}\n    except Exception as e:",
            "    try:\n        tracking_file.parent.mkdir(parents=True, exist_ok=True)\n        with open(tracking_file, 'w') as f:\n            json.dump(counts, f, indent=2)\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\self_healing_integration.py",
          "count": 2,
          "lines": [
            "        try:\n            # Use optimized prompt generator\n            from src.core.optimized_stall_resume_prompt import generate_optimized_resume_prompt\n\n            # Generate optimized prompt\n            rescue_message = generate_optimized_resume_prompt(\n                agent_id=agent_id,\n                fsm_state=None,  # Will be loaded from status.json\n                last_mission=None,  # Will be loaded from status.json\n                stall_duration_minutes=stall_duration_minutes\n            )\n\n            # Send via messaging service\n            try:\n                from src.core.messaging_core import send_message\n                from src.core.messaging_models_core import (\n                    UnifiedMessageType,\n                    UnifiedMessagePriority,\n                    UnifiedMessageTag\n                )\n\n                success = send_message(\n                    content=rescue_message,\n                    sender=\"SYSTEM\",\n                    recipient=agent_id,\n                    message_type=UnifiedMessageType.SYSTEM_TO_AGENT,\n                    priority=UnifiedMessagePriority.URGENT,\n                    tags=[UnifiedMessageTag.SYSTEM]\n                )\n\n                if success:\n                    logger.info(\n                        f\" Optimized rescue message sent to {agent_id}\")\n                    return True\n                else:\n                    logger.warning(\n                        f\" Rescue message send failed for {agent_id}\")\n                    return False\n            except ImportError:",
            "        try:\n            from src.services.hard_onboarding_service import HardOnboardingService\n\n            service = HardOnboardingService()\n\n            # Get default onboarding message\n            onboarding_message = (\n                f\"[S2A] {agent_id}: System recovery - Hard onboarding initiated. \"\n                f\"Status: ACTIVE_AGENT_MODE. Ready for task assignment.\"\n            )\n\n            success = service.execute_hard_onboarding(\n                agent_id=agent_id,\n                onboarding_message=onboarding_message,\n                role=None  # Use default role\n            )\n\n            if success:\n                logger.info(f\" {agent_id}: Hard onboarding successful\")\n            else:\n                logger.error(f\" {agent_id}: Hard onboarding failed\")\n\n            return success\n\n        except ImportError as e:"
          ]
        },
        {
          "file": "src\\core\\self_healing_operations.py",
          "count": 4,
          "lines": [
            "        try:\n            status_file = self.workspace_root / agent_id / \"status.json\"\n\n            if not status_file.exists():\n                return False\n\n            with open(status_file, 'r') as f:\n                status = json.load(f)\n\n            # Clear current_tasks\n            if \"current_tasks\" in status:\n                status[\"current_tasks\"] = []\n                if \"healing_applied\" not in status:\n                    status[\"healing_applied\"] = []\n                status[\"healing_applied\"].append({\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"action\": \"clear_stuck_tasks\",\n                })\n\n                status[\"last_updated\"] = datetime.now().strftime(\n                    \"%Y-%m-%d %H:%M:%S\")\n\n                with open(status_file, 'w') as f:\n                    json.dump(status, f, indent=2)\n\n                return True\n\n            return False\n\n        except Exception as e:",
            "        try:\n            status_file = self.workspace_root / agent_id / \"status.json\"\n            if not status_file.exists():\n                return False\n\n            file_mtime = status_file.stat().st_mtime\n            age_seconds = time.time() - file_mtime\n\n            # If status updated in last 30 seconds, agent recovered\n            return age_seconds < 30\n        except Exception:",
            "        try:\n            status_file = self.workspace_root / agent_id / \"status.json\"\n            status_file.parent.mkdir(parents=True, exist_ok=True)\n\n            # Create fresh status\n            reset_status = {\n                \"agent_id\": agent_id,\n                \"status\": \"ACTIVE_AGENT_MODE\",\n                \"current_phase\": \"TASK_EXECUTION\",\n                \"last_updated\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"healing_applied\": [{\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"action\": \"reset\",\n                    \"reason\": \"stall_recovery\",\n                }],\n                \"current_mission\": \"System recovery - ready for new tasks\",\n                \"mission_priority\": \"HIGH\",\n                \"current_tasks\": [],\n                \"next_actions\": [\"Awaiting task assignment\"],\n            }\n\n            with open(status_file, 'w') as f:\n                json.dump(reset_status, f, indent=2)\n\n            logger.info(f\" {agent_id}: Status reset successfully\")\n            return True\n\n        except Exception as e:",
            "        try:\n            x, y = self.agent_coordinates[agent_id]\n            logger.info(f\" Cancelling terminal for {agent_id} at ({x}, {y})\")\n\n            # Click chat input coordinates\n            self.pyautogui.moveTo(x, y, duration=0.5)\n            self.pyautogui.click()\n            await asyncio.sleep(0.3)\n\n            # Press SHIFT+BACKSPACE to cancel terminal operations\n            self.pyautogui.hotkey(\"shift\", \"backspace\")\n            await asyncio.sleep(0.5)\n\n            # Record cancellation if callback provided\n            if self.record_cancellation:\n                cancel_count = self.record_cancellation(agent_id)\n                logger.info(\n                    f\" Terminal cancelled for {agent_id} (cancellation #{cancel_count} today)\")\n            else:\n                logger.info(f\" Terminal cancelled for {agent_id}\")\n\n            return True\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\smart_assignment_optimizer.py",
          "count": 4,
          "lines": [
            "            try:\n                history = self.swarm_memory.search_swarm_knowledge(\n                    \"agent performance history violations\"\n                )\n                # Update chain based on history if available\n                # (Simplified - would need more sophisticated parsing)\n            except Exception:",
            "        try:\n            # Search for agent's past performance on similar violations\n            query = f\"{agent_id} {violation_type} performance success\"\n            results = self.swarm_memory.search_swarm_knowledge(query)\n\n            if results:\n                # If found in brain, boost score\n                return 0.8\n            else:\n                # No history found, neutral score\n                return 0.5\n        except Exception:",
            "        try:\n            from src.discord_commander.status_reader import StatusReader\n\n            status_reader = StatusReader()\n            agent_status = status_reader.read_agent_status(agent_id)\n\n            if agent_status:\n                current_tasks = len(agent_status.get(\"current_tasks\", []))\n                # Prefer agents with fewer current tasks\n                # Score decreases as tasks increase\n                if current_tasks == 0:\n                    return 1.0\n                elif current_tasks <= 2:\n                    return 0.8\n                elif current_tasks <= 5:\n                    return 0.5\n                else:\n                    return 0.2\n\n            return 0.5  # Default if status unavailable\n        except Exception:",
            "        try:\n            from src.swarm_brain.swarm_memory import SwarmMemory\n\n            self.swarm_memory = SwarmMemory(agent_id=\"GaslineHub\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\stall_resumer_guard.py",
          "count": 1,
          "lines": [
            "    try:\n        from src.core.hardened_activity_detector import HardenedActivityDetector\n        \n        detector = HardenedActivityDetector()\n        assessment = detector.assess_agent_activity(agent_id, lookback_minutes)\n        \n        # Don't send resume if agent is active\n        if assessment.is_active:\n            reason = (\n                f\"Agent is ACTIVE (confidence: {assessment.confidence:.2f}). \"\n                f\"Last activity: {assessment.last_activity} \"\n                f\"({assessment.inactivity_minutes:.1f} min ago). \"\n                f\"Reasons: {', '.join(assessment.reasons)}\"\n            )\n            logger.info(f\" Skipping resume for {agent_id}: {reason}\")\n            return False, reason\n        \n        # Send resume if agent is inactive and validation passed\n        if not assessment.validation_passed:\n            reason = (\n                f\"Activity signals failed validation. \"\n                f\"Confidence: {assessment.confidence:.2f}. \"\n                f\"Signals: {len(assessment.signals)}\"\n            )\n            logger.warning(f\" Resume validation failed for {agent_id}: {reason}\")\n            # Still send resume but with warning\n            return True, f\"Validation failed but sending anyway: {reason}\"\n        \n        reason = (\n            f\"Agent is INACTIVE (confidence: {assessment.confidence:.2f}). \"\n            f\"Last activity: {assessment.last_activity or 'Never'}. \"\n            f\"Inactivity: {assessment.inactivity_minutes:.1f} minutes. \"\n            f\"Reasons: {', '.join(assessment.reasons)}\"\n        )\n        logger.info(f\" Sending resume to {agent_id}: {reason}\")\n        return True, reason\n        \n    except ImportError:"
          ]
        },
        {
          "file": "src\\core\\stress_test_runner.py",
          "count": 2,
          "lines": [
            "            try:\n                # Select random recipient (excluding self)\n                recipients = [a for a in self.AGENT_IDS if a != agent_id]\n                recipient = random.choice(recipients)\n                \n                # Select random message type\n                message_type = random.choice(self.message_types)\n                \n                # Generate message content\n                content = self._generate_message_content(\n                    sender=agent_id,\n                    recipient=recipient,\n                    message_type=message_type,\n                    count=message_count\n                )\n                \n                # Send message\n                start_time = time.time()\n                success = self._send_message(\n                    sender=agent_id,\n                    recipient=recipient,\n                    content=content,\n                    message_type=message_type\n                )\n                latency_ms = (time.time() - start_time) * 1000\n                \n                # Record statistics\n                agent.record_send(success, latency_ms)\n                message_count += 1\n                \n                with self._lock:\n                    self.total_messages_sent += 1\n                \n                # Wait for next message\n                time.sleep(interval)\n                \n            except Exception as e:",
            "        try:\n            # Call delivery callback with message data\n            if callable(self.delivery_callback):\n                return self.delivery_callback(\n                    sender=sender,\n                    recipient=recipient,\n                    content=content,\n                    message_type=message_type.value,\n                )\n            else:\n                # Assume it's a send_message method\n                return self.delivery_callback.send_message(\n                    content=content,\n                    sender=sender,\n                    recipient=recipient,\n                )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\workspace_agent_registry.py",
          "count": 2,
          "lines": [
            "        try:\n            from .coordinate_loader import get_coordinate_loader\n\n            loader = get_coordinate_loader()\n            return loader.get_onboarding_coordinates(agent_id)\n        except Exception:",
            "class AgentRegistry:\n    def __init__(self, root: str = \"agent_workspaces\") -> None:\n        self.root = root\n        os.makedirs(self.root, exist_ok=True)\n\n    def list_agents(self) -> list[str]:\n        if not os.path.isdir(self.root):\n            return []\n        return sorted(\n            [\n                d\n                for d in os.listdir(self.root)\n                if d.startswith(\"Agent-\") and os.path.isdir(os.path.join(self.root, d))\n            ]\n        )\n\n    def _status_path(self, agent_id: str) -> str:\n        return os.path.join(self.root, agent_id, \"status.json\")\n\n    def _onboard_path(self, agent_id: str) -> str:\n        return os.path.join(self.root, agent_id, \"onboarding.json\")\n\n    # --- Mutations ---\n    def reset_statuses(self, agents: list[str]) -> None:\n        for a in agents:\n            p = self._status_path(a)\n            os.makedirs(os.path.dirname(p), exist_ok=True)\n            with open(p, \"w\", encoding=\"utf-8\") as f:\n                json.dump({\"state\": \"RESET\", \"updated\": True}, f)\n\n    def clear_onboarding_flags(self, agents: list[str]) -> None:\n        for a in agents:\n            p = self._onboard_path(a)\n            os.makedirs(os.path.dirname(p), exist_ok=True)\n            with open(p, \"w\", encoding=\"utf-8\") as f:\n                json.dump({\"onboarded\": False, \"hard_onboarding\": True}, f)\n\n    def force_onboard(self, agent_id: str, timeout: int = 30) -> None:\n        \"\"\"Send aggressive onboarding messages via your existing messaging bus.\n<!-- SSOT Domain: core -->\n\n        Here we simulate by marking onboarded=True. Replace with real bus call.\n        \"\"\"\n        p = self._onboard_path(agent_id)\n        data = {\"onboarded\": True, \"hard_onboarding\": True, \"timeout\": timeout}\n        with open(p, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f)\n\n    def verify_onboarded(self, agent_id: str) -> bool:\n        p = self._onboard_path(agent_id)\n        if not os.path.exists(p):\n            return False\n        try:\n            data = json.load(open(p, encoding=\"utf-8\"))\n            return bool(data.get(\"onboarded\") is True)\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\coordinate_loader.py",
          "count": 3,
          "lines": [
            "        try:\n            global COORDINATES\n            COORDINATES = _load_coordinates()\n            self.coordinates = COORDINATES.copy()\n            import logging\n            logger = logging.getLogger(__name__)\n            logger.debug(f\" Reloaded coordinates from SSOT\")\n        except Exception as e:",
            "    try:\n        data = json.loads(coord_file.read_text(encoding=\"utf-8\"))\n    except (json.JSONDecodeError, IOError):",
            "    try:\n        from .agent_mode_manager import get_mode_manager\n        mode_manager = get_mode_manager()\n        active_agents = set(mode_manager.get_active_agents())\n        current_mode = mode_manager.get_current_mode()\n        logger.debug(f\" Loading coordinates for all agents (mode: {current_mode}, {len(active_agents)} active)\")\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\messaging_core.py",
          "count": 10,
          "lines": [
            "                try:\n                    from .messaging_models import MessageCategory\n                    category = MessageCategory(category_str.lower())\n                except (ValueError, AttributeError):",
            "            try:\n                from ..repositories.message_repository import MessageRepository\n                self.message_repository = MessageRepository()\n            except ImportError:",
            "        try:\n            # Apply A2A bilateral coordination template if needed\n            if (message.category and\n                message.category.name == 'A2A' and\n                    isinstance(message.content, str)):\n                try:\n                    from .messaging_template_texts import MESSAGE_TEMPLATES\n                    from .messaging_models_core import MessageCategory\n                    import uuid\n                    from datetime import datetime\n\n                    template = MESSAGE_TEMPLATES.get(MessageCategory.A2A)\n                    if template:\n                        # Populate template with message data\n                        extra_meta = {\n                            \"ask\": message.content,\n                            \"context\": \"\",\n                        }\n\n                        now = datetime.now().isoformat(timespec=\"seconds\")\n                        templated_content = template.format(\n                            sender=message.sender,\n                            recipient=message.recipient,\n                            priority=message.priority.value if hasattr(\n                                message.priority, 'value') else str(message.priority),\n                            message_id=str(uuid.uuid4()),\n                            timestamp=now,\n                            agent_id=message.recipient,\n                            ask=extra_meta.get(\"ask\", message.content),\n                            context=extra_meta.get(\"context\", \"\"),\n                            coordination_rationale=extra_meta.get(\n                                \"coordination_rationale\", \"To leverage parallel processing and accelerate completion\"),\n                            expected_contribution=extra_meta.get(\n                                \"expected_contribution\", \"Domain expertise and parallel execution\"),\n                            coordination_timeline=extra_meta.get(\n                                \"coordination_timeline\", \"ASAP - coordination needed to maintain momentum\"),\n                            next_step=extra_meta.get(\n                                \"next_step\",\n                                \"Reply via messaging_cli with ACCEPT/DECLINE, ETA, and a 23 bullet plan, \"\n                                \"then update status.json and MASTER_TASK_LOG.md.\",\n                            ),\n                            fallback=extra_meta.get(\n                                \"fallback\", \"If blocked: send blocker + proposed fix + owner.\"),\n                        )\n                        message.content = templated_content\n                except Exception as e:",
            "        try:\n            # Simple history display - in practice would use message queue\n            self.logger.info(\" Message History:\")\n            self.logger.info(\"-\" * 40)\n            # This would be enhanced to show actual message history\n            self.logger.info(\"Message history functionality available\")\n        except Exception as e:",
            "        try:\n            from .agent_mode_manager import get_active_agents\n            agents = get_active_agents()\n        except Exception:",
            "        try:\n            from .agent_mode_manager import get_active_agents, get_mode_manager\n            mode_manager = get_mode_manager()\n            current_mode = mode_manager.get_current_mode()\n            agents = get_active_agents()\n            self.logger.info(\n                f\" Available Agents (Mode: {current_mode}, {len(agents)} active):\")\n        except Exception:",
            "        try:\n            from .messaging_pyautogui import PyAutoGUIMessagingDelivery\n\n            if not self.delivery_service:\n                self.delivery_service = PyAutoGUIMessagingDelivery()\n        except ImportError:",
            "        try:\n            from .onboarding_service import OnboardingService\n\n            if not self.onboarding_service:\n                self.onboarding_service = OnboardingService()\n                self.logger.info(\" Onboarding service initialized\")\n        except ImportError as e:",
            "    try:\n        core = get_messaging_core()\n        if not core:\n            logger.error(\"Messaging core not available\")\n            return False\n\n        # Test basic functionality\n        test_message = UnifiedMessage(\n            content=\"System validation test\",\n            sender=\"SYSTEM\",\n            recipient=\"TEST_AGENT\",\n            message_type=UnifiedMessageType.TEXT,\n        )\n\n        logger.info(\" Messaging system validation passed\")\n        return True\n\n    except Exception as e:",
            "try:\n    initialize_messaging_system()\nexcept Exception as e:"
          ]
        },
        {
          "file": "src\\core\\optimized_stall_resume_prompt.py",
          "count": 5,
          "lines": [
            "            try:\n                from src.core.stall_resumer_guard import should_send_resume\n                should_send, reason = should_send_resume(agent_id, lookback_minutes=60)\n                \n                if not should_send:\n                    logger.info(\n                        f\" Skipping resume prompt for {agent_id}: {reason}\"\n                    )\n                    return None\n            except Exception as e:",
            "        try:\n            content = activation_file.read_text(encoding='utf-8')\n\n            # Extract agent-specific section\n            agent_section_pattern = rf\"### \\*\\*{re.escape(agent_id)}:.*?\\*\\*.*?\\n(.*?)(?=\\n---|\\n### \\*\\*|$)\"\n            match = re.search(agent_section_pattern, content,\n                              re.DOTALL | re.IGNORECASE)\n\n            if not match:\n                return {}\n\n            agent_section = match.group(1)\n\n            # Extract mission and tasks\n            assignments = {\n                \"mission\": \"\",\n                \"tasks\": []\n            }\n\n            # Extract mission\n            mission_match = re.search(\n                r'\\*\\*Mission\\*\\*:\\s*(.+?)(?:\\n|$)', agent_section, re.IGNORECASE)\n            if mission_match:\n                assignments[\"mission\"] = mission_match.group(1).strip()\n\n            # Extract tasks (numbered list items)\n            task_pattern = r'\\d+\\.\\s+\\*\\*(?:URGENT|HIGH|MEDIUM|CRITICAL)\\*\\*:\\s*(.+?)(?=\\n\\d+\\.|\\n\\*\\*|$)'\n            tasks = re.findall(task_pattern, agent_section,\n                               re.DOTALL | re.IGNORECASE)\n            assignments[\"tasks\"] = [task.strip()\n                                    for task in tasks[:3]]  # Top 3 tasks\n\n            return assignments\n        except Exception as e:",
            "        try:\n            from src.core.resume_cycle_planner_integration import ResumeCyclePlannerIntegration\n            self.resume_planner = ResumeCyclePlannerIntegration()\n        except ImportError:",
            "        try:\n            with open(cycle_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                tasks = data.get(\"pending_tasks\", data.get(\"tasks\", []))\n\n                # Find first pending task\n                for task in tasks:\n                    status = task.get(\"status\", \"\").lower()\n                    if status in [\"pending\", \"ready\"]:\n                        return task\n\n                return tasks[0] if tasks else None\n        except Exception as e:",
            "        try:\n            with open(status_file, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\messaging_coordinate_routing.py",
          "count": 2,
          "lines": [
            "        try:\n            # Get expected coordinates for this agent\n            expected_chat_coords = self.coord_loader.get_chat_coordinates(agent_id)\n            expected_onboarding_coords = self.coord_loader.get_onboarding_coordinates(agent_id)\n            \n            # Check if coords match expected (within tolerance for screen variations)\n            coords_match = (\n                coords == expected_chat_coords or \n                coords == expected_onboarding_coords or\n                (abs(x - expected_chat_coords[0]) <= 5 and abs(y - expected_chat_coords[1]) <= 5) or\n                (abs(x - expected_onboarding_coords[0]) <= 5 and abs(y - expected_onboarding_coords[1]) <= 5)\n            )\n            \n            if not coords_match:\n                logger.warning(\n                    f\" Coordinate mismatch for {agent_id}: got {coords}, \"\n                    f\"expected chat={expected_chat_coords} or onboarding={expected_onboarding_coords}\"\n                )\n            \n            # Legacy coarse bounds (kept as a final backstop)\n            if x < -5000 or x > 5000:\n                logger.error(f\" X coordinate out of coarse bounds for {agent_id}: {x} (expected -5000..5000)\")\n                return False\n            if y < -2000 or y > 5000:\n                logger.error(f\" Y coordinate out of coarse bounds for {agent_id}: {y} (expected -2000..5000)\")\n                return False\n            \n            logger.debug(f\" Coordinates validated for {agent_id}: {coords}\")\n            return True\n            \n        except Exception as e:",
            "        try:\n            from .utilities.validation_utilities import get_virtual_screen_bounds\n            min_x, min_y, max_x, max_y = get_virtual_screen_bounds()\n            if x < min_x or x > max_x:\n                logger.error(\n                    f\" X coordinate out of virtual screen bounds for {agent_id}: {x} \"\n                    f\"(bounds: {min_x}..{max_x}). Re-capture coords in cursor_agent_coords.json.\"\n                )\n                return False\n            if y < min_y or y > max_y:\n                logger.error(\n                    f\" Y coordinate out of virtual screen bounds for {agent_id}: {y} \"\n                    f\"(bounds: {min_y}..{max_y}). Re-capture coords in cursor_agent_coords.json.\"\n                )\n                return False\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\messaging_formatting.py",
          "count": 5,
          "lines": [
            "                try:\n                    # Import the template application function\n                    from src.services.messaging.message_formatters import _apply_template\n                    from src.core.messaging_models_core import MessageCategory\n                    import uuid\n                    from datetime import datetime\n\n                    # Convert category string to enum\n                    template_category = None\n                    if category_value == \"a2a\":\n                        template_category = MessageCategory.A2A\n                    elif category_value == \"d2a\":\n                        template_category = MessageCategory.D2A\n                    elif category_value == \"c2a\":\n                        template_category = MessageCategory.C2A\n                    elif category_value == \"s2a\":\n                        template_category = MessageCategory.S2A\n\n                    if template_category:\n                        # Apply the template\n                        msg_content = _apply_template(\n                            category=template_category,\n                            message=message.content,\n                            sender=sender,\n                            recipient=message.recipient,\n                            priority=message.priority.value,\n                            message_id=str(uuid.uuid4()),\n                            extra={\n                                \"ask\": message.content,\n                                \"context\": \"\",\n                                \"coordination_rationale\": \"To leverage parallel processing and accelerate completion\",\n                                \"expected_contribution\": \"Domain expertise and parallel execution\",\n                                \"coordination_timeline\": \"ASAP - coordination needed to maintain momentum\",\n                            }\n                        )\n                        logger.info(\n                            f\" Applied {category_value.upper()} template successfully \"\n                            f\"(original: {len(message.content)} chars, templated: {len(msg_content)} chars)\"\n                        )\n                    else:\n                        # Fallback to prefix formatting\n                        msg_content = format_c2a_message(\n                            recipient=message.recipient,\n                            content=message.content,\n                            priority=message.priority.value,\n                            sender=sender\n                        )\n                        logger.warning(f\" Unknown template category '{category_value}', using prefix formatting\")\n\n                except Exception as e:",
            "                try:\n                    category = MessageCategory(category_str.lower())\n                except (ValueError, AttributeError):",
            "                try:\n                    tags.append(UnifiedMessageTag(tag))\n                except (ValueError, AttributeError):",
            "            try:\n                message_type = UnifiedMessageType(message_type_str)\n            except (ValueError, AttributeError):",
            "            try:\n                priority = UnifiedMessagePriority(priority_str)\n            except (ValueError, AttributeError):"
          ]
        },
        {
          "file": "src\\core\\messaging_clipboard.py",
          "count": 3,
          "lines": [
            "            try:\n                clipboard_check = pyperclip.paste()\n                if clipboard_check == expected_content:\n                    logger.debug(\" Clipboard verified\")\n                    return True\n                else:\n                    logger.warning(\n                        f\" Clipboard mismatch: expected {len(expected_content)} chars, \"\n                        f\"got {len(clipboard_check)} chars\"\n                    )\n                    return False\n            except Exception as e:",
            "            try:\n                logger.info(f\" Copying message to clipboard: {len(content)} chars (preview: {content[:50]}...)\")\n                pyperclip.copy(content)\n                time.sleep(0.5)  # Wait for clipboard to be ready\n                logger.info(\" Content copied to clipboard successfully\")\n                return True\n            except Exception as e:",
            "try:\n    import pyperclip\n    PYPERCLIP_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\messaging_validation.py",
          "count": 2,
          "lines": [
            "        try:\n            from ..core.multi_agent_request_validator import get_multi_agent_validator\n            \n            validator = get_multi_agent_validator()\n            can_send, error_message, pending_info = validator.validate_agent_can_send_message(\n                agent_id=recipient,\n                target_recipient=sender,  # Allow if responding to request sender\n                message_content=content\n            )\n            \n            if not can_send:\n                # Recipient has pending request - block and show error\n                logger.warning(\n                    f\" Message blocked - {recipient} has pending multi-agent request\"\n                )\n                # Store error in metadata for caller to access\n                if metadata is None:\n                    metadata = {}\n                metadata[\"blocked\"] = True\n                metadata[\"blocked_reason\"] = \"pending_multi_agent_request\"\n                metadata[\"blocked_error_message\"] = error_message\n                return False, metadata\n            \n            # If responding to request sender, auto-route to collector\n            if pending_info and sender == pending_info[\"sender\"]:\n                self._auto_route_response(pending_info, recipient, content)\n            \n            return True, metadata\n            \n        except ImportError:",
            "        try:\n            from ..core.multi_agent_responder import get_multi_agent_responder\n            responder = get_multi_agent_responder()\n            \n            # Auto-submit response to collector\n            collector_id = pending_info[\"collector_id\"]\n            responder.submit_response(collector_id, recipient, content)\n            \n            logger.info(\n                f\" Auto-routed response from {recipient} to collector {collector_id}\"\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\messaging_template_resolution.py",
          "count": 1,
          "lines": [
            "        try:\n            from ..services.messaging.policy_loader import (\n                load_template_policy,\n                resolve_template_by_channel,\n                resolve_template_by_roles,\n            )\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\messaging_history.py",
          "count": 3,
          "lines": [
            "        try:\n            # Serialize metadata to ensure JSON compatibility\n            metadata_serialized = self._serialize_metadata(\n                message.metadata\n            ) if message.metadata else {}\n            \n            message_dict = {\n                \"from\": message.sender,\n                \"to\": message.recipient,\n                \"content\": (\n                    message.content[:200] + \"...\"\n                    if len(message.content) > 200\n                    else message.content\n                ),\n                \"content_length\": len(message.content),\n                \"message_type\": (\n                    message.message_type.value\n                    if hasattr(message.message_type, \"value\")\n                    else str(message.message_type)\n                ),\n                \"priority\": (\n                    message.priority.value\n                    if hasattr(message.priority, \"value\")\n                    else str(message.priority)\n                ),\n                \"tags\": [\n                    tag.value if hasattr(tag, \"value\") else str(tag)\n                    for tag in message.tags\n                ],\n                \"metadata\": metadata_serialized,\n                \"timestamp\": format_swarm_timestamp(),\n                \"status\": status,\n            }\n            \n            self.message_repository.save_message(message_dict)\n            logger.debug(\n                f\" Message logged to history: {message.sender}  {message.recipient}\"\n            )\n            return True\n            \n        except Exception as e:",
            "        try:\n            history_entry = {\n                \"from\": message.sender,\n                \"to\": message.recipient,\n                \"content\": message.content[:500],\n                \"timestamp\": message.timestamp,\n                \"status\": \"FAILED\",\n                \"error\": str(error)[:200],\n            }\n            self.message_repository.save_message(history_entry)\n            return True\n        except Exception:",
            "        try:\n            metadata_serialized = self._serialize_metadata(\n                message.metadata\n            ) if message.metadata else {}\n            \n            message_dict = {\n                \"from\": message.sender,\n                \"to\": message.recipient,\n                \"content\": (\n                    message.content[:200] + \"...\"\n                    if len(message.content) > 200\n                    else message.content\n                ),\n                \"content_length\": len(message.content),\n                \"message_type\": (\n                    message.message_type.value\n                    if hasattr(message.message_type, \"value\")\n                    else str(message.message_type)\n                ),\n                \"priority\": (\n                    message.priority.value\n                    if hasattr(message.priority, \"value\")\n                    else str(message.priority)\n                ),\n                \"tags\": [\n                    tag.value if hasattr(tag, \"value\") else str(tag)\n                    for tag in message.tags\n                ],\n                \"metadata\": metadata_serialized,\n                \"timestamp\": format_swarm_timestamp(),\n                \"status\": status,\n            }\n            \n            self.message_repository.save_message(message_dict)\n            logger.debug(\n                f\" Delivery status logged: {message.sender}  {message.recipient}\"\n            )\n            return True\n            \n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\messaging_delivery_orchestration.py",
          "count": 1,
          "lines": [
            "        try:\n            from .message_queue import MessageQueue\n            queue = MessageQueue()\n            queue_id = queue.enqueue(message)\n            \n            logger.info(\n                f\" Message queued for later processing: {queue_id} \"\n                f\"({message.sender}  {message.recipient})\"\n            )\n            return True\n            \n        except Exception as queue_error:"
          ]
        },
        {
          "file": "src\\core\\message_queue_persistence.py",
          "count": 12,
          "lines": [
            "                            try:\n                                temp_file.unlink()\n                            except Exception:",
            "                        try:\n                            obj_data = json.loads(current_obj)\n                            entry = QueueEntry.from_dict(obj_data)\n                            entries.append(entry)\n                        except Exception:",
            "                        try:\n                            temp_file.unlink()\n                        except Exception:",
            "                    try:\n                        temp_file.unlink()\n                    except Exception:",
            "                try:\n                    temp_file.unlink()\n                except Exception:",
            "            try:\n                # Try to remove existing file if it exists\n                if self.queue_file.exists():\n                    try:\n                        self.queue_file.unlink()\n                    except PermissionError:",
            "        try:\n            # CRITICAL FIX: Ensure temp file directory exists before writing\n            temp_file.parent.mkdir(parents=True, exist_ok=True)\n            with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, separators=(',', ':'),\n                          ensure_ascii=False, default=str)\n            # CRITICAL FIX: Verify temp file was actually written before proceeding\n            if not temp_file.exists() or temp_file.stat().st_size == 0:\n                raise IOError(\n                    f\"Temp file was not created or is empty: {temp_file}\")\n        except Exception as e:",
            "        try:\n            # Read raw file content\n            raw_content = self.queue_file.read_text(encoding=\"utf-8\").strip()\n\n            if not raw_content:\n                # Empty file - return empty list\n                return []\n\n            # Try to parse as standard JSON array\n            try:\n                data = json.loads(raw_content)\n\n                # Validate structure\n                if not isinstance(data, list):\n                    # If it's a dict, try to extract entries\n                    if isinstance(data, dict) and \"entries\" in data:\n                        data = data[\"entries\"]\n                    elif isinstance(data, dict) and \"pending_pushes\" in data:\n                        # Handle deferred push queue format\n                        data = data[\"pending_pushes\"]\n                    else:\n                        # Invalid structure - backup and reset\n                        self._backup_corrupted_file()\n                        return []\n\n                # Parse entries with error isolation\n                entries = []\n                for idx, entry_data in enumerate(data):\n                    try:\n                        entry = QueueEntry.from_dict(entry_data)\n                        entries.append(entry)\n                    except (KeyError, ValueError, TypeError) as entry_error:",
            "        try:\n            # Strategy 1: Try to find array start\n            if raw_content.startswith('['):\n                # Try to parse up to the error position\n                error_pos = json_error.pos if hasattr(\n                    json_error, 'pos') else len(raw_content)\n                partial_content = raw_content[:error_pos]\n\n                # Try to find last valid array closing\n                last_bracket = partial_content.rfind(']')\n                if last_bracket > 0:\n                    try:\n                        data = json.loads(partial_content[:last_bracket + 1])\n                        if isinstance(data, list):\n                            entries = []\n                            for entry_data in data:\n                                try:\n                                    entries.append(\n                                        QueueEntry.from_dict(entry_data))\n                                except Exception:",
            "        try:\n            from tools.file_locking_monitor import FileLockingMonitor\n            monitor = FileLockingMonitor()\n        except (ImportError, Exception):",
            "        try:\n            if self.queue_file.exists():\n                from datetime import datetime\n                import shutil\n\n                # Create backup directory if it doesn't exist\n                backup_dir = self.queue_file.parent / \"backups\"\n                backup_dir.mkdir(exist_ok=True)\n\n                # Generate unique backup filename with timestamp\n                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                backup_name = f\"{self.queue_file.stem}_{timestamp}_corrupted.json\"\n                backup_path = backup_dir / backup_name\n\n                # Use copy instead of rename to preserve original for analysis\n                shutil.copy2(self.queue_file, backup_path)\n\n                # Only remove original after successful backup\n                self.queue_file.unlink()\n\n                print(f\" Backed up corrupted queue file to: {backup_path}\")\n        except PermissionError as e:"
          ]
        },
        {
          "file": "src\\core\\messaging_pyautogui.py",
          "count": 4,
          "lines": [
            "            try:\n                success = self._send_message_attempt(message, attempt + 1)\n                if success:\n                    return True\n                \n                # Wait before retry\n                if attempt < 2:\n                    logger.warning(f\" Retry {attempt + 1}/3 for {recipient}\")\n                    time.sleep(1.0)\n            \n            except Exception as e:",
            "        try:\n            coords = self.coordinate_service.get_coordinates_for_message(message, sender)\n            if not coords:\n                logger.error(f\"No coordinates for {message.recipient}\")\n                return False\n\n            x, y = coords\n            msg_content = self.formatting_service.format_message_content(message, sender)\n\n            # Execute PyAutoGUI operations\n            if not self.operations_service.move_to_coordinates(\n                (x, y), message.recipient, validate_callback=self.validate_coordinates\n            ):\n                return False\n            \n            self.operations_service.focus_input_field(message.recipient, (x, y))\n            self.operations_service.clear_input_field(message.recipient)\n            self.operations_service.verify_coordinates_before_paste(message.recipient, (x, y))\n            \n            # Copy and paste\n            if not self.clipboard_service.copy_to_clipboard(msg_content):\n                return False\n                \n            if not self.operations_service.paste_content(message.recipient, (x, y), self.clipboard_service):\n                return False\n            \n            # Send and verify\n            message_metadata = message.metadata if isinstance(message.metadata, dict) else {}\n            if not self.operations_service.send_message(message.recipient, message_metadata):\n                return False\n            \n            self.operations_service.verify_send_completion(message.recipient, (x, y))\n            logger.info(f\" Sent to {message.recipient} at {coords} (attempt {attempt_num})\")\n            return True\n\n        except Exception as e:",
            "    try:\n        delivery = PyAutoGUIMessagingDelivery()\n        from .messaging_core import UnifiedMessage, UnifiedMessagePriority, UnifiedMessageType\n\n        msg = UnifiedMessage(\n            content=message,\n            sender=\"CAPTAIN\",\n            recipient=agent_id,\n            message_type=UnifiedMessageType.CAPTAIN_TO_AGENT,\n            priority=UnifiedMessagePriority.URGENT,\n            tags=[],\n            metadata={},\n        )\n        return delivery.send_message(msg)\n    except Exception as e:",
            "try:\n    import pyautogui\n    import pyperclip\n\n    PYAUTOGUI_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\message_queue_registry.py",
          "count": 1,
          "lines": [
            "    try:\n        # Import and register core components using sys.path manipulation\n        import sys\n        import os\n\n        # Temporarily modify sys.path to ensure we can import the module directly\n        project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\n        if project_root not in sys.path:\n            sys.path.insert(0, project_root)\n\n        # Import from the message_queue package (which imports from impl)\n        from src.core.message_queue import MessageQueue, QueueConfig, AsyncQueueProcessor, IMessageQueue\n\n        register_component('MessageQueue', MessageQueue)\n        register_component('QueueConfig', QueueConfig)\n        register_component('AsyncQueueProcessor', AsyncQueueProcessor)\n        register_component('IMessageQueue', IMessageQueue)\n\n        logger.info(\"Message queue registry initialized successfully\")\n\n    except ImportError as e:"
          ]
        },
        {
          "file": "src\\core\\health_check.py",
          "count": 5,
          "lines": [
            "    try:\n        # Check core services\n        if include_services:\n            health_data.update(_check_services())\n\n        # Add performance metrics if requested\n        if include_metrics:\n            health_data[\"metrics\"] = _get_performance_metrics()\n\n        # Determine overall status based on service health\n        if include_services:\n            service_statuses = [\n                health_data.get(\"fastapi_status\", \"unknown\"),\n                health_data.get(\"analytics_status\", \"unknown\"),\n                health_data.get(\"database_status\", \"unknown\")\n            ]\n\n            # If any critical service is unhealthy, overall status is unhealthy\n            if any(status in [\"unhealthy\", \"unavailable\", \"error\"] for status in service_statuses):\n                health_data[\"overall_status\"] = \"unhealthy\"\n                health_data[\"status\"] = \"degraded\"\n            else:\n                health_data[\"overall_status\"] = \"healthy\"\n                health_data[\"status\"] = \"healthy\"\n        else:\n            # Basic health check without services\n            health_data[\"overall_status\"] = \"healthy\"\n            health_data[\"status\"] = \"healthy\"\n\n    except Exception as e:",
            "    try:\n        # Check if message queue is accessible\n        import os\n        if os.path.exists(\"agent_workspaces\"):\n            services[\"message_queue_status\"] = \"healthy\"\n        else:\n            services[\"message_queue_status\"] = \"unavailable\"\n    except Exception as e:",
            "    try:\n        from tools.infrastructure_tools import UnifiedInfrastructureManager\n        infra_mgr = UnifiedInfrastructureManager()\n        analytics_svc = infra_mgr.get_service(\"analytics\")\n        services[\"analytics_status\"] = \"healthy\" if analytics_svc else \"unavailable\"\n    except ImportError:",
            "    try:\n        import os\n        # Simple check for database availability\n        if os.path.exists(\"database/__init__.py\"):\n            services[\"database_status\"] = \"healthy\"\n        else:\n            services[\"database_status\"] = \"unavailable\"\n    except Exception as e:",
            "    try:\n        import psutil\n        import os\n\n        # Basic system metrics\n        metrics = {\n            \"cpu_percent\": psutil.cpu_percent(interval=0.1),\n            \"memory_percent\": psutil.virtual_memory().percent,\n            \"disk_usage\": psutil.disk_usage('/').percent,\n            \"process_count\": len(psutil.pids()),\n            \"uptime\": time.time() - psutil.boot_time()\n        }\n\n        # Application-specific metrics\n        pid = os.getpid()\n        try:\n            process = psutil.Process(pid)\n            metrics.update({\n                \"app_cpu_percent\": process.cpu_percent(interval=0.1),\n                \"app_memory_mb\": process.memory_info().rss / (1024 * 1024),\n                \"app_threads\": process.num_threads()\n            })\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\unified_service_base.py",
          "count": 12,
          "lines": [
            "                try:\n                    config[config_key] = json.loads(value)\n                except (json.JSONDecodeError, TypeError):",
            "                try:\n                    if config_path.suffix == '.json':\n                        with open(config_path, 'r') as f:\n                            file_config = json.load(f)\n                    elif config_path.suffix == '.yaml':\n                        import yaml\n                        with open(config_path, 'r') as f:\n                            file_config = yaml.safe_load(f)\n                    else:\n                        continue\n\n                    config.update(file_config)\n                    break  # Use first config file found\n\n                except Exception as e:",
            "            try:\n                status[\"resources\"] = {\n                    \"cpu_percent\": self._process.cpu_percent(),\n                    \"memory_mb\": self._process.memory_info().rss / 1024 / 1024,\n                    \"threads\": self._process.num_threads(),\n                }\n            except (psutil.AccessDenied, psutil.NoSuchProcess):",
            "        try:\n            # Call service-specific health check\n            if hasattr(self.service, 'check_health'):\n                health_result = self.service.check_health()\n                if isinstance(health_result, dict):\n                    return health_result\n                elif isinstance(health_result, bool):\n                    return {\"healthy\": health_result, \"status\": \"healthy\" if health_result else \"unhealthy\"}\n                else:\n                    return {\"healthy\": True, \"status\": \"unknown\"}\n            else:\n                # Basic health check - service is running\n                return {\"healthy\": self._running, \"status\": \"running\" if self._running else \"stopped\"}\n\n        except Exception as e:",
            "        try:\n            # Try unified logging system first\n            return UnifiedLoggingSystem.get_logger(self.service_name)\n        except Exception:",
            "        try:\n            if mode == \"background\":\n                return self._start_background()\n            else:\n                return self._start_foreground()\n        except Exception as e:",
            "        try:\n            old_config = self.config.copy()\n            self.config = UnifiedServiceConfig.load_config(\n                self.service_name, self.config_schema\n            )\n\n            # Log configuration changes\n            if old_config != self.config:\n                self.logger.info(\" Configuration reloaded\")\n                return True\n            else:\n                self.logger.info(\" Configuration unchanged\")\n                return False\n\n        except Exception as e:",
            "        try:\n            self._process = psutil.Process()\n            self.service.on_start()\n            self._thread = threading.Thread(target=self._run_service_loop, daemon=True)\n            self._thread.start()\n            self.service.logger.info(\"Service started in foreground mode\")\n            return True\n        except Exception as e:",
            "        try:\n            self._running = False\n\n            if self._thread and self._thread.is_alive():\n                self._thread.join(timeout=10)\n\n            if self._process:\n                try:\n                    self._process.terminate()\n                    self._process.wait(timeout=5)\n                except psutil.TimeoutExpired:",
            "        try:\n            self.cleanup_resources()\n        except Exception:",
            "        try:\n            self.stop()\n            time.sleep(2)  # Brief pause before restart\n            self.start()\n        except Exception as e:",
            "        try:\n            while self._running:\n                try:\n                    self.service.run_once()\n                except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\message_queue_impl.py",
          "count": 4,
          "lines": [
            "            if entry:\n                self._update_entry_status(entry, \"DELIVERED\")\n                self.persistence.save_entries(entries)\n                self._log_info(f\"Message marked as delivered: {queue_id}\")\n                return True\n\n            self._log_warning(f\"Queue entry not found: {queue_id}\")\n            return False\n\n        return self.persistence.atomic_operation(_mark_delivered_operation)\n\n    def mark_failed(self, queue_id: str, error: str) -> bool:\n        \"\"\"Mark message as failed and schedule retry.\"\"\"\n\n        def _mark_failed_operation():\n            entries = self.persistence.load_entries()\n            entry = self._find_entry_by_id(entries, queue_id)\n\n            if entry:\n                self._update_entry_status(entry, \"FAILED\", error)\n                self.persistence.save_entries(entries)\n                self._log_warning(\n                    f\"Message marked as failed: {queue_id} - {error}\")\n                return True\n\n            self._log_warning(f\"Queue entry not found: {queue_id}\")\n            return False\n\n        return self.persistence.atomic_operation(_mark_failed_operation)\n    \n    def _reset_entry_for_retry(self, queue_id: str, attempts: int, delay: float) -> bool:\n        \"\"\"Reset entry to PENDING status for retry with backoff delay.\n        \n        Args:\n            queue_id: Queue entry ID\n            attempts: Current attempt count\n            delay: Delay in seconds before next retry\n            \n        Returns:\n            True if reset successful, False otherwise\n        \"\"\"\n        from datetime import datetime, timedelta\n        \n        def _reset_operation():\n            entries = self.persistence.load_entries()\n            entry = self._find_entry_by_id(entries, queue_id)\n            \n            if entry:\n                entry.status = \"PENDING\"\n                entry.updated_at = datetime.now()\n                \n                # Store retry metadata\n                if not hasattr(entry, 'metadata'):\n                    entry.metadata = {}\n                entry.metadata['delivery_attempts'] = attempts\n                entry.metadata['last_retry_time'] = datetime.now().isoformat()\n                entry.metadata['next_retry_delay'] = delay\n                \n                # Update priority score to schedule retry after delay\n                # Higher priority for urgent retries, but still respect delay\n                entry.priority_score = getattr(entry, 'priority_score', 0.5)\n                \n                self.persistence.save_entries(entries)\n                self._log_info(\n                    f\"Entry {queue_id} reset to PENDING for retry \"\n                    f\"(attempt {attempts}, delay {delay}s)\")\n                return True\n            \n            self._log_warning(f\"Queue entry not found for retry: {queue_id}\")\n            return False\n        \n        return self.persistence.atomic_operation(_reset_operation)\n    \n    def resend_failed_messages(self, max_messages: Optional[int] = None) -> int:\n        \"\"\"Resend failed messages that haven't exceeded max retries.\n        \n        Args:\n            max_messages: Maximum number of messages to resend (None = all)\n            \n        Returns:\n            Number of messages reset for retry\n        \"\"\"\n        from datetime import datetime, timedelta\n        \n        def _resend_operation():\n            entries = self.persistence.load_entries()\n            failed_entries = [\n                e for e in entries \n                if getattr(e, 'status', '') == 'FAILED'\n            ]\n            \n            if not failed_entries:\n                return 0\n            \n            # Filter entries that can be retried (haven't exceeded max retries)\n            max_retries = 3\n            resettable = []\n            \n            for entry in failed_entries:\n                attempts = getattr(entry, 'delivery_attempts', 0)\n                if attempts < max_retries:\n                    resettable.append(entry)\n            \n            if max_messages:\n                resettable = resettable[:max_messages]\n            \n            # Reset entries to PENDING\n            for entry in resettable:\n                entry.status = \"PENDING\"\n                entry.updated_at = datetime.now()\n                attempts = getattr(entry, 'delivery_attempts', 0)\n                \n                if not hasattr(entry, 'metadata'):\n                    entry.metadata = {}\n                entry.metadata['delivery_attempts'] = attempts\n                entry.metadata['resend_time'] = datetime.now().isoformat()\n            \n            if resettable:\n                self.persistence.save_entries(entries)\n                self._log_info(f\"Reset {len(resettable)} failed messages for retry\")\n            \n            return len(resettable)\n        \n        return self.persistence.atomic_operation(_resend_operation)\n\n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive queue statistics.\"\"\"\n\n        def _get_statistics_operation():\n            entries = self.persistence.load_entries()\n            return self.statistics_calculator.calculate_statistics(entries)\n\n        return self.persistence.atomic_operation(_get_statistics_operation)\n\n    def cleanup_expired(self) -> int:\n        \"\"\"Remove expired entries from queue.\"\"\"\n\n        def _cleanup_operation():\n            entries = self.persistence.load_entries()\n            original_count = len(entries)\n            active_entries = self._filter_expired_entries(entries)\n            expired_count = original_count - len(active_entries)\n\n            self.persistence.save_entries(active_entries)\n            if expired_count > 0:\n                self._log_info(f\"Cleaned up {expired_count} expired entries\")\n\n            return expired_count\n\n        return self.persistence.atomic_operation(_cleanup_operation)\n\n    def _filter_expired_entries(self, entries: List[IQueueEntry]) -> List[IQueueEntry]:\n        \"\"\"Filter out expired entries.\"\"\"\n        max_age_seconds = self.config.max_age_days * 24 * 60 * 60\n        now = datetime.now()\n        active_entries = []\n\n        for entry in entries:\n            if not hasattr(entry, 'created_at'):\n                active_entries.append(entry)\n                continue\n\n            age = (now - entry.created_at).total_seconds()\n            if age <= max_age_seconds:\n                active_entries.append(entry)\n\n        return active_entries\n\n    def get_health_status(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive queue health status.\"\"\"\n\n        def _get_health_operation():\n            entries = self.persistence.load_entries()\n            return self.health_monitor.assess_health(entries)\n\n        return self.persistence.atomic_operation(_get_health_operation)\n\n\nclass AsyncQueueProcessor(IQueueProcessor):\n    \"\"\"SOLID-compliant asynchronous queue processor.\n\n    Processes queued messages with retry logic and error handling.\n    Follows Single Responsibility Principle with focused processing logic.\n    \"\"\"\n\n    def __init__(\n        self,\n        queue: IMessageQueue,\n        delivery_callback: Callable[[Any], bool],\n        logger: Optional[IMessageQueueLogger] = None\n    ):\n        \"\"\"Initialize queue processor with dependency injection.\"\"\"\n        self.queue = queue\n        self.delivery_callback = delivery_callback\n        self.logger = logger\n        self.running = False\n        self.last_cleanup = 0.0\n\n    async def start_processing(self, interval: float = 5.0) -> None:\n        \"\"\"Start continuous queue processing.\"\"\"\n        self.running = True\n        if self.logger:\n            self.logger.info(\"Queue processor started\")\n\n        while self.running:\n            try:\n                await self.process_batch()\n                await self._cleanup_if_needed(interval)\n                await asyncio.sleep(interval)\n            except Exception as e:",
            "        try:\n            success = self.delivery_callback(message)\n            if success:\n                self.queue.mark_delivered(queue_id)\n            else:\n                self.queue.mark_failed(\n                    queue_id, \"Delivery callback returned False\")\n        except Exception as e:",
            "    ) -> QueueEntry:\n        \"\"\"Create new queue entry.\n        \n        FIXED: Normalize message format to dict to ensure consistent routing.\n        Handles both UnifiedMessage objects and dict messages.\n        \"\"\"\n        now = datetime.now()\n        \n        # FIXED: Normalize message to dict format for consistent routing\n        # Prevents routing issues when messages come from different sources\n        normalized_message = self._normalize_message(message)\n        \n        # Merge delivery callback with existing message metadata\n        message_metadata = normalized_message.get(\"metadata\", {})\n        merged_metadata = {**message_metadata, \"delivery_callback\": delivery_callback is not None}\n\n        return QueueEntry(\n            message=normalized_message,\n            queue_id=queue_id,\n            priority_score=priority_score,\n            status=\"PENDING\",\n            created_at=now,\n            updated_at=now,\n            metadata=merged_metadata,\n        )\n    \n    def _normalize_message(self, message: Any) -> dict:\n        \"\"\"Normalize message to dict format for consistent routing.\n        \n        Handles both UnifiedMessage objects and dict messages.\n        Ensures recipient is always extractable regardless of source.\n        \"\"\"\n        if isinstance(message, dict):\n            # Already a dict - ensure required fields exist\n            return message\n        \n        # UnifiedMessage object - convert to dict\n        from src.core.messaging_models_core import UnifiedMessage\n        if isinstance(message, UnifiedMessage):\n            message_dict = {\n                \"recipient\": message.recipient,\n                \"content\": message.content,\n                \"sender\": message.sender,\n                \"message_type\": getattr(message.message_type, \"value\", None) or str(message.message_type),\n                \"priority\": getattr(message.priority, \"value\", None) or str(message.priority),\n                \"tags\": [getattr(tag, \"value\", None) or str(tag) for tag in message.tags],\n                \"metadata\": message.metadata or {},\n            }\n            return message_dict\n        \n        # Fallback: try to extract as object attributes\n        return {\n            \"recipient\": getattr(message, \"recipient\", None) or getattr(message, \"to\", None),\n            \"content\": getattr(message, \"content\", None) or getattr(message, \"message\", None),\n            \"sender\": getattr(message, \"sender\", None) or getattr(message, \"from\", \"SYSTEM\"),\n            \"message_type\": getattr(message, \"message_type\", \"text\"),\n            \"priority\": getattr(message, \"priority\", \"normal\"),\n            \"tags\": getattr(message, \"tags\", []),\n            \"metadata\": getattr(message, \"metadata\", {}),\n        }\n\n    def _validate_queue_size(self, entries: List[IQueueEntry]) -> None:\n        \"\"\"Validate queue size limit.\"\"\"\n        if len(entries) >= self.config.max_queue_size:\n            raise RuntimeError(\n                f\"Queue size limit exceeded: {self.config.max_queue_size}\")\n\n    def _calculate_priority_score(self, message: Any, now: datetime) -> float:\n        \"\"\"Calculate priority score for message.\n        \n        FIXED: Handles both UnifiedMessage objects and normalized dict messages.\n        \"\"\"\n        # Check if message is dict (normalized format)\n        if isinstance(message, dict):\n            priority = message.get(\"priority\", \"regular\")\n            if isinstance(priority, str):\n                # Map priority strings to scores\n                priority_map = {\"urgent\": 1.0, \"high\": 0.8, \"normal\": 0.5, \"regular\": 0.5, \"low\": 0.3}\n                return priority_map.get(priority.lower(), 0.5)\n            elif isinstance(priority, (int, float)):\n                return float(priority)\n        \n        # UnifiedMessage object format\n        if hasattr(message, 'priority'):\n            if hasattr(message.priority, 'value'):\n                priority_value = message.priority.value\n                if isinstance(priority_value, str):\n                    priority_map = {\"urgent\": 1.0, \"high\": 0.8, \"normal\": 0.5, \"regular\": 0.5, \"low\": 0.3}\n                    return priority_map.get(priority_value.lower(), 0.5)\n                return float(priority_value)\n            elif isinstance(message.priority, (int, float)):\n                return float(message.priority)\n\n        # Default priority\n        return 0.5\n\n    def dequeue(self, batch_size: Optional[int] = None) -> List[IQueueEntry]:\n        \"\"\"Get next messages for processing based on priority.\n\n        Args:\n            batch_size: Number of messages to retrieve (defaults to config)\n\n        Returns:\n            List of queue entries ready for processing\n        \"\"\"\n        batch_size = batch_size or self.config.processing_batch_size\n\n        def _dequeue_operation():\n            entries = self.persistence.load_entries()\n            pending_entries = self._get_pending_entries(entries)\n\n            if not pending_entries:\n                return []\n\n            entries_to_process = self._select_top_priority_entries(\n                pending_entries, batch_size)\n            self._mark_entries_processing(entries_to_process)\n            self.persistence.save_entries(entries)\n            self._log_info(\n                f\"Dequeued {len(entries_to_process)} messages for processing\")\n            return entries_to_process\n\n        return self.persistence.atomic_operation(_dequeue_operation)\n\n    def _get_pending_entries(self, entries: List[IQueueEntry]) -> List[tuple]:\n        \"\"\"Get pending entries with priority scores for heap.\n        \n        Respects retry delays - entries with next_retry_delay won't be\n        processed until delay has passed.\n        \"\"\"\n        import heapq\n        now = datetime.now()\n        \n        pending = []\n        \n        for i, e in enumerate(entries):\n            if getattr(e, 'status', '') != 'PENDING':\n                continue\n            \n            # Check if retry delay has passed\n            metadata = getattr(e, 'metadata', {})\n            next_retry_delay = metadata.get('next_retry_delay')\n            last_retry_time = metadata.get('last_retry_time')\n            \n            if next_retry_delay and last_retry_time:\n                try:\n                    # Parse last_retry_time (handle both ISO format and datetime)\n                    if isinstance(last_retry_time, str):\n                        last_retry = datetime.fromisoformat(last_retry_time.replace('Z', '+00:00'))\n                    else:\n                        last_retry = last_retry_time\n                    \n                    # Handle timezone-aware vs naive\n                    if hasattr(last_retry, 'tzinfo') and last_retry.tzinfo is not None:\n                        # Timezone-aware - convert to naive for comparison\n                        last_retry = last_retry.replace(tzinfo=None)\n                    \n                    elapsed = (now - last_retry).total_seconds()\n                    if elapsed < next_retry_delay:\n                        # Delay hasn't passed yet, skip this entry\n                        continue\n                except Exception:",
            "try:\n    # Try relative imports first (when imported as part of package)\n    from src.core.message_queue_interfaces import (\n        IMessageQueue,\n        IQueuePersistence,\n        IQueueProcessor,\n        IMessageQueueLogger,\n        IQueueEntry\n    )\n    from src.core.message_queue_persistence import FileQueuePersistence, QueueEntry\n    from src.core.message_queue_statistics import QueueStatisticsCalculator, QueueHealthMonitor\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\messaging_pyautogui_operations.py",
          "count": 3,
          "lines": [
            "        try:\n            # Select all\n            self.pyautogui.hotkey(\"ctrl\", \"a\")\n            time.sleep(0.2)\n            # Delete selected text\n            self.pyautogui.press(\"delete\")\n            time.sleep(0.2)\n            # Click again to ensure focus is maintained\n            self.pyautogui.click()\n            time.sleep(0.2)\n            logger.debug(f\" Input field cleared for {recipient}\")\n            return True\n        except Exception as e:",
            "        try:\n            self.pyautogui.hotkey(\"ctrl\", \"v\")\n            time.sleep(0.5)\n            return True\n        except Exception as e:",
            "try:\n    import pyautogui\n    PYAUTOGUI_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\project_scanner_integration.py",
          "count": 10,
          "lines": [
            "        try:\n            # For now, create Thea-style guidance based on scan results\n            # This is a synchronous implementation that can be enhanced with actual Thea integration\n            guidance = self._generate_thea_style_guidance(scan_results)\n            return guidance\n\n        except Exception as e:",
            "        try:\n            # For now, create a mock Thea response\n            # In production, this would send to Thea via messaging system\n\n            guidance = {\n                \"status\": \"mock_guidance\",\n                \"timestamp\": datetime.now().isoformat(),\n                \"priority_tasks\": [\n                    \"Review high-complexity files for refactoring opportunities\",\n                    \"Implement comprehensive test coverage for critical functions\",\n                    \"Add API documentation for discovered routes\",\n                    \"Consider modular architecture improvements\",\n                    \"Set up automated code quality checks\"\n                ],\n                \"architectural_notes\": [\n                    \"Project has good separation of concerns\",\n                    \"Consider adding type hints for better maintainability\",\n                    \"API routes suggest microservice potential\",\n                    \"File complexity indicates need for smaller modules\"\n                ],\n                \"development_recommendations\": [\n                    \"Implement automated testing pipeline\",\n                    \"Add code quality gates (linting, formatting)\",\n                    \"Consider containerization for deployment\",\n                    \"Set up monitoring and logging infrastructure\"\n                ]\n            }\n\n            logger.info(\" Getting Thea guidance from project scan\")\n\n            # Implement actual Thea integration\n            thea_guidance = self._get_thea_guidance_sync(scan_results)\n\n            # Merge Thea guidance with fallback recommendations\n            if thea_guidance and \"recommendations\" in thea_guidance:\n                guidance[\"thea_recommendations\"] = thea_guidance[\"recommendations\"]\n                guidance[\"guidance_source\"] = \"thea_integrated\"\n                logger.info(\" Integrated Thea guidance with project scan results\")\n            else:\n                guidance[\"guidance_source\"] = \"fallback_generated\"\n                logger.warning(\" Thea guidance unavailable, using fallback recommendations\")\n\n            return guidance\n\n        except Exception as e:",
            "        try:\n            # Format scan summary for Thea\n            thea_prompt = self._format_thea_prompt(scan_results, project_path)\n\n            # Send to Thea (via messaging system)\n            guidance = self._get_thea_guidance(thea_prompt)\n\n            # Enhance results with guidance\n            scan_results[\"thea_guidance\"] = guidance\n            scan_results[\"guidance_timestamp\"] = datetime.now().isoformat()\n\n            logger.info(\" Scan results sent to Thea for guidance\")\n\n            return scan_results\n\n        except Exception as e:",
            "        try:\n            # Import Thea client if available\n            try:\n                from src.services.thea_client import TheaClient\n            except ImportError:",
            "        try:\n            # Try the main project scanner first\n            scanner_path = self.project_root / \"temp_repos\" / \"Auto_Blogger\" / \"project_scanner.py\"\n            if scanner_path.exists():\n                import sys\n                sys.path.insert(0, str(scanner_path.parent))\n\n                from project_scanner import ProjectScanner\n                logger.info(\" Project scanner imported successfully\")\n                return ProjectScanner\n            else:\n                logger.warning(\" Project scanner not found in temp_repos\")\n                return None\n\n        except ImportError as e:",
            "        try:\n            cache_file = self.scan_results_dir / f\"{project_path.name}_scan_cache.json\"\n\n            if not cache_file.exists():\n                return None\n\n            with open(cache_file, 'r', encoding='utf-8') as f:\n                cached_data = json.load(f)\n\n            # Check if cache is still valid (within 24 hours)\n            cache_time = datetime.fromisoformat(cached_data.get(\"scan_metadata\", {}).get(\"scan_timestamp\", \"\"))\n            cache_age = datetime.now() - cache_time\n\n            if cache_age.total_seconds() > 86400:  # 24 hours\n                logger.info(\"Cache expired, performing fresh scan\")\n                return None\n\n            logger.info(\"Using cached scan results\")\n            return cached_data\n\n        except Exception as e:",
            "        try:\n            cache_file = self.scan_results_dir / f\"{project_path.name}_scan_cache.json\"\n\n            with open(cache_file, 'w', encoding='utf-8') as f:\n                json.dump(results, f, indent=2, ensure_ascii=False)\n\n            logger.debug(f\"Cached scan results: {cache_file}\")\n\n        except Exception as e:",
            "        try:\n            history = {}\n            for cache_file in self.scan_results_dir.glob(\"*_scan_cache.json\"):\n                try:\n                    with open(cache_file, 'r', encoding='utf-8') as f:\n                        data = json.load(f)\n\n                    project_name = cache_file.stem.replace(\"_scan_cache\", \"\")\n                    history[project_name] = {\n                        \"last_scan\": data.get(\"scan_metadata\", {}).get(\"scan_timestamp\"),\n                        \"scan_duration\": data.get(\"scan_metadata\", {}).get(\"scan_duration\"),\n                        \"has_thea_guidance\": \"thea_guidance\" in data\n                    }\n\n                except Exception as e:",
            "        try:\n            if not self.project_scanner:\n                return {\"error\": \"Project scanner not available\"}\n\n            # Create scanner instance\n            scanner = self.project_scanner(project_root=str(project_path))\n\n            # Run scan\n            scan_start = time.time()\n            scanner.scan_project()\n            scan_time = time.time() - scan_start\n\n            # Load results\n            results_file = project_path / \"project_analysis.json\"\n            if results_file.exists():\n                with open(results_file, 'r', encoding='utf-8') as f:\n                    scan_data = json.load(f)\n\n                # Add metadata\n                scan_results = {\n                    \"scan_metadata\": {\n                        \"project_path\": str(project_path),\n                        \"scan_timestamp\": datetime.now().isoformat(),\n                        \"scan_duration\": round(scan_time, 2),\n                        \"scanner_version\": \"universal_v1\",\n                        \"cached\": False\n                    },\n                    \"scan_data\": scan_data\n                }\n\n                # Cache results\n                self._cache_scan_results(project_path, scan_results)\n\n                logger.info(f\" Project scan completed in {scan_time:.2f}s\")\n\n                if send_to_thea:\n                    return self._send_to_thea(scan_results, project_path)\n\n                return scan_results\n\n            else:\n                error_msg = f\"Scan completed but no results file found at {results_file}\"\n                logger.error(f\" {error_msg}\")\n                return {\"error\": error_msg}\n\n        except Exception as e:",
            "    try:\n        scanner = ProjectScannerIntegration()\n\n        if args.action == \"scan\":\n            project_path = Path(args.project) if args.project else None\n\n            print(f\" Scanning project: {project_path or 'current directory'}\")\n\n            results = scanner.scan_project(\n                project_path=project_path,\n                send_to_thea=not args.no_thea,\n                force_rescan=args.force\n            )\n\n            if \"error\" in results:\n                print(f\" Scan failed: {results['error']}\")\n                return 1\n\n            print(\" Scan completed successfully\")\n\n            # Save to output file if requested\n            if args.output:\n                with open(args.output, 'w', encoding='utf-8') as f:\n                    json.dump(results, f, indent=2, ensure_ascii=False)\n                print(f\" Results saved to: {args.output}\")\n\n            # Display summary\n            metadata = results.get(\"scan_metadata\", {})\n            print(f\" Files analyzed: {len(results.get('scan_data', {}))}\")\n            print(f\" Scan duration: {metadata.get('scan_duration', 'Unknown')}s\")\n\n            if \"thea_guidance\" in results and \"error\" not in results[\"thea_guidance\"]:\n                guidance = results[\"thea_guidance\"]\n                priority_tasks = guidance.get(\"priority_tasks\", [])\n                if priority_tasks:\n                    print(f\" Thea suggests focusing on {len(priority_tasks)} priority tasks\")\n\n        elif args.action == \"history\":\n            history = scanner.get_scan_history()\n            print(\" Project Scan History\")\n            print(\"=\" * 40)\n            print(f\"Total scans cached: {history.get('total_scans', 0)}\")\n\n            projects = history.get(\"projects\", {})\n            if projects:\n                for project, data in projects.items():\n                    print(f\"\\n {project}:\")\n                    print(f\"  Last scan: {data.get('last_scan', 'Unknown')}\")\n                    print(f\"  Duration: {data.get('scan_duration', 'Unknown')}s\")\n                    print(f\"  Thea guidance: {'' if data.get('has_thea_guidance') else ''}\")\n            else:\n                print(\"No cached scans found\")\n\n        elif args.action == \"cache\":\n            history = scanner.get_scan_history()\n            print(f\"Cache location: {history.get('cache_location')}\")\n            print(f\"Cached projects: {len(history.get('projects', {}))}\")\n\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling.py",
          "count": 7,
          "lines": [
            "                    try:\n                        return await func(*args, **kwargs)\n                    except exceptions as e:",
            "                    try:\n                        return func(*args, **kwargs)\n                    except exceptions as e:",
            "                try:\n                    return func(*args, **kwargs)\n                except Exception as e:",
            "            try:\n                return func(*args, **kwargs)\n            except Exception as e:",
            "        try:\n            return await coro\n        except Exception as e:",
            "        try:\n            return func(*args, **kwargs)\n        except Exception as e:",
            "        try:\n            yield\n        except error_types or Exception as e:"
          ]
        },
        {
          "file": "src\\core\\service_base.py",
          "count": 8,
          "lines": [
            "            try:\n                # Perform health check\n                health = self.health_check()\n                self._last_health_check = datetime.utcnow()\n\n                # Update health status based on checks\n                if health[\"state\"] == ServiceState.RUNNING:\n                    self._health_status = \"healthy\"\n                else:\n                    self._health_status = \"degraded\"\n\n            except Exception as e:",
            "            try:\n                # Update uptime metrics\n                if self._start_time:\n                    self.metrics.uptime_seconds = (datetime.utcnow() - self._start_time).total_seconds()\n\n                # Calculate error rate\n                if self.metrics.requests_total > 0:\n                    self.metrics.error_rate = self.metrics.requests_error / self.metrics.requests_total\n\n            except Exception as e:",
            "        try:\n            # Cancel all background tasks\n            for task in self._background_tasks:\n                if not task.done():\n                    task.cancel()\n\n            # Wait for tasks to complete\n            await asyncio.gather(*self._background_tasks, return_exceptions=True)\n\n            self.logger.info(\"Background service shut down gracefully\")\n            return True\n        except Exception as e:",
            "        try:\n            # Check dependencies\n            if not await self._check_dependencies():\n                self.logger.error(\"Dependencies not ready, aborting startup\")\n                self._state = ServiceState.ERROR\n                return False\n\n            # Run startup tasks\n            for task in self._startup_tasks:\n                await ErrorHandler.safe_execute_async(\n                    task(),\n                    log_errors=True,\n                    context=f\"startup task in {self.config.name}\"\n                )\n\n            # Service-specific initialization\n            if not await self.initialize():\n                self.logger.error(\"Service-specific initialization failed\")\n                self._state = ServiceState.ERROR\n                return False\n\n            # Start health check loop if enabled\n            if self.config.enable_health_checks:\n                asyncio.create_task(self._health_check_loop())\n\n            # Start metrics collection if enabled\n            if self.config.enable_metrics:\n                asyncio.create_task(self._metrics_loop())\n\n            self._state = ServiceState.RUNNING\n            self._health_status = \"healthy\"\n            self.logger.info(f\"Service {self.config.name} started successfully\")\n\n            return True\n\n        except Exception as e:",
            "        try:\n            # Create API application (subclass must implement)\n            self._app = await self.create_app()\n\n            # Add common middleware\n            await self._setup_middleware()\n\n            # Add health check routes\n            await self._add_health_routes()\n\n            self.logger.info(f\"API service initialized on {self.host}:{self.port}\")\n            return True\n        except Exception as e:",
            "        try:\n            # Run shutdown tasks in reverse order\n            for task in reversed(self._shutdown_tasks):\n                await ErrorHandler.safe_execute_async(\n                    task(),\n                    log_errors=True,\n                    context=f\"shutdown task in {self.config.name}\"\n                )\n\n            # Service-specific shutdown\n            await self.shutdown()\n\n            self._state = ServiceState.STOPPED\n            self._stop_time = datetime.utcnow()\n            self._health_status = \"stopped\"\n\n            uptime = self.uptime\n            uptime_str = f\"{uptime.total_seconds():.1f}s\" if uptime else \"unknown\"\n            self.logger.info(f\"Service {self.config.name} stopped (uptime: {uptime_str})\")\n\n            return True\n\n        except Exception as e:",
            "        try:\n            # Start background processing tasks\n            await self.start_background_tasks()\n            self.logger.info(\"Background service initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            if self._server:\n                await self._server.shutdown()\n                self.logger.info(\"API server shut down gracefully\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\coordinators\\analytics_coordinator.py",
          "count": 3,
          "lines": [
            "        try:\n            result = {\"processed\": True, \"timestamp\": datetime.now().isoformat()}\n\n            # Process with registered engines\n            for name, engine in self.engines.items():\n                if hasattr(engine, \"process\"):\n                    engine_result = await engine.process(data)\n                    result[f\"{name}_result\"] = engine_result\n\n            # Execute callbacks\n            for event_type, callback in self.callbacks.items():\n                if event_type in data.get(\"type\", \"\"):\n                    await callback(data)\n\n            return result\n        except Exception as e:",
            "        try:\n            self.active = False\n            self.logger.info(\"Analytics processing stopped\")\n            return {\"status\": \"stopped\", \"timestamp\": datetime.now().isoformat()}\n        except Exception as e:",
            "        try:\n            self.active = True\n            self.logger.info(\"Analytics processing started\")\n            return {\"status\": \"started\", \"timestamp\": datetime.now().isoformat()}\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\coordinators\\processing_coordinator.py",
          "count": 1,
          "lines": [
            "        try:\n            self.stats[\"total_processed\"] += 1\n            result = {\"processed\": True, \"timestamp\": datetime.now().isoformat()}\n\n            # Process with registered processors\n            for name, processor in self.processors.items():\n                if hasattr(processor, \"process\"):\n                    processor_result = await processor.process(data)\n                    result[f\"{name}_result\"] = processor_result\n\n            self.stats[\"successful\"] += 1\n            return result\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\batch_analytics_engine.py",
          "count": 3,
          "lines": [
            "        try:\n            if not data:\n                return {\"error\": \"No data provided\"}\n\n            # Simple batch processing\n            processed = self._process_items(data)\n            metrics = self._calculate_metrics(processed)\n\n            result = {\n                \"processed_items\": processed,\n                \"metrics\": metrics,\n                \"batch_size\": len(data),\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\n            # Update stats\n            self.stats[\"batches_processed\"] += 1\n            self.stats[\"total_items\"] += len(data)\n\n            self.logger.info(f\"Batch processed: {len(data)} items\")\n            return result\n\n        except Exception as e:",
            "        try:\n            processed = []\n\n            for item in data:\n                if isinstance(item, dict):\n                    # Simple processing\n                    processed_item = {\n                        \"original\": item,\n                        \"processed\": True,\n                        \"timestamp\": datetime.now().isoformat(),\n                    }\n                    processed.append(processed_item)\n\n            return processed\n        except Exception as e:",
            "        try:\n            return {\n                \"items_processed\": len(processed),\n                \"success_rate\": 1.0,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\caching_engine_fixed.py",
          "count": 5,
          "lines": [
            "        try:\n            # If key exists, move to end\n            if key in self.cache:\n                self.cache.move_to_end(key)\n\n            self.cache[key] = value\n            self.stats[\"sets\"] += 1\n\n            # Evict if over max_size (LRU: remove from front)\n            if len(self.cache) > self.max_size:\n                evicted_key = next(iter(self.cache))\n                del self.cache[evicted_key]\n                self.stats[\"evictions\"] += 1\n                self.logger.debug(f\"Cache eviction (LRU): {evicted_key}\")\n\n            self.logger.debug(f\"Cache set: {key}\")\n            return True\n        except Exception as e:",
            "        try:\n            if key in self.cache:\n                # Move to end (most recently used)\n                self.cache.move_to_end(key)\n                self.stats[\"hits\"] += 1\n                self.logger.debug(f\"Cache hit: {key}\")\n                return self.cache[key]\n            else:\n                self.stats[\"misses\"] += 1\n                self.logger.debug(f\"Cache miss: {key}\")\n                return None\n        except Exception as e:",
            "        try:\n            if key in self.cache:\n                del self.cache[key]\n                self.logger.debug(f\"Cache deleted: {key}\")\n                return True\n            return False\n        except Exception as e:",
            "        try:\n            self.cache.clear()\n            self.logger.info(\"Cache cleared\")\n        except Exception as e:",
            "        try:\n            total_requests = self.stats[\"hits\"] + self.stats[\"misses\"]\n            hit_rate = self.stats[\"hits\"] / total_requests if total_requests > 0 else 0\n\n            return {\n                \"hits\": self.stats[\"hits\"],\n                \"misses\": self.stats[\"misses\"],\n                \"sets\": self.stats[\"sets\"],\n                \"evictions\": self.stats[\"evictions\"],\n                \"hit_rate\": hit_rate,\n                \"cache_size\": len(self.cache),\n                \"max_size\": self.max_size,\n                \"memory_safe\": True,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\coordination_analytics_engine.py",
          "count": 4,
          "lines": [
            "        try:\n            if not coordination_data:\n                return {\"error\": \"No coordination data provided\"}\n\n            # Simple analytics collection\n            metrics = self._extract_metrics(coordination_data)\n            insights = self._generate_insights(metrics)\n\n            result = {\n                \"metrics\": metrics,\n                \"insights\": insights,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\n            # Store in history\n            self.analytics_history.append(result)\n            if len(self.analytics_history) > 100:  # Keep only last 100\n                self.analytics_history.pop(0)\n\n            self.logger.info(\"Coordination analytics collected\")\n            return result\n\n        except Exception as e:",
            "        try:\n            if not self.analytics_history:\n                return {\"message\": \"No analytics data available\"}\n\n            total_analytics = len(self.analytics_history)\n            recent_analytics = self.analytics_history[-1] if self.analytics_history else {}\n\n            return {\n                \"total_analytics\": total_analytics,\n                \"recent_analytics\": recent_analytics,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        except Exception as e:",
            "        try:\n            insights = []\n\n            # Simple insight generation\n            if metrics.get(\"agent_count\", 0) > 0:\n                insights.append(\n                    {\n                        \"type\": \"coordination\",\n                        \"description\": f\"Active agents: {metrics['agent_count']}\",\n                        \"priority\": \"normal\",\n                    }\n                )\n\n            return insights\n        except Exception as e:",
            "        try:\n            metrics = {\n                \"data_points\": len(data) if isinstance(data, dict) else 1,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\n            # Simple metric extraction\n            if \"agents\" in data:\n                metrics[\"agent_count\"] = len(data[\"agents\"])\n\n            return metrics\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\metrics_engine.py",
          "count": 11,
          "lines": [
            "            try:\n                self.metrics_repository = MetricsRepository()\n                self.logger.debug(\"MetricsRepository initialized for persistence\")\n            except Exception as e:",
            "        try:\n            all_metrics = self.get_all_metrics()\n            success = self.metrics_repository.save_metrics_snapshot(all_metrics, source=source)\n            if success:\n                self.logger.debug(f\"Metrics snapshot saved: {source} ({len(all_metrics)} metrics)\")\n            return success\n        except Exception as e:",
            "        try:\n            if isinstance(value, (int, float)):\n                self.metrics[name] = value\n            else:\n                self.metrics[f\"{name}_count\"] += 1\n            self.logger.debug(f\"Recorded metric: {name} = {value}\")\n        except Exception as e:",
            "        try:\n            if not self.error_history:\n                return {\"message\": \"No error data available\"}\n\n            error_types = defaultdict(int)\n            for error in self.error_history:\n                error_types[error[\"error_type\"]] += 1\n\n            return {\n                \"total_errors\": len(self.error_history),\n                \"error_types\": dict(error_types),\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        except Exception as e:",
            "        try:\n            if not self.performance_history:\n                return {\"message\": \"No performance data available\"}\n\n            durations = [p[\"duration\"] for p in self.performance_history]\n            avg_duration = sum(durations) / len(durations)\n            max_duration = max(durations)\n            min_duration = min(durations)\n\n            return {\n                \"total_operations\": len(self.performance_history),\n                \"avg_duration\": round(avg_duration, 3),\n                \"max_duration\": round(max_duration, 3),\n                \"min_duration\": round(min_duration, 3),\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        except Exception as e:",
            "        try:\n            return self.metrics_repository.get_metrics_history(source=source, limit=limit)\n        except Exception as e:",
            "        try:\n            return self.metrics_repository.get_metrics_trend(metric_name, source=source, limit=limit)\n        except Exception as e:",
            "        try:\n            self.error_history.append(\n                {\n                    \"error_type\": error_type,\n                    \"message\": message,\n                    \"timestamp\": datetime.now().isoformat(),\n                }\n            )\n            self.logger.debug(f\"Recorded error: {error_type} - {message}\")\n        except Exception as e:",
            "        try:\n            self.metrics[name] += amount\n            self.logger.debug(f\"Incremented metric: {name} by {amount}\")\n        except Exception as e:",
            "        try:\n            self.performance_history.append(\n                {\n                    \"operation\": operation,\n                    \"duration\": duration,\n                    \"timestamp\": datetime.now().isoformat(),\n                }\n            )\n            self.logger.debug(f\"Recorded performance: {operation} took {duration}s\")\n        except Exception as e:",
            "try:\n    from src.repositories.metrics_repository import MetricsRepository\n    METRICS_REPOSITORY_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\analytics\\engines\\realtime_analytics_engine.py",
          "count": 5,
          "lines": [
            "            try:\n                if self.queue:\n                    data = self.queue.popleft()\n                    await self._process_data(data)\n                else:\n                    await asyncio.sleep(0.1)  # Small delay when queue is empty\n            except Exception as e:",
            "        try:\n            self.active = False\n            if self.task:\n                self.task.cancel()\n            self.logger.info(\"Real-time analytics processing stopped\")\n            return {\"status\": \"stopped\", \"timestamp\": datetime.now().isoformat()}\n        except Exception as e:",
            "        try:\n            self.active = True\n            self.task = asyncio.create_task(self._processing_loop())\n            self.logger.info(\"Real-time analytics processing started\")\n            return {\"status\": \"started\", \"timestamp\": datetime.now().isoformat()}\n        except Exception as e:",
            "        try:\n            self.queue.append(data)\n            self.logger.debug(f\"Added data to queue: {data.get('id', 'unknown')}\")\n        except Exception as e:",
            "        try:\n            self.stats[\"processed\"] += 1\n            self.logger.debug(f\"Processed data: {data.get('id', 'unknown')}\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\anomaly_detection_engine.py",
          "count": 1,
          "lines": [
            "        try:\n            mean = statistics.mean(data)\n            stdev = statistics.stdev(data)\n\n            if stdev == 0:\n                return []\n\n            anomalies = []\n            for i, value in enumerate(data):\n                z_score = abs((value - mean) / stdev)\n                if z_score > self.thresholds[\"z_score\"]:\n                    anomalies.append(\n                        {\n                            \"index\": i,\n                            \"value\": value,\n                            \"z_score\": z_score,\n                            \"type\": \"statistical\",\n                            \"severity\": \"high\" if z_score > 3.0 else \"medium\",\n                        }\n                    )\n\n            return anomalies\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\business_intelligence_engine_core.py",
          "count": 2,
          "lines": [
            "        try:\n            if not data:\n                return {\"error\": \"No data provided\"}\n\n            # Simple insight generation\n            insights = self._analyze_data(data)\n            recommendations = self._generate_recommendations(insights)\n            kpis = self._calculate_kpis(data)\n\n            insight_result = {\n                \"insights\": insights,\n                \"recommendations\": recommendations,\n                \"kpis\": kpis,\n                \"data_points\": len(data),\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\n            # Store insights\n            self.insights.append(insight_result)\n            if len(self.insights) > 50:  # Keep only last 50\n                self.insights = self.insights[-50:]\n\n            return insight_result\n\n        except Exception as e:",
            "        try:\n            mean_val = statistics.mean(values)\n            median_val = statistics.median(values)\n            std_val = statistics.stdev(values) if len(values) > 1 else 0\n\n            # Simple trend analysis\n            if len(values) >= 2:\n                trend = \"increasing\" if values[-1] > values[0] else \"decreasing\"\n                trend_strength = abs(values[-1] - values[0]) / values[0] if values[0] != 0 else 0\n            else:\n                trend = \"stable\"\n                trend_strength = 0\n\n            return {\n                \"field\": field,\n                \"mean\": mean_val,\n                \"median\": median_val,\n                \"std_dev\": std_val,\n                \"trend\": trend,\n                \"trend_strength\": trend_strength,\n                \"data_points\": len(values),\n            }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\business_intelligence_engine_operations.py",
          "count": 3,
          "lines": [
            "        try:\n            # Simple optimization - clear old insights\n            if len(self.insights) > 100:\n                self.insights = self.insights[-50:]\n\n            return {\n                \"optimization_applied\": \"cleared_old_insights\",\n                \"insights_count\": len(self.insights),\n                \"status\": \"optimized\",\n            }\n\n        except Exception as e:",
            "        try:\n            if format == \"json\":\n                import json\n\n                return json.dumps(self.insights, indent=2)\n            elif format == \"csv\":\n                return self._export_csv()\n            else:\n                return str(self.insights)\n\n        except Exception as e:",
            "        try:\n            if not data:\n                return {\"error\": \"No data provided\"}\n\n            # Generate dashboard metrics\n            dashboard = {\n                \"summary\": self._generate_summary(data),\n                \"charts\": self._generate_chart_data(data),\n                \"alerts\": self._generate_alerts(data),\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\n            return dashboard\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis_engine.py",
          "count": 2,
          "lines": [
            "        try:\n            if not data:\n                return {\"error\": \"No data provided\"}\n\n            # Use modular components for analysis\n            patterns = self.pattern_extractor.extract_patterns(data)\n            trends = self.trend_analyzer.analyze_trends(data)\n            anomalies = self.anomaly_detector.detect_anomalies(data)\n\n            analysis_result = {\n                \"patterns\": patterns,\n                \"trends\": trends,\n                \"anomalies\": anomalies,\n                \"data_points\": len(data),\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\n            # Store in history\n            self.analysis_history.append(analysis_result)\n\n            return analysis_result\n        except Exception as e:",
            "        try:\n            if not self.analysis_history:\n                return {\"message\": \"No analysis data available\"}\n\n            total_analyses = len(self.analysis_history)\n            recent_analysis = self.analysis_history[-1] if self.analysis_history else {}\n\n            return {\n                \"total_analyses\": total_analyses,\n                \"recent_analysis\": recent_analysis,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\predictive_modeling_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            # Simple prediction simulation\n            values = [v for v in input_data.values() if isinstance(v, (int, float))]\n            if not values:\n                return 0.0\n\n            # Use mean as base prediction\n            base_prediction = statistics.mean(values)\n\n            # Add some randomness for simulation\n            import random\n\n            noise = random.uniform(-0.1, 0.1)\n            return round(base_prediction * (1 + noise), 3)\n        except Exception as e:",
            "        try:\n            if model_name in self.models:\n                del self.models[model_name]\n                self.logger.info(f\"Deleted model: {model_name}\")\n                return True\n            return False\n        except Exception as e:",
            "        try:\n            if model_name not in self.models:\n                self.logger.error(f\"Model {model_name} not found\")\n                return False\n\n            # Simple training simulation\n            self.models[model_name][\"trained\"] = True\n            self.models[model_name][\"accuracy\"] = 0.85  # Simulated accuracy\n            self.models[model_name][\"training_samples\"] = len(training_data)\n\n            self.logger.info(f\"Trained model: {model_name} with {len(training_data)} samples\")\n            return True\n        except Exception as e:",
            "        try:\n            if model_name not in self.models:\n                self.logger.error(f\"Model {model_name} not found\")\n                return None\n\n            model = self.models[model_name]\n            if not model[\"trained\"]:\n                self.logger.error(f\"Model {model_name} not trained\")\n                return None\n\n            # Simple prediction simulation\n            prediction = {\n                \"model_name\": model_name,\n                \"predicted_value\": self._simulate_prediction(input_data),\n                \"confidence\": model[\"accuracy\"],\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\n            # Store prediction history\n            self.predictions_history.append(prediction)\n            if len(self.predictions_history) > 1000:  # Keep only last 1000\n                self.predictions_history.pop(0)\n\n            self.logger.info(f\"Prediction made with model: {model_name}\")\n            return prediction\n        except Exception as e:",
            "        try:\n            if not self.predictions_history:\n                return {\"message\": \"No predictions available\"}\n\n            total_predictions = len(self.predictions_history)\n            recent_predictions = self.predictions_history[-10:]  # Last 10\n\n            return {\n                \"total_predictions\": total_predictions,\n                \"recent_predictions\": recent_predictions,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        except Exception as e:",
            "        try:\n            self.models[model_name] = {\n                \"type\": model_type,\n                \"created_at\": datetime.now().isoformat(),\n                \"trained\": False,\n                \"accuracy\": 0.0,\n            }\n            self.logger.info(f\"Created model: {model_name} ({model_type})\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\anomaly_detector.py",
          "count": 5,
          "lines": [
            "        try:\n            if len(values) < 3:\n                return []\n\n            # Calculate mean and standard deviation\n            mean_val = statistics.mean(values)\n            stdev_val = statistics.stdev(values) if len(values) > 1 else 0\n\n            if stdev_val == 0:\n                return []\n\n            anomalies = []\n\n            # Find values that are more than 2 standard deviations from mean\n            threshold = 2 * stdev_val\n            for i, value in enumerate(values):\n                if abs(value - mean_val) > threshold:\n                    anomalies.append(\n                        {\n                            \"index\": i,\n                            \"value\": value,\n                            \"deviation\": round(abs(value - mean_val), 3),\n                            \"z_score\": round((value - mean_val) / stdev_val, 3),\n                        }\n                    )\n\n            return anomalies[:5]  # Limit to 5 anomalies\n        except Exception as e:",
            "        try:\n            if len(values) < 3:\n                return []\n\n            mean_val = statistics.mean(values)\n            stdev_val = statistics.stdev(values)\n\n            if stdev_val == 0:\n                return []\n\n            outliers = []\n            threshold = 2.5  # Z-score threshold\n\n            for i, value in enumerate(values):\n                z_score = abs((value - mean_val) / stdev_val)\n                if z_score > threshold:\n                    outliers.append(\n                        {\n                            \"index\": i,\n                            \"value\": value,\n                            \"z_score\": round(z_score, 3),\n                            \"threshold\": threshold,\n                        }\n                    )\n\n            return outliers\n        except Exception as e:",
            "        try:\n            if len(values) < 4:\n                return []\n\n            if method == \"iqr\":\n                return self._detect_outliers_iqr(values)\n            elif method == \"zscore\":\n                return self._detect_outliers_zscore(values)\n            else:\n                return self._detect_outliers_iqr(values)\n        except Exception as e:",
            "        try:\n            if not data:\n                return []\n\n            # Extract numeric values for anomaly detection\n            numeric_values = []\n            for item in data:\n                for value in item.values():\n                    if isinstance(value, (int, float)):\n                        numeric_values.append(value)\n\n            if len(numeric_values) < 3:\n                return []\n\n            # Detect anomalies using statistical methods\n            anomalies = self._detect_statistical_anomalies(numeric_values)\n\n            return anomalies\n        except Exception as e:",
            "        try:\n            sorted_values = sorted(values)\n            n = len(sorted_values)\n\n            # Calculate quartiles\n            q1_idx = n // 4\n            q3_idx = 3 * n // 4\n\n            q1 = sorted_values[q1_idx]\n            q3 = sorted_values[q3_idx]\n\n            # Calculate IQR\n            iqr = q3 - q1\n\n            # Define outlier bounds\n            lower_bound = q1 - 1.5 * iqr\n            upper_bound = q3 + 1.5 * iqr\n\n            outliers = []\n            for i, value in enumerate(values):\n                if value < lower_bound or value > upper_bound:\n                    outliers.append(\n                        {\n                            \"index\": i,\n                            \"value\": value,\n                            \"lower_bound\": lower_bound,\n                            \"upper_bound\": upper_bound,\n                        }\n                    )\n\n            return outliers\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\pattern_extractor.py",
          "count": 4,
          "lines": [
            "        try:\n            # Count occurrences of each key\n            key_counts = Counter()\n            for item in data:\n                for key in item.keys():\n                    key_counts[key] += 1\n\n            # Find most common keys\n            most_common = key_counts.most_common(5)\n\n            return {\n                \"most_common_keys\": most_common,\n                \"total_keys\": len(key_counts),\n                \"unique_keys\": list(key_counts.keys()),\n            }\n        except Exception as e:",
            "        try:\n            # Extract numeric values\n            numeric_values = []\n            for item in data:\n                for value in item.values():\n                    if isinstance(value, (int, float)):\n                        numeric_values.append(value)\n\n            if not numeric_values:\n                return {\"message\": \"No numeric values found\"}\n\n            # Calculate statistics\n            mean_val = statistics.mean(numeric_values)\n            median_val = statistics.median(numeric_values)\n            stdev_val = statistics.stdev(numeric_values) if len(numeric_values) > 1 else 0\n\n            return {\n                \"mean\": round(mean_val, 3),\n                \"median\": round(median_val, 3),\n                \"stdev\": round(stdev_val, 3),\n                \"min\": min(numeric_values),\n                \"max\": max(numeric_values),\n                \"count\": len(numeric_values),\n            }\n        except Exception as e:",
            "        try:\n            # Look for timestamp fields\n            timestamp_fields = []\n            for item in data:\n                for key, value in item.items():\n                    if \"time\" in key.lower() or \"date\" in key.lower():\n                        timestamp_fields.append(key)\n                        break\n\n            return {\n                \"timestamp_fields\": list(set(timestamp_fields)),\n                \"has_temporal_data\": len(timestamp_fields) > 0,\n            }\n        except Exception as e:",
            "        try:\n            if not data:\n                return {\"error\": \"No data provided\"}\n\n            patterns = {\n                \"frequency_patterns\": self._extract_frequency_patterns(data),\n                \"value_patterns\": self._extract_value_patterns(data),\n                \"temporal_patterns\": self._extract_temporal_patterns(data),\n            }\n\n            return patterns\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\intelligence\\pattern_analysis\\trend_analyzer.py",
          "count": 4,
          "lines": [
            "        try:\n            if len(values) < 2:\n                return \"stable\"\n\n            # Simple linear regression slope\n            n = len(values)\n            x_mean = (n - 1) / 2\n            y_mean = statistics.mean(values)\n\n            numerator = sum((i - x_mean) * (values[i] - y_mean) for i in range(n))\n            denominator = sum((i - x_mean) ** 2 for i in range(n))\n\n            if denominator == 0:\n                return \"stable\"\n\n            slope = numerator / denominator\n\n            # Determine direction based on slope\n            if slope > 0.01:\n                return \"increasing\"\n            elif slope < -0.01:\n                return \"decreasing\"\n            else:\n                return \"stable\"\n        except Exception as e:",
            "        try:\n            if len(values) < 2:\n                return 0.0\n\n            n = len(values)\n            x_mean = (n - 1) / 2\n            y_mean = statistics.mean(values)\n\n            numerator = sum((i - x_mean) * (values[i] - y_mean) for i in range(n))\n            denominator = sum((i - x_mean) ** 2 for i in range(n))\n\n            if denominator == 0:\n                return 0.0\n\n            slope = numerator / denominator\n            return round(slope, 3)\n        except Exception as e:",
            "        try:\n            if len(values) < 3:\n                return 0.0\n\n            # Use coefficient of variation as a measure of trend strength\n            mean_val = statistics.mean(values)\n            if mean_val == 0:\n                return 0.0\n\n            stdev_val = statistics.stdev(values) if len(values) > 1 else 0\n            cv = abs(stdev_val / mean_val) if mean_val != 0 else 0\n\n            # Convert to strength (higher variation = stronger trend if consistent direction)\n            strength = min(1.0, cv * 2)  # Scale to 0-1 range\n\n            return round(strength, 3)\n        except Exception as e:",
            "        try:\n            if not data:\n                return {\"error\": \"No data provided\"}\n\n            # Extract numeric values for trend analysis\n            numeric_values = []\n            for item in data:\n                for value in item.values():\n                    if isinstance(value, (int, float)):\n                        numeric_values.append(value)\n\n            if len(numeric_values) < 2:\n                return {\"message\": \"Insufficient data for trend analysis\"}\n\n            # Calculate trend direction and slope\n            trend_direction = self._calculate_trend_direction(numeric_values)\n            trend_strength = self._calculate_trend_strength(numeric_values)\n            trend_slope = self._calculate_trend_slope(numeric_values)\n\n            return {\n                \"direction\": trend_direction,\n                \"strength\": trend_strength,\n                \"slope\": trend_slope,\n                \"data_points\": len(numeric_values),\n                \"mean\": round(statistics.mean(numeric_values), 3),\n                \"median\": round(statistics.median(numeric_values), 3),\n            }\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\orchestrators\\coordination_analytics_orchestrator.py",
          "count": 5,
          "lines": [
            "        try:\n            if not self.active:\n                return {\"error\": \"System not active\"}\n\n            self.stats[\"analytics_processed\"] += 1\n\n            # Simple analytics processing\n            result = {\n                \"analysis_id\": f\"analysis_{datetime.now().timestamp()}\",\n                \"data_processed\": len(data),\n                \"timestamp\": datetime.now().isoformat(),\n                \"recommendations\": self._generate_recommendations(data),\n            }\n\n            self.logger.info(f\"Processed analytics: {result['analysis_id']}\")\n            return result\n\n        except Exception as e:",
            "        try:\n            recommendations = []\n\n            # Simple recommendation logic\n            if data.get(\"efficiency\", 0) < 0.8:\n                recommendations.append(\n                    {\n                        \"type\": \"efficiency_improvement\",\n                        \"priority\": \"high\",\n                        \"message\": \"Consider optimizing coordination efficiency\",\n                    }\n                )\n\n            if data.get(\"coordination_score\", 0) < 0.7:\n                recommendations.append(\n                    {\n                        \"type\": \"coordination_enhancement\",\n                        \"priority\": \"medium\",\n                        \"message\": \"Enhance coordination mechanisms\",\n                    }\n                )\n\n            self.stats[\"recommendations_generated\"] += len(recommendations)\n            return recommendations\n\n        except Exception as e:",
            "        try:\n            return {\n                \"system_status\": \"active\" if self.active else \"inactive\",\n                \"stats\": self.stats.copy(),\n                \"timestamp\": datetime.now().isoformat(),\n                \"uptime\": datetime.now().isoformat(),\n            }\n        except Exception as e:",
            "        try:\n            self.active = False\n            self.logger.info(\"Coordination analytics system stopped\")\n            return {\"status\": \"stopped\", \"timestamp\": datetime.now().isoformat()}\n        except Exception as e:",
            "        try:\n            self.active = True\n            self.logger.info(\"Coordination analytics system started\")\n            return {\"status\": \"started\", \"timestamp\": datetime.now().isoformat()}\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\insight_processor.py",
          "count": 3,
          "lines": [
            "        try:\n            # Simple validation\n            required_fields = [\"insight_id\", \"type\", \"message\"]\n            for field in required_fields:\n                if field not in insight or not insight[field]:\n                    return False\n\n            # Validate confidence\n            confidence = insight.get(\"confidence\", 0)\n            if not isinstance(confidence, (int, float)) or confidence < 0 or confidence > 1:\n                return False\n\n            return True\n        except Exception as e:",
            "        try:\n            results = []\n            for insight in insights:\n                result = self.process_insight(insight)\n                results.append(result)\n\n            self.logger.info(f\"Batch processed {len(insights)} insights\")\n            return results\n        except Exception as e:",
            "        try:\n            self.stats[\"insights_processed\"] += 1\n\n            # Simple insight processing\n            processed_insight = {\n                \"insight_id\": insight_data.get(\n                    \"insight_id\", f\"insight_{datetime.now().timestamp()}\"\n                ),\n                \"type\": insight_data.get(\"type\", \"unknown\"),\n                \"message\": insight_data.get(\"message\", \"\"),\n                \"confidence\": insight_data.get(\"confidence\", 0.5),\n                \"timestamp\": datetime.now().isoformat(),\n                \"metadata\": insight_data.get(\"metadata\", {}),\n            }\n\n            # Validate insight\n            if self._validate_insight(processed_insight):\n                self.logger.info(f\"Processed insight: {processed_insight['insight_id']}\")\n                return processed_insight\n            else:\n                self.stats[\"validation_errors\"] += 1\n                return {\n                    \"error\": \"validation_failed\",\n                    \"insight_id\": processed_insight[\"insight_id\"],\n                }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction_processor.py",
          "count": 3,
          "lines": [
            "        try:\n            # Simple validation\n            required_fields = [\"prediction_id\", \"predicted_value\", \"confidence\"]\n            for field in required_fields:\n                if field not in prediction:\n                    return False\n\n            # Validate confidence\n            confidence = prediction.get(\"confidence\", 0)\n            if not isinstance(confidence, (int, float)) or confidence < 0 or confidence > 1:\n                return False\n\n            return True\n        except Exception as e:",
            "        try:\n            results = []\n            for prediction in predictions:\n                result = self.process_prediction(prediction)\n                results.append(result)\n\n            self.logger.info(f\"Batch processed {len(predictions)} predictions\")\n            return results\n        except Exception as e:",
            "        try:\n            self.stats[\"predictions_generated\"] += 1\n\n            # Simple prediction processing\n            prediction = {\n                \"prediction_id\": f\"pred_{datetime.now().timestamp()}\",\n                \"predicted_value\": data.get(\"value\", 0),\n                \"confidence\": data.get(\"confidence\", 0.8),\n                \"timestamp\": datetime.now().isoformat(),\n                \"metadata\": data.get(\"metadata\", {}),\n            }\n\n            # Validate prediction\n            if self._validate_prediction(prediction):\n                self.logger.info(f\"Processed prediction: {prediction['prediction_id']}\")\n                return prediction\n            else:\n                self.stats[\"validation_errors\"] += 1\n                return {\n                    \"error\": \"validation_failed\",\n                    \"prediction_id\": prediction[\"prediction_id\"],\n                }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\analytics\\processors\\prediction\\prediction_analyzer.py",
          "count": 1,
          "lines": [
            "try:\n    from ...prediction.base_analyzer import BasePredictionAnalyzer\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\config\\config_dataclasses.py",
          "count": 2,
          "lines": [
            "        try:\n            from .test_categories_config import get_test_categories\n\n            return get_test_categories()\n        except ImportError:",
            "try:\n    from ..error_handling.error_models_enums import RetryStrategy\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\config\\config_manager.py",
          "count": 1,
          "lines": [
            "try:\n    from src.architecture import Singleton\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\config\\configuration_factory.py",
          "count": 8,
          "lines": [
            "                try:\n                    config[config_key] = json.loads(value)\n                except (json.JSONDecodeError, TypeError):",
            "                try:\n                    if not validator(field_value):\n                        errors.append(f\"Field '{field_name}' failed custom validation\")\n                except Exception as e:",
            "                try:\n                    if path.name == \"config.json\":\n                        with open(path, 'r') as f:\n                            config.update(json.load(f))\n                    elif path.name == \"config.yaml\" and YAML_AVAILABLE:\n                        with open(path, 'r') as f:\n                            config.update(yaml.safe_load(f))\n                    elif path.name == \".env\":\n                        # Load .env file\n                        self._load_dotenv_file(path, config)\n                except Exception as e:",
            "                try:\n                    if path.suffix == '.json':\n                        with open(path, 'r') as f:\n                            config.update(json.load(f))\n                    elif path.suffix == '.yaml' and YAML_AVAILABLE:\n                        with open(path, 'r') as f:\n                            config.update(yaml.safe_load(f))\n                except Exception as e:",
            "        try:\n            # Load configuration using layered approach\n            config = self._load_layered_config(service_name)\n\n            # Apply schema defaults if provided\n            if schema:\n                defaults = schema.get_defaults()\n                for key, value in defaults.items():\n                    if key not in config:\n                        config[key] = value\n\n            # Validate against schema if provided\n            if schema:\n                errors = schema.validate(config)\n                if errors:\n                    error_msg = f\"Configuration validation failed for {service_name}:\\n\" + \"\\n\".join(f\"  - {error}\" for error in errors)\n                    raise ConfigurationError(error_msg)\n\n            # Cache the configuration\n            self._cache[cache_key] = config.copy()\n\n            self.logger.info(f\" Loaded configuration for {service_name} \"\n                           f\"({len(config)} settings)\")\n            return config\n\n        except Exception as e:",
            "        try:\n            float(value)\n            return True\n        except ValueError:",
            "        try:\n            from dotenv import load_dotenv\n            # Load .env but don't override existing env vars\n            load_dotenv(path, override=False)\n\n            # Re-scan environment variables\n            for key, value in os.environ.items():\n                if key not in config:  # Don't override already loaded values\n                    config[key.lower()] = value\n        except ImportError:",
            "try:\n    import yaml\n    YAML_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\consolidation\\utility_consolidation\\utility_consolidation_engine.py",
          "count": 5,
          "lines": [
            "        try:\n            duplicates = []\n            seen = set()\n\n            for utility in utilities:\n                if isinstance(utility, dict) and \"name\" in utility:\n                    name = utility[\"name\"]\n                    if name in seen:\n                        duplicates.append(utility)\n                    else:\n                        seen.add(name)\n\n            return duplicates\n        except Exception as e:",
            "        try:\n            if not self.consolidation_history:\n                return {\"message\": \"No consolidation data available\"}\n\n            total_consolidations = len(self.consolidation_history)\n            recent_consolidation = (\n                self.consolidation_history[-1] if self.consolidation_history else {}\n            )\n\n            return {\n                \"total_consolidations\": total_consolidations,\n                \"recent_consolidation\": recent_consolidation,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n        except Exception as e:",
            "        try:\n            if not utilities:\n                return {\"error\": \"No utilities provided\"}\n\n            # Simple consolidation logic\n            consolidated = self._merge_utilities(utilities)\n            duplicates = self._find_duplicates(utilities)\n            optimized = self._optimize_utilities(consolidated)\n\n            result = {\n                \"consolidated\": consolidated,\n                \"duplicates_found\": len(duplicates),\n                \"optimized\": optimized,\n                \"original_count\": len(utilities),\n                \"consolidated_count\": len(consolidated),\n                \"timestamp\": datetime.now().isoformat(),\n            }\n\n            # Store in history\n            self.consolidation_history.append(result)\n            if len(self.consolidation_history) > 100:  # Keep only last 100\n                self.consolidation_history.pop(0)\n\n            self.logger.info(f\"Utilities consolidated: {len(utilities)} -> {len(consolidated)}\")\n            return result\n\n        except Exception as e:",
            "        try:\n            merged = []\n            seen = set()\n\n            for utility in utilities:\n                if isinstance(utility, dict) and \"name\" in utility:\n                    name = utility[\"name\"]\n                    if name not in seen:\n                        merged.append(utility)\n                        seen.add(name)\n\n            return merged\n        except Exception as e:",
            "        try:\n            optimized = []\n\n            for utility in utilities:\n                if isinstance(utility, dict):\n                    # Simple optimization\n                    optimized_utility = utility.copy()\n                    optimized_utility[\"optimized\"] = True\n                    optimized.append(optimized_utility)\n\n            return optimized\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\consolidation\\utility_consolidation\\utility_consolidation_orchestrator.py",
          "count": 1,
          "lines": [
            "        try:\n            logger.info(\n                f' Executing consolidation for {opportunity.primary_function.name}...'\n                )\n            consolidated_content = self._create_consolidated_function(\n                opportunity)\n            consolidated_path = self._write_consolidated_file(opportunity.\n                primary_function.name, consolidated_content)\n            self._update_references(opportunity)\n            return ConsolidationResult(success=True, functions_consolidated\n                =len(opportunity.duplicate_functions) + 1, lines_reduced=\n                opportunity.estimated_reduction, new_file_path=\n                consolidated_path)\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\constants\\fsm_utilities.py",
          "count": 1,
          "lines": [
            "    try:\n        # Validate state timeout\n        if FSM_STATE_TIMEOUT_SECONDS is not None and FSM_STATE_TIMEOUT_SECONDS <= 0:\n            return False\n\n        # Validate retry count\n        if FSM_STATE_RETRY_COUNT < 0:\n            return False\n\n        # Validate retry delay\n        if FSM_STATE_RETRY_DELAY < 0:\n            return False\n\n        # Validate transition priority\n        if FSM_TRANSITION_PRIORITY_DEFAULT < 0:\n            return False\n\n        # Validate transition timeout\n        if FSM_TRANSITION_TIMEOUT_SECONDS is not None and FSM_TRANSITION_TIMEOUT_SECONDS <= 0:\n            return False\n\n        return True\n    except Exception:"
          ]
        },
        {
          "file": "src\\core\\constants\\manager.py",
          "count": 1,
          "lines": [
            "    try:\n        with config_path.open(\"r\", encoding=\"utf-8\") as fh:\n            return yaml.safe_load(fh) or {}\n    except FileNotFoundError:"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\engines\\performance_monitoring_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            # Update basic metrics\n            self.metrics.total_tasks += 1\n\n            if result.success:\n                self.metrics.successful_tasks += 1\n            else:\n                self.metrics.failed_tasks += 1\n\n            # Update execution time metrics\n            if result.execution_time_seconds:\n                self.metrics.total_execution_time += result.execution_time_seconds\n                self.metrics.average_execution_time = (\n                    self.metrics.total_execution_time / self.metrics.total_tasks\n                )\n\n            # Update efficiency\n            efficiency = self._calculate_efficiency(result)\n            self.efficiency_history.append(efficiency)\n            self.metrics.average_efficiency = sum(self.efficiency_history) / len(\n                self.efficiency_history\n            )\n\n            # Store performance data\n            self.performance_history.append(\n                {\n                    \"task_id\": result.task_id,\n                    \"success\": result.success,\n                    \"execution_time\": result.execution_time_seconds,\n                    \"efficiency\": efficiency,\n                    \"timestamp\": datetime.now(),\n                }\n            )\n\n        except Exception as e:",
            "        try:\n            if len(self.efficiency_history) < window_size:\n                return list(self.efficiency_history)\n\n            return list(self.efficiency_history)[-window_size:]\n\n        except Exception as e:",
            "        try:\n            if not result.success:\n                return 0.0\n\n            # Base efficiency from success\n            base_efficiency = 1.0\n\n            # Time-based efficiency (faster is better)\n            if result.execution_time_seconds:\n                time_efficiency = max(\n                    0.0, 1.0 - (result.execution_time_seconds / 10.0)\n                )  # Normalize to 10s max\n            else:\n                time_efficiency = 0.5\n\n            # Combine factors\n            efficiency = (base_efficiency * 0.7) + (time_efficiency * 0.3)\n            return min(efficiency, 1.0)\n\n        except Exception as e:",
            "        try:\n            return {\n                \"coordination_metrics\": self.metrics.to_dict(),\n                \"efficiency_history\": list(self.efficiency_history),\n                \"performance_history\": list(self.performance_history),\n                \"export_timestamp\": datetime.now().isoformat(),\n            }\n\n        except Exception as e:",
            "        try:\n            self.metrics = create_coordination_metrics()\n            self.efficiency_history.clear()\n            self.performance_history.clear()\n            self.logger.info(\"Performance metrics reset\")\n\n        except Exception as e:",
            "        try:\n            total_tasks = self.metrics.total_tasks\n            success_rate = (\n                self.metrics.successful_tasks / total_tasks * 100 if total_tasks > 0 else 0\n            )\n\n            return {\n                \"metrics\": {\n                    \"total_tasks\": total_tasks,\n                    \"successful_tasks\": self.metrics.successful_tasks,\n                    \"failed_tasks\": self.metrics.failed_tasks,\n                    \"success_rate\": success_rate,\n                    \"average_execution_time\": self.metrics.average_execution_time,\n                    \"total_execution_time\": self.metrics.total_execution_time,\n                    \"average_efficiency\": self.metrics.average_efficiency,\n                },\n                \"efficiency_trend\": list(self.efficiency_history),\n                \"recent_performance\": (\n                    list(self.performance_history)[-10:] if self.performance_history else []\n                ),\n                \"performance_indicators\": {\n                    \"high_efficiency_tasks\": sum(\n                        1 for p in self.performance_history if p.get(\"efficiency\", 0) > 0.8\n                    ),\n                    \"low_efficiency_tasks\": sum(\n                        1 for p in self.performance_history if p.get(\"efficiency\", 0) < 0.5\n                    ),\n                    \"average_task_duration\": self.metrics.average_execution_time,\n                },\n            }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\engines\\task_coordination_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            # Add to priority queue\n            self.priority_queues[task.priority].append(task)\n\n            # Simulate priority-based execution\n            await asyncio.sleep(0.15)  # Simulate processing time\n\n            return create_coordination_result(\n                task_id=task.task_id,\n                success=True,\n                result_data={\n                    \"strategy\": \"priority_based\",\n                    \"priority\": task.priority.value,\n                },\n            )\n\n        except Exception as e:",
            "        try:\n            # Simulate default execution\n            await asyncio.sleep(0.1)  # Simulate processing time\n\n            return create_coordination_result(\n                task_id=task.task_id,\n                success=True,\n                result_data={\"strategy\": \"default\", \"execution_mode\": \"standard\"},\n            )\n\n        except Exception as e:",
            "        try:\n            # Simulate parallel execution\n            await asyncio.sleep(0.1)  # Simulate processing time\n\n            return create_coordination_result(\n                task_id=task.task_id,\n                success=True,\n                result_data={\"strategy\": \"parallel\", \"execution_mode\": \"concurrent\"},\n            )\n\n        except Exception as e:",
            "        try:\n            # Simulate sequential execution\n            await asyncio.sleep(0.2)  # Simulate processing time\n\n            return create_coordination_result(\n                task_id=task.task_id,\n                success=True,\n                result_data={\"strategy\": \"sequential\", \"execution_mode\": \"ordered\"},\n            )\n\n        except Exception as e:",
            "        try:\n            if task.strategy == \"parallel\":\n                return await self._execute_parallel_strategy(task)\n            elif task.strategy == \"sequential\":\n                return await self._execute_sequential_strategy(task)\n            elif task.strategy == \"priority_based\":\n                return await self._execute_priority_based_strategy(task)\n            else:\n                return await self._execute_default_strategy(task)\n\n        except Exception as e:",
            "        try:\n            start_time = time.time()\n\n            # Add to active tasks\n            self.active_tasks[task.task_id] = task\n\n            # Execute task based on strategy\n            result = await self._execute_task_strategy(task)\n\n            # Calculate execution time\n            execution_time = time.time() - start_time\n\n            # Update result with timing\n            result.execution_time_seconds = execution_time\n            result.completed_at = datetime.now()\n\n            # Move to completed tasks\n            self.active_tasks.pop(task.task_id, None)\n            self.completed_tasks.append(task)\n            self.task_results[task.task_id] = result\n\n            self.logger.info(f\"Task {task.task_id} coordinated successfully\")\n            return result\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\coordination\\swarm\\orchestrators\\swarm_coordination_orchestrator.py",
          "count": 5,
          "lines": [
            "        try:\n            # Coordinate task through task engine\n            result = await self.task_engine.coordinate_task(task)\n\n            # Update performance metrics\n            self.performance_engine.update_metrics(result)\n\n            return result\n\n        except Exception as e:",
            "        try:\n            if not self.is_active:\n                self.logger.warning(\"Coordination system is not active\")\n                return True\n\n            self.is_active = False\n\n            self.logger.info(\"Swarm coordination system stopped\")\n            return True\n\n        except Exception as e:",
            "        try:\n            if self.is_active:\n                self.logger.warning(\"Coordination system is already active\")\n                return True\n\n            self.is_active = True\n            self.start_time = datetime.now()\n\n            self.logger.info(\"Swarm coordination system started\")\n            return True\n\n        except Exception as e:",
            "        try:\n            self.config.validate()\n        except Exception as e:",
            "try:\n    from ...utils.agent_matching import AgentMatchingUtils\n    from ...utils.coordination_utils import CoordinationUtils\n    from ...utils.performance_metrics import PerformanceMetricsUtils\n    from ...utils.vector_insights import VectorInsightsUtils\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\engines\\analysis_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            content = data.get(\"content\", \"\")\n            analysis_type = data.get(\"type\", \"general\")\n\n            # Simplified analysis logic\n            analysis_result = {\n                \"content_length\": len(content),\n                \"analysis_type\": analysis_type,\n                \"complexity_score\": len(content) / 1000,\n                \"issues_found\": 0,\n            }\n\n            return EngineResult(\n                success=True,\n                data=analysis_result,\n                metrics={\"content_size\": len(content)},\n            )\n        except Exception as e:",
            "        try:\n            content = data.get(\"content\", \"\")\n            pattern_type = data.get(\"pattern_type\", \"general\")\n\n            # Simplified pattern extraction\n            patterns = [\n                {\n                    \"type\": \"function\",\n                    \"count\": content.count(\"def \"),\n                    \"pattern_type\": pattern_type,\n                },\n                {\n                    \"type\": \"class\",\n                    \"count\": content.count(\"class \"),\n                    \"pattern_type\": pattern_type,\n                },\n                {\n                    \"type\": \"import\",\n                    \"count\": content.count(\"import \"),\n                    \"pattern_type\": pattern_type,\n                },\n            ]\n\n            self.patterns[pattern_type] = patterns\n\n            return EngineResult(\n                success=True,\n                data={\"patterns\": patterns},\n                metrics={\"patterns_found\": len(patterns)},\n            )\n        except Exception as e:",
            "        try:\n            content = data.get(\"content\", \"\")\n            violation_type = data.get(\"violation_type\", \"general\")\n\n            # Simplified violation detection\n            violations = []\n            if len(content) > 300:\n                violations.append(\n                    {\n                        \"type\": \"line_count\",\n                        \"severity\": \"high\",\n                        \"message\": \"File exceeds 300 lines\",\n                        \"line\": 0,\n                    }\n                )\n\n            if content.count(\"class \") > 5:\n                violations.append(\n                    {\n                        \"type\": \"class_count\",\n                        \"severity\": \"medium\",\n                        \"message\": \"Too many classes in file\",\n                        \"line\": 0,\n                    }\n                )\n\n            self.violations.extend(violations)\n\n            return EngineResult(\n                success=True,\n                data={\"violations\": violations},\n                metrics={\"violations_found\": len(violations)},\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"analyze\":\n                return self.analyze(context, payload)\n            elif operation == \"extract_patterns\":\n                return self.extract_patterns(context, payload)\n            elif operation == \"detect_violations\":\n                return self.detect_violations(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown analysis operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Analysis Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.patterns.clear()\n            self.violations.clear()\n            self.is_initialized = False\n            context.logger.info(\"Analysis Core Engine cleaned up\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\data_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            dataset_id = payload.get(\"dataset_id\", \"default\")\n\n            if dataset_id not in self.datasets:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Dataset {dataset_id} not found\",\n                )\n\n            return EngineResult(\n                success=True,\n                data=self.datasets[dataset_id],\n                metrics={\"dataset_id\": dataset_id},\n            )\n        except Exception as e:",
            "        try:\n            dataset_id = payload.get(\"dataset_id\", \"default\")\n            data = payload.get(\"data\", {})\n\n            self.datasets[dataset_id] = data\n\n            return EngineResult(\n                success=True,\n                data={\"dataset_id\": dataset_id, \"status\": \"stored\"},\n                metrics={\"data_size\": len(str(data))},\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"store\":\n                return self._store_data(context, payload)\n            elif operation == \"retrieve\":\n                return self._retrieve_data(context, payload)\n            elif operation == \"query\":\n                return self._query_data(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown data operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            query_id = payload.get(\"query_id\", f\"query_{len(self.queries)}\")\n            query = payload.get(\"query\", \"\")\n            dataset_id = payload.get(\"dataset_id\", \"default\")\n\n            # Simplified query logic\n            result = {\"query_id\": query_id, \"results\": [], \"count\": 0}\n            self.queries[query_id] = result\n\n            return EngineResult(success=True, data=result, metrics={\"query_id\": query_id})\n        except Exception as e:",
            "        try:\n            self.datasets.clear()\n            self.queries.clear()\n            self.is_initialized = False\n            context.logger.info(\"Data Core Engine cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Data Core Engine initialized\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\integration_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            connection_id = config.get(\"connection_id\", \"default\")\n            connection_type = config.get(\"type\", \"api\")\n            endpoint = config.get(\"endpoint\", \"\")\n\n            # Simplified connection logic\n            self.connections[connection_id] = {\n                \"type\": connection_type,\n                \"endpoint\": endpoint,\n                \"status\": \"connected\",\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            return EngineResult(\n                success=True,\n                data={\"connection_id\": connection_id, \"status\": \"connected\"},\n                metrics={\"connection_type\": connection_type},\n            )\n        except Exception as e:",
            "        try:\n            connection_id = data.get(\"connection_id\", \"default\")\n            sync_data = data.get(\"data\", {})\n\n            if connection_id not in self.connections:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Connection {connection_id} not found\",\n                )\n\n            # Simplified sync logic\n            sync_result = {\n                \"connection_id\": connection_id,\n                \"records_synced\": len(sync_data),\n                \"status\": \"synced\",\n            }\n\n            return EngineResult(\n                success=True,\n                data=sync_result,\n                metrics={\"records_synced\": len(sync_data)},\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"connect\":\n                return self.connect(context, payload)\n            elif operation == \"sync\":\n                return self.sync(context, payload)\n            elif operation == \"transform\":\n                return self.transform(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown integration operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self.connections.clear()\n            self.transforms.clear()\n            self.is_initialized = False\n            context.logger.info(\"Integration Core Engine cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Integration Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            transform_id = data.get(\"transform_id\", \"default\")\n            input_data = data.get(\"data\", {})\n            transform_type = data.get(\"type\", \"json\")\n\n            # Simplified transform logic\n            if transform_type == \"json\":\n                transformed = {\"transformed\": True, \"data\": input_data}\n            elif transform_type == \"xml\":\n                transformed = {\n                    \"transformed\": True,\n                    \"data\": f\"<root>{input_data}</root>\",\n                }\n            else:\n                transformed = {\"transformed\": True, \"data\": input_data}\n\n            self.transforms[transform_id] = transformed\n\n            return EngineResult(\n                success=True,\n                data=transformed,\n                metrics={\"transform_type\": transform_type},\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\ml_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            model_id = config.get(\"model_id\", \"default\")\n            optimization_params = config.get(\"params\", {})\n\n            if model_id not in self.models:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Model {model_id} not found\",\n                )\n\n            # Simplified optimization logic\n            self.models[model_id][\"optimized\"] = True\n\n            return EngineResult(\n                success=True,\n                data={\"model_id\": model_id, \"status\": \"optimized\"},\n                metrics={\"optimization_params\": len(optimization_params)},\n            )\n        except Exception as e:",
            "        try:\n            model_id = data.get(\"model_id\", \"default\")\n            training_data = data.get(\"data\", [])\n\n            # Simplified training logic\n            self.models[model_id] = {\"trained\": True, \"data_size\": len(training_data)}\n\n            return EngineResult(\n                success=True,\n                data={\"model_id\": model_id, \"status\": \"trained\"},\n                metrics={\"training_samples\": len(training_data)},\n            )\n        except Exception as e:",
            "        try:\n            model_id = input_data.get(\"model_id\", \"default\")\n            features = input_data.get(\"features\", [])\n\n            if model_id not in self.models:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Model {model_id} not found\",\n                )\n\n            # Simplified prediction logic\n            prediction = {\"prediction\": \"sample_output\", \"confidence\": 0.85}\n\n            return EngineResult(\n                success=True, data=prediction, metrics={\"features_count\": len(features)}\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"train\":\n                return self.train_model(context, payload)\n            elif operation == \"predict\":\n                return self.predict(context, payload)\n            elif operation == \"optimize\":\n                return self.optimize(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown ML operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"ML Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.models.clear()\n            self.is_initialized = False\n            context.logger.info(\"ML Core Engine cleaned up\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\monitoring_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            alert_id = f\"alert_{len(self.alerts)}\"\n            alert_type = payload.get(\"type\", \"info\")\n            message = payload.get(\"message\", \"\")\n            severity = payload.get(\"severity\", \"medium\")\n\n            alert_data = {\n                \"alert_id\": alert_id,\n                \"type\": alert_type,\n                \"message\": message,\n                \"severity\": severity,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n                \"status\": \"active\",\n            }\n\n            self.alerts.append(alert_data)\n\n            return EngineResult(success=True, data=alert_data, metrics={\"alert_id\": alert_id})\n        except Exception as e:",
            "        try:\n            component = payload.get(\"component\", \"system\")\n\n            # Simplified health check\n            health_status = {\n                \"component\": component,\n                \"status\": \"healthy\",\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n                \"metrics\": {\n                    \"cpu_usage\": 25.5,\n                    \"memory_usage\": 60.2,\n                    \"disk_usage\": 45.8,\n                },\n            }\n\n            return EngineResult(success=True, data=health_status, metrics={\"component\": component})\n        except Exception as e:",
            "        try:\n            metric_name = payload.get(\"metric_name\", \"default\")\n            metric_value = payload.get(\"value\", 0)\n            metric_type = payload.get(\"type\", \"counter\")\n\n            metric_data = {\n                \"name\": metric_name,\n                \"value\": metric_value,\n                \"type\": metric_type,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.metrics[metric_name] = metric_data\n\n            return EngineResult(\n                success=True, data=metric_data, metrics={\"metric_name\": metric_name}\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"collect_metrics\":\n                return self._collect_metrics(context, payload)\n            elif operation == \"check_health\":\n                return self._check_health(context, payload)\n            elif operation == \"create_alert\":\n                return self._create_alert(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown monitoring operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Monitoring Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.metrics.clear()\n            self.alerts.clear()\n            self.is_initialized = False\n            context.logger.info(\"Monitoring Core Engine cleaned up\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\orchestration_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            coordination_id = f\"coord_{len(self.executions)}\"\n            components = payload.get(\"components\", [])\n\n            # Simplified coordination\n            coordination_result = {\n                \"coordination_id\": coordination_id,\n                \"components_coordinated\": len(components),\n                \"status\": \"coordinated\",\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.executions.append(coordination_result)\n\n            return EngineResult(\n                success=True,\n                data=coordination_result,\n                metrics={\"coordination_id\": coordination_id},\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"orchestrate\":\n                return self._orchestrate(context, payload)\n            elif operation == \"execute_workflow\":\n                return self._execute_workflow(context, payload)\n            elif operation == \"coordinate\":\n                return self._coordinate(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown orchestration operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            orchestration_id = f\"orch_{len(self.executions)}\"\n            operations = payload.get(\"operations\", [])\n\n            # Simplified orchestration\n            orchestration_result = {\n                \"orchestration_id\": orchestration_id,\n                \"operations_count\": len(operations),\n                \"status\": \"completed\",\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            return EngineResult(\n                success=True,\n                data=orchestration_result,\n                metrics={\"orchestration_id\": orchestration_id},\n            )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Orchestration Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.workflows.clear()\n            self.executions.clear()\n            self.is_initialized = False\n            context.logger.info(\"Orchestration Core Engine cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            workflow_id = payload.get(\"workflow_id\", f\"workflow_{len(self.workflows)}\")\n            steps = payload.get(\"steps\", [])\n\n            # Simplified workflow execution\n            workflow_result = {\n                \"workflow_id\": workflow_id,\n                \"steps_executed\": len(steps),\n                \"status\": \"completed\",\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.workflows[workflow_id] = workflow_result\n\n            return EngineResult(\n                success=True, data=workflow_result, metrics={\"workflow_id\": workflow_id}\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\performance_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            benchmark_id = payload.get(\"benchmark_id\", f\"bench_{len(self.benchmarks)}\")\n            test_data = payload.get(\"test_data\", {})\n\n            # Simplified benchmarking\n            benchmark_result = {\n                \"benchmark_id\": benchmark_id,\n                \"execution_time\": 0.125,\n                \"memory_usage\": 1024,\n                \"cpu_usage\": 15.5,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.benchmarks[benchmark_id] = benchmark_result\n\n            return EngineResult(\n                success=True,\n                data=benchmark_result,\n                metrics={\"benchmark_id\": benchmark_id},\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"benchmark\":\n                return self._benchmark(context, payload)\n            elif operation == \"optimize\":\n                return self._optimize(context, payload)\n            elif operation == \"profile\":\n                return self._profile(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown performance operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            optimization_id = f\"opt_{len(self.optimizations)}\"\n            target = payload.get(\"target\", \"general\")\n            optimization_type = payload.get(\"type\", \"memory\")\n\n            # Simplified optimization\n            optimization_result = {\n                \"optimization_id\": optimization_id,\n                \"target\": target,\n                \"type\": optimization_type,\n                \"improvement\": 15.5,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.optimizations.append(optimization_result)\n\n            return EngineResult(\n                success=True,\n                data=optimization_result,\n                metrics={\"optimization_id\": optimization_id},\n            )\n        except Exception as e:",
            "        try:\n            profile_id = f\"profile_{len(self.optimizations)}\"\n            component = payload.get(\"component\", \"system\")\n\n            # Simplified profiling\n            profile_result = {\n                \"profile_id\": profile_id,\n                \"component\": component,\n                \"metrics\": {\n                    \"execution_time\": 0.250,\n                    \"memory_peak\": 2048,\n                    \"cpu_peak\": 25.0,\n                },\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            return EngineResult(\n                success=True, data=profile_result, metrics={\"profile_id\": profile_id}\n            )\n        except Exception as e:",
            "        try:\n            self.benchmarks.clear()\n            self.optimizations.clear()\n            self.is_initialized = False\n            context.logger.info(\"Performance Core Engine cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Performance Core Engine initialized\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\processing_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            batch_id = f\"batch_{len(self.jobs)}\"\n            items = payload.get(\"items\", [])\n            processor_type = payload.get(\"processor_type\", \"general\")\n\n            # Simplified batch processing\n            batch_result = {\n                \"batch_id\": batch_id,\n                \"items_processed\": len(items),\n                \"processor_type\": processor_type,\n                \"success_rate\": 95.5,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            return EngineResult(success=True, data=batch_result, metrics={\"batch_id\": batch_id})\n        except Exception as e:",
            "        try:\n            job_id = f\"job_{len(self.jobs)}\"\n            job_type = payload.get(\"job_type\", \"general\")\n            priority = payload.get(\"priority\", \"normal\")\n\n            # Simplified job queuing\n            job = {\n                \"job_id\": job_id,\n                \"job_type\": job_type,\n                \"priority\": priority,\n                \"status\": \"queued\",\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.jobs.append(job)\n\n            return EngineResult(success=True, data=job, metrics={\"job_id\": job_id})\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"process\":\n                return self._process(context, payload)\n            elif operation == \"batch_process\":\n                return self._batch_process(context, payload)\n            elif operation == \"queue_job\":\n                return self._queue_job(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown processing operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            processor_id = payload.get(\"processor_id\", \"default\")\n            data = payload.get(\"data\", {})\n            process_type = payload.get(\"type\", \"general\")\n\n            # Simplified processing\n            process_result = {\n                \"processor_id\": processor_id,\n                \"processed\": True,\n                \"type\": process_type,\n                \"input_size\": len(str(data)),\n                \"output_size\": len(str(data)) * 0.8,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.processors[processor_id] = process_result\n\n            return EngineResult(\n                success=True,\n                data=process_result,\n                metrics={\"processor_id\": processor_id},\n            )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Processing Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.processors.clear()\n            self.jobs.clear()\n            self.is_initialized = False\n            context.logger.info(\"Processing Core Engine cleaned up\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\security_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            event = payload.get(\"event\", \"unknown\")\n            user_id = payload.get(\"user_id\", \"system\")\n            details = payload.get(\"details\", {})\n\n            audit_entry = {\n                \"event\": event,\n                \"user_id\": user_id,\n                \"details\": details,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n                \"audit_id\": f\"audit_{len(self.audit_logs)}\",\n            }\n\n            self.audit_logs.append(audit_entry)\n\n            return EngineResult(success=True, data=audit_entry, metrics={\"event\": event})\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"authenticate\":\n                return self._authenticate(context, payload)\n            elif operation == \"authorize\":\n                return self._authorize(context, payload)\n            elif operation == \"audit\":\n                return self._audit(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown security operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Security Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.permissions.clear()\n            self.audit_logs.clear()\n            self.is_initialized = False\n            context.logger.info(\"Security Core Engine cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            user_id = payload.get(\"user_id\", \"anonymous\")\n            action = payload.get(\"action\", \"read\")\n            resource = payload.get(\"resource\", \"default\")\n\n            # Simplified authorization\n            authz_result = {\n                \"user_id\": user_id,\n                \"action\": action,\n                \"resource\": resource,\n                \"authorized\": True,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            return EngineResult(success=True, data=authz_result, metrics={\"user_id\": user_id})\n        except Exception as e:",
            "        try:\n            user_id = payload.get(\"user_id\", \"anonymous\")\n            credentials = payload.get(\"credentials\", {})\n\n            # Simplified authentication\n            auth_result = {\n                \"user_id\": user_id,\n                \"authenticated\": True,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n                \"session_id\": f\"session_{user_id}\",\n            }\n\n            return EngineResult(success=True, data=auth_result, metrics={\"user_id\": user_id})\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\storage_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            cache_key = payload.get(\"key\", \"default\")\n            data = payload.get(\"data\", {})\n            ttl = payload.get(\"ttl\", 3600)\n\n            # Simplified caching\n            cache_entry = {\n                \"key\": cache_key,\n                \"data\": data,\n                \"ttl\": ttl,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.cache[cache_key] = cache_entry\n\n            return EngineResult(success=True, data=cache_entry, metrics={\"cache_key\": cache_key})\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"store\":\n                return self._store(context, payload)\n            elif operation == \"retrieve\":\n                return self._retrieve(context, payload)\n            elif operation == \"cache\":\n                return self._cache(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown storage operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Storage Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.stores.clear()\n            self.cache.clear()\n            self.is_initialized = False\n            context.logger.info(\"Storage Core Engine cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            store_id = payload.get(\"store_id\", \"default\")\n\n            if store_id not in self.stores:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Store {store_id} not found\",\n                )\n\n            data = self.stores[store_id]\n\n            return EngineResult(success=True, data=data, metrics={\"store_id\": store_id})\n        except Exception as e:",
            "        try:\n            store_id = payload.get(\"store_id\", \"default\")\n            data = payload.get(\"data\", {})\n            storage_type = payload.get(\"type\", \"memory\")\n\n            # Simplified storage\n            store_result = {\n                \"store_id\": store_id,\n                \"stored\": True,\n                \"type\": storage_type,\n                \"size\": len(str(data)),\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            self.stores[store_id] = data\n\n            return EngineResult(success=True, data=store_result, metrics={\"store_id\": store_id})\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\utility_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"process\":\n                return self.process(context, payload)\n            elif operation == \"validate\":\n                return self.validate(context, payload)\n            elif operation == \"transform\":\n                return self.transform(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown utility operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            processor_id = data.get(\"processor_id\", \"default\")\n            input_data = data.get(\"data\", {})\n            process_type = data.get(\"type\", \"general\")\n\n            # Simplified processing logic\n            if process_type == \"format\":\n                processed = {\"formatted\": True, \"data\": str(input_data)}\n            elif process_type == \"normalize\":\n                processed = {\"normalized\": True, \"data\": input_data}\n            elif process_type == \"sanitize\":\n                processed = {\"sanitized\": True, \"data\": str(input_data).strip()}\n            else:\n                processed = {\"processed\": True, \"data\": input_data}\n\n            self.processors[processor_id] = processed\n\n            return EngineResult(\n                success=True, data=processed, metrics={\"process_type\": process_type}\n            )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Utility Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.processors.clear()\n            self.validators.clear()\n            self.transformers.clear()\n            self.is_initialized = False\n            context.logger.info(\"Utility Core Engine cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            transformer_id = data.get(\"transformer_id\", \"default\")\n            input_data = data.get(\"data\", {})\n            transform_type = data.get(\"type\", \"general\")\n\n            # Simplified transformation logic\n            if transform_type == \"uppercase\":\n                transformed = {\"transformed\": True, \"data\": str(input_data).upper()}\n            elif transform_type == \"lowercase\":\n                transformed = {\"transformed\": True, \"data\": str(input_data).lower()}\n            elif transform_type == \"json\":\n                transformed = {\"transformed\": True, \"data\": str(input_data)}\n            else:\n                transformed = {\"transformed\": True, \"data\": input_data}\n\n            self.transformers[transformer_id] = transformed\n\n            return EngineResult(\n                success=True,\n                data=transformed,\n                metrics={\"transform_type\": transform_type},\n            )\n        except Exception as e:",
            "        try:\n            validator_id = data.get(\"validator_id\", \"default\")\n            input_data = data.get(\"data\", {})\n            validation_rules = data.get(\"rules\", [])\n\n            # Simplified validation logic\n            validation_result = {\"valid\": True, \"errors\": [], \"warnings\": []}\n\n            for rule in validation_rules:\n                if rule == \"required\" and not input_data:\n                    validation_result[\"valid\"] = False\n                    validation_result[\"errors\"].append(\"Field is required\")\n                elif rule == \"length\" and len(str(input_data)) > 100:\n                    validation_result[\"warnings\"].append(\"Field is too long\")\n\n            self.validators[validator_id] = validation_result\n\n            return EngineResult(\n                success=validation_result[\"valid\"],\n                data=validation_result,\n                metrics={\"rules_checked\": len(validation_rules)},\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\validation_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            data = payload.get(\"data\", {})\n            rules = payload.get(\"rules\", [])\n\n            validation_result = {\n                \"valid\": True,\n                \"errors\": [],\n                \"warnings\": [],\n                \"rules_checked\": len(rules),\n            }\n\n            for rule in rules:\n                if rule == \"required\" and not data:\n                    validation_result[\"valid\"] = False\n                    validation_result[\"errors\"].append(\"Data is required\")\n                elif rule == \"v2_compliance\" and len(str(data)) > 300:\n                    validation_result[\"valid\"] = False\n                    validation_result[\"errors\"].append(\"V2 compliance violation: exceeds 300 lines\")\n                elif rule == \"format\" and not isinstance(data, dict):\n                    validation_result[\"warnings\"].append(\"Data format warning\")\n\n            self.validations.append(validation_result)\n\n            return EngineResult(\n                success=validation_result[\"valid\"],\n                data=validation_result,\n                metrics={\"rules_checked\": len(rules)},\n            )\n        except Exception as e:",
            "        try:\n            data = payload.get(\"data\", {})\n            standard = payload.get(\"standard\", \"v2\")\n\n            compliance_result = {\n                \"compliant\": True,\n                \"standard\": standard,\n                \"violations\": [],\n                \"score\": 100,\n            }\n\n            if standard == \"v2\":\n                if len(str(data)) > 300:\n                    compliance_result[\"compliant\"] = False\n                    compliance_result[\"violations\"].append(\"Line count exceeds 300\")\n                    compliance_result[\"score\"] -= 20\n\n                if \"class \" in str(data) and str(data).count(\"class \") > 5:\n                    compliance_result[\"violations\"].append(\"Too many classes\")\n                    compliance_result[\"score\"] -= 10\n\n            return EngineResult(\n                success=compliance_result[\"compliant\"],\n                data=compliance_result,\n                metrics={\"standard\": standard},\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"validate\":\n                return self._validate_data(context, payload)\n            elif operation == \"add_rule\":\n                return self._add_rule(context, payload)\n            elif operation == \"check_compliance\":\n                return self._check_compliance(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown validation operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            rule_id = payload.get(\"rule_id\", f\"rule_{len(self.rules)}\")\n            rule_definition = payload.get(\"rule\", {})\n\n            self.rules[rule_id] = rule_definition\n\n            return EngineResult(\n                success=True,\n                data={\"rule_id\": rule_id, \"status\": \"added\"},\n                metrics={\"rules_count\": len(self.rules)},\n            )\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Validation Core Engine initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            self.rules.clear()\n            self.validations.clear()\n            self.is_initialized = False\n            context.logger.info(\"Validation Core Engine cleaned up\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\base_engine.py",
          "count": 2,
          "lines": [
            "        try:\n            self.is_active = False\n            self.logger.info(f\"{self.__class__.__name__} stopped\")\n            return True\n        except Exception as e:",
            "        try:\n            self.is_active = True\n            self.start_time = datetime.now()\n            self.logger.info(f\"{self.__class__.__name__} started\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\communication_core_engine.py",
          "count": 6,
          "lines": [
            "        try:\n            channel_id = payload.get(\"channel_id\", \"default\")\n\n            # Simplified receive logic - return last message\n            if self.messages:\n                last_message = self.messages[-1]\n                return EngineResult(\n                    success=True, data=last_message, metrics={\"channel_id\": channel_id}\n                )\n            else:\n                return EngineResult(\n                    success=True,\n                    data={\"message\": \"No messages available\"},\n                    metrics={\"channel_id\": channel_id},\n                )\n        except Exception as e:",
            "        try:\n            channel_id = payload.get(\"channel_id\", \"default\")\n            message = payload.get(\"message\", \"\")\n            recipient = payload.get(\"recipient\", \"unknown\")\n\n            message_data = {\n                \"channel_id\": channel_id,\n                \"message\": message,\n                \"recipient\": recipient,\n                \"timestamp\": context.metrics.get(\"timestamp\", 0),\n                \"status\": \"sent\",\n            }\n\n            self.messages.append(message_data)\n\n            return EngineResult(success=True, data=message_data, metrics={\"channel_id\": channel_id})\n        except Exception as e:",
            "        try:\n            message = payload.get(\"message\", \"\")\n            channels = payload.get(\"channels\", [\"default\"])\n\n            broadcast_results = []\n            for channel_id in channels:\n                message_data = {\n                    \"channel_id\": channel_id,\n                    \"message\": message,\n                    \"timestamp\": context.metrics.get(\"timestamp\", 0),\n                    \"status\": \"broadcast\",\n                }\n                broadcast_results.append(message_data)\n                self.messages.append(message_data)\n\n            return EngineResult(\n                success=True,\n                data={\"broadcast_results\": broadcast_results},\n                metrics={\"channels_broadcast\": len(channels)},\n            )\n        except Exception as e:",
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n\n            if operation == \"send\":\n                return self._send_message(context, payload)\n            elif operation == \"receive\":\n                return self._receive_message(context, payload)\n            elif operation == \"broadcast\":\n                return self._broadcast_message(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown communication operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self.channels.clear()\n            self.messages.clear()\n            self.is_initialized = False\n            context.logger.info(\"Communication Core Engine cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(\"Communication Core Engine initialized\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\coordination_core_engine.py",
          "count": 4,
          "lines": [
            "        try:\n            coordination_id = f\"coord_{len(self.tasks)}\"\n            results = []\n\n            for i, task in enumerate(tasks):\n                task_id = task.get(\"id\", f\"task_{i}\")\n                task_type = task.get(\"type\", \"unknown\")\n\n                # Simplified task coordination\n                task_result = {\n                    \"task_id\": task_id,\n                    \"type\": task_type,\n                    \"status\": \"completed\",\n                    \"priority\": task.get(\"priority\", \"normal\"),\n                }\n                results.append(task_result)\n                self.tasks[task_id] = task_result\n\n            return EngineResult(\n                success=True,\n                data={\"coordination_id\": coordination_id, \"results\": results},\n                metrics={\"tasks_coordinated\": len(tasks)},\n            )\n        except Exception as e:",
            "        try:\n            monitor_id = f\"monitor_{len(self.monitors)}\"\n            statuses = []\n\n            for target in targets:\n                # Simplified monitoring logic\n                status = {\n                    \"target\": target,\n                    \"status\": \"healthy\",\n                    \"last_check\": context.metrics.get(\"timestamp\", 0),\n                }\n                statuses.append(status)\n                self.monitors[target] = status\n\n            return EngineResult(\n                success=True,\n                data={\"monitor_id\": monitor_id, \"statuses\": statuses},\n                metrics={\"targets_monitored\": len(targets)},\n            )\n        except Exception as e:",
            "        try:\n            schedule_id = schedule.get(\"schedule_id\", f\"schedule_{len(self.schedules)}\")\n            tasks = schedule.get(\"tasks\", [])\n            timing = schedule.get(\"timing\", \"immediate\")\n\n            # Simplified scheduling logic\n            self.schedules[schedule_id] = {\n                \"tasks\": tasks,\n                \"timing\": timing,\n                \"status\": \"scheduled\",\n                \"created_at\": context.metrics.get(\"timestamp\", 0),\n            }\n\n            return EngineResult(\n                success=True,\n                data={\"schedule_id\": schedule_id, \"status\": \"scheduled\"},\n                metrics={\"tasks_scheduled\": len(tasks)},\n            )\n        except Exception as e:",
            "        try:\n            self.tasks.clear()\n            self.schedules.clear()\n            self.monitors.clear()\n            return self._base._standard_cleanup(context, \"Coordination Core Engine\")\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_base_helpers.py",
          "count": 3,
          "lines": [
            "        try:\n            operation = payload.get(\"operation\", \"unknown\")\n            handler = operation_map.get(operation)\n            \n            if handler:\n                return handler(context, payload)\n            else:\n                return EngineResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"{default_error}: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self.is_initialized = False\n            context.logger.info(f\"{engine_name} cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            self.is_initialized = True\n            context.logger.info(f\"{engine_name} initialized\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_lifecycle.py",
          "count": 2,
          "lines": [
            "        try:\n            self.initialized_at = datetime.now()\n            self.last_operation_at = datetime.now()\n\n            return EngineResult(\n                success=True,\n                engine_id=self.engine_id,\n                operation=\"initialize\",\n                data={\"initialized_at\": self.initialized_at.isoformat()},\n                metadata={\"lifecycle\": \"initialized\"}\n            )\n        except Exception as e:",
            "        try:\n            self.last_operation_at = datetime.now()\n\n            return EngineResult(\n                success=True,\n                engine_id=self.engine_id,\n                operation=\"shutdown\",\n                data={\"shutdown_at\": datetime.now().isoformat()},\n                metadata={\"lifecycle\": \"shutdown\"}\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\engines\\engine_monitoring.py",
          "count": 2,
          "lines": [
            "        try:\n            cpu_percent = psutil.cpu_percent(interval=0.1)\n            memory_info = psutil.virtual_memory()\n\n            self.cpu_usage_history.append(cpu_percent)\n            self.memory_usage_history.append(memory_info.percent)\n\n            # Prevent memory leak: keep only last 100 readings (more efficient trimming)\n            if len(self.cpu_usage_history) > 100:\n                self.cpu_usage_history = self.cpu_usage_history[-100:]\n                self.memory_usage_history = self.memory_usage_history[-100:]\n\n            # Update peak memory\n            if memory_info.percent > self.peak_memory_usage:\n                self.peak_memory_usage = memory_info.percent\n\n        except Exception:",
            "try:\n    import psutil\n\n    PSUTIL_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\engines\\registry.py",
          "count": 6,
          "lines": [
            "                try:\n                    module = importlib.import_module(f'{package_name}.{name}')\n                    engine_class = self._find_engine_class(module, Engine)\n                    \n                    if engine_class:\n                        engine_type = name.replace('_core_engine', '')\n                        self._engines[engine_type] = engine_class\n                        discovered_count += 1\n                        logger.info(\n                            f\" Discovered engine: {engine_type} \"\n                            f\"({engine_class.__name__})\"\n                        )\n                    else:\n                        logger.warning(\n                            f\" Module {name} found but no Engine implementation\"\n                        )\n                        \n                except (ImportError, AttributeError) as e:",
            "            try:\n                attr = getattr(module, attr_name)\n                \n                # Check if it's a class\n                if not isinstance(attr, type):\n                    continue\n                \n                # Check if it has all required methods (Protocol compliance)\n                has_all_methods = all(\n                    hasattr(attr, method) and callable(getattr(attr, method))\n                    for method in required_methods\n                )\n                \n                if has_all_methods:\n                    return attr\n                    \n            except (TypeError, AttributeError):",
            "            try:\n                engine = self.get_engine(engine_type)\n                results[engine_type] = engine.initialize(context)\n            except Exception as e:",
            "            try:\n                results[engine_type] = engine.cleanup(context)\n            except Exception as e:",
            "            try:\n                status[engine_type] = engine.get_status()\n            except Exception as e:",
            "class EngineRegistry:\n    \"\"\"\n    Registry for all core engines - SSOT for engine management.\n    \n    Uses Plugin Discovery Pattern to auto-discover engines implementing\n    the Engine protocol, eliminating circular dependencies.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize registry with auto-discovery.\"\"\"\n        self._engines: dict[str, Type[Any]] = {}\n        self._instances: dict[str, object] = {}\n        self._discover_engines()\n\n    def _discover_engines(self) -> None:\n        \"\"\"\n        Auto-discover engines implementing Engine protocol.\n        \n        Uses pkgutil and importlib to scan for engine modules and\n        register classes that implement the Engine protocol.\n        \"\"\"\n        from .contracts import Engine\n        \n        package_path = Path(__file__).parent\n        # Get package name from __package__ or derive from file path\n        if __package__:\n            package_name = __package__\n        else:\n            # Fallback: derive from file path\n            # __file__ is src/core/engines/registry.py\n            # Package is src.core.engines\n            parts = Path(__file__).parts\n            if 'src' in parts:\n                src_idx = parts.index('src')\n                package_name = '.'.join(parts[src_idx:-1])\n            else:\n                package_name = \"src.core.engines\"\n        \n        discovered_count = 0\n        failed_count = 0\n        \n        logger.info(\" Starting engine discovery...\")\n        \n        try:\n            modules = list(pkgutil.iter_modules([str(package_path)]))\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_reporting_utilities.py",
          "count": 5,
          "lines": [
            "    try:\n        reporter = get_error_reporter()\n\n        if component:\n            if component in reporter.reports:\n                reporter.reports[component].clear_errors()\n                logger.info(f\"Cleared error report for component: {component}\")\n            else:\n                logger.warning(f\"No report found for component: {component}\")\n        else:\n            reporter.clear_all_reports()\n            logger.info(\"Cleared all error reports\")\n    except Exception as e:",
            "    try:\n        reporter = get_error_reporter()\n\n        if component:\n            report = reporter.get_report(component)\n            if report:\n                return report.get_detailed_report()\n            else:\n                return {\"error\": f\"No report found for component: {component}\"}\n        else:\n            return reporter.get_global_summary()\n    except Exception as e:",
            "    try:\n        reporter = get_error_reporter()\n        reporter.add_error_to_report(component, error)\n        logger.info(f\"Reported error to {component}: {getattr(error, 'error_id', 'unknown')}\")\n    except Exception as e:",
            "    try:\n        reporter = get_error_reporter()\n        return reporter.create_report(component, time_range)\n    except Exception as e:",
            "    try:\n        reporter = get_error_reporter()\n        return reporter.get_error_statistics()\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_utilities_core.py",
          "count": 1,
          "lines": [
            "try:\n    from ..utilities.standardized_logging import LogLevel\n\n    _LOGGING_AVAILABLE = True\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\__init__.py",
          "count": 1,
          "lines": [
            "try:\n    from .circuit_breaker.implementation import CircuitBreaker\n    from .circuit_breaker.protocol import ICircuitBreaker\n    from .circuit_breaker.provider import CircuitBreakerProvider\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\component_management.py",
          "count": 3,
          "lines": [
            "        try:\n            health = intelligence_engine.get_component_health(component)\n            metrics[\"intelligence_health\"] = health\n        except Exception as e:",
            "        try:\n            health_report = intelligence_engine.get_component_health(component)\n        except Exception as e:",
            "        try:\n            intelligence_report = intelligence_engine.get_system_intelligence_report()\n\n            return {\n                \"system_health\": {\n                    \"total_errors\": intelligence_report[\"summary\"][\"total_errors\"],\n                    \"critical_errors\": intelligence_report[\"summary\"][\"critical_errors\"],\n                    \"components_tracked\": intelligence_report[\"summary\"][\"components_tracked\"],\n                    \"high_risk_components\": len(intelligence_report[\"high_risk_components\"]),\n                },\n                \"intelligence\": intelligence_report,\n                \"circuit_breakers\": {\n                    name: breaker.state.value for name, breaker in self.circuit_breakers.items()\n                },\n                \"retry_mechanisms\": list(self.retry_mechanisms.keys()),\n                \"recovery_strategies\": [s.name for s in self.recovery_strategies],\n            }\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_execution.py",
          "count": 4,
          "lines": [
            "        try:\n            error_type = type(error).__name__\n            severity, category = self.classifier.classify_error(error)\n            intelligence_engine.record_error(\n                error_type=error_type,\n                component=component,\n                severity=severity.value,\n                context={\"operation\": operation_name, \"category\": category.value},\n            )\n        except Exception as e:",
            "        try:\n            if strategy.execute({\"component\": component, \"error_type\": error_type}):\n                logger.info(f\"Recovery successful with strategy: {strategy.name}\")\n                return True\n        except Exception as e:",
            "        try:\n            return self._execute_operation_with_handling(\n                operation, retry_mechanism, operation_name, component, use_intelligence\n            )\n        except Exception as e:",
            "        try:\n            risk_score, risk_level = intelligence_engine.predict_failure_risk(component)\n            if risk_level in (\"high\", \"critical\"):\n                logger.warning(\n                    f\"High failure risk detected for {component}: {risk_score:.2f} ({risk_level})\"\n                )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\recovery_strategies.py",
          "count": 7,
          "lines": [
            "        try:\n            logger.info(f\"Executing configuration reset for {error_context.operation}\")\n            success = self.config_reset_func()\n            if success:\n                logger.info(f\"Configuration reset successful for {error_context.operation}\")\n            return success\n        except Exception as e:",
            "        try:\n            logger.info(f\"Executing fallback strategy for {error_context.operation}\")\n            result = self.fallback_func()\n            logger.info(f\"Fallback successful for {error_context.operation}\")\n            return True\n        except Exception as e:",
            "        try:\n            logger.info(f\"Executing graceful degradation for {error_context.operation}\")\n            result = self.degraded_func()\n            logger.info(f\"Graceful degradation successful for {error_context.operation}\")\n            return True\n        except Exception as e:",
            "        try:\n            logger.info(f\"Executing resource cleanup for {error_context.operation}\")\n            success = self.cleanup_func()\n            if success:\n                logger.info(f\"Resource cleanup successful for {error_context.operation}\")\n            return success\n        except Exception as e:",
            "        try:\n            logger.info(f\"Executing retry strategy for {error_context.operation}\")\n            for attempt in range(self.max_retries):\n                delay = self.base_delay * (2 ** attempt)\n                if attempt > 0:\n                    logger.info(f\"Retry attempt {attempt + 1}/{self.max_retries} after {delay:.1f}s delay\")\n                    time.sleep(delay)\n                try:\n                    result = self.operation_func()\n                    logger.info(f\"Retry successful on attempt {attempt + 1} for {error_context.operation}\")\n                    return True\n                except Exception as retry_error:",
            "        try:\n            logger.info(f\"Executing service restart for {error_context.operation}\")\n            success = self.service_manager()\n            if success:\n                self.last_restart = datetime.now()\n                logger.info(f\"Service restart successful for {error_context.operation}\")\n            return success\n        except Exception as e:",
            "        try:\n            logger.info(f\"Executing timeout strategy for {error_context.operation} (timeout: {self.extended_timeout}s)\")\n            # Set up timeout\n            signal.signal(signal.SIGALRM, timeout_handler)\n            signal.alarm(int(self.extended_timeout))\n\n            try:\n                result = self.operation_func()\n                signal.alarm(0)  # Cancel timeout\n                logger.info(f\"Timeout strategy successful for {error_context.operation}\")\n                return True\n            except TimeoutError:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\retry_mechanisms.py",
          "count": 1,
          "lines": [
            "            try:\n                return func(*args, **kwargs)\n            except self.config.exceptions as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\retry_safety_engine.py",
          "count": 5,
          "lines": [
            "            try:\n                self._log_retry_attempt(attempt, config.max_retries, logger)\n                return operation_func()\n            except config.exceptions as e:",
            "        try:\n            result = operation_func()\n            signal.alarm(0)\n            return result\n        finally:\n            signal.alarm(0)\n\n    def _handle_timeout(\n        self, error: TimeoutError, logger: logging.Logger | None, default: Any\n    ) -> Any:\n        \"\"\"Handle timeout error.\"\"\"\n        if logger:\n            self._get_logger().warning(f\" Operation timed out: {error}\")\n        return default\n\n    def _handle_timeout_error(\n        self, error: Exception, logger: logging.Logger | None, default: Any\n    ) -> Any:\n        \"\"\"Handle timeout execution error.\"\"\"\n        if logger:\n            self._get_logger().error(f\" Operation failed: {error}\")\n        return default\n\n    def circuit_breaker_execute(\n        self,\n        operation_func: Callable,\n        failure_threshold: int = 5,\n        recovery_timeout: float = 60.0,\n        logger: logging.Logger | None = None,\n    ) -> Any:\n        \"\"\"Execute operation with circuit breaker pattern.\n\n        Args:\n            operation_func: Function to execute\n            failure_threshold: Number of failures before opening circuit\n            recovery_timeout: Time to wait before attempting recovery\n            logger: Optional logger\n\n        Returns:\n            Any: Result of operation\n\n        Raises:\n            Exception: If circuit is open or operation fails\n        \"\"\"\n        # Simple circuit breaker implementation\n        # In production, this would use a more sophisticated state machine\n        try:\n            return operation_func()\n        except Exception as e:",
            "        try:\n            return operation_func()\n        except Exception as e:",
            "        try:\n            return self._execute_with_signal_timeout(operation_func, timeout, timeout_handler)\n        except TimeoutError as e:",
            "        try:\n            self._validate_input(validation_func, error_message)\n            return operation_func()\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\error_handling.py",
          "count": 7,
          "lines": [
            "            try:\n                return func(*args, **kwargs)\n            except Exception as e:",
            "        try:\n            strategy = self._recovery_strategies[report.category]\n            result = strategy(report)\n\n            if result:\n                report.resolved = True\n                report.resolved_at = datetime.now()\n                self.logger.info(f\" Auto-recovered error {report.error_id}\")\n            else:\n                report.recovery_attempts += 1\n                self.logger.warning(f\" Recovery failed for error {report.error_id}\")\n\n        except Exception as recovery_error:",
            "    try:\n        file_path = Path(file_path)\n        if not file_path.exists():\n            return False, f\"File not found: {file_path}\"\n\n        with open(file_path, 'r', encoding='utf-8') as f:\n            source = f.read()\n\n        compile(source, str(file_path), 'exec')\n        return True, None\n\n    except SyntaxError as e:",
            "    try:\n        if data is None:\n            return False\n\n        # Basic type checks\n        if not isinstance(data, (dict, list)):\n            return False\n\n        # Schema validation if provided\n        if schema:\n            return _validate_against_schema(data, schema)\n\n        return True\n\n    except Exception as e:",
            "    try:\n        if data is None:\n            return default\n        return data.get(key, default)\n    except (TypeError, AttributeError) as e:",
            "    try:\n        if items is None or not isinstance(items, list):\n            return default\n        return items[index] if 0 <= index < len(items) else default\n    except (IndexError, TypeError) as e:",
            "    try:\n        yield context\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\unified_error_handler.py",
          "count": 7,
          "lines": [
            "            try:\n                result = await func(*args, **kwargs)\n\n                if log_end:\n                    duration = time.time() - start_time\n                    logger.info(f\"Completed operation: {op_name} in {duration:.3f}s\")\n\n                return result\n\n            except Exception as e:",
            "            try:\n                result = func(*args, **kwargs)\n\n                if log_end:\n                    duration = time.time() - start_time\n                    logger.info(f\"Completed operation: {op_name} in {duration:.3f}s\")\n\n                return result\n\n            except Exception as e:",
            "            try:\n                return await func(*args, **kwargs)\n            except Exception as e:",
            "            try:\n                return func(*args, **kwargs)\n            except Exception as e:",
            "        try:\n            return await func(*args, **kwargs)\n        except exceptions as e:",
            "        try:\n            return func(*args, **kwargs)\n        except exceptions as e:",
            "    try:\n        yield\n        duration = time.time() - start_time\n        logger.info(f\"Completed operation: {operation_name} in {duration:.3f}s\")\n\n    except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\error_handling\\circuit_breaker\\implementation.py",
          "count": 1,
          "lines": [
            "        try:\n            result = func(*args, **kwargs)\n            self._on_success()\n            return result\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_engine_platform.py",
          "count": 4,
          "lines": [
            "        try:\n            if lock_file.exists():\n                lock_file.unlink()\n                if self.logger:\n                    self.logger.info(f\"Removed stale lock file: {lock_file}\")\n        except Exception as e:",
            "        try:\n            if not lock_file.exists():\n                return False\n\n            # Check file age\n            file_age = time.time() - lock_file.stat().st_mtime\n            stale_threshold = getattr(\n                self.base_engine.config, \"stale_lock_threshold\", 3600\n            )  # 1 hour default\n\n            return file_age > stale_threshold\n\n        except Exception as e:",
            "        try:\n            if not self.base_engine._fcntl:\n                return False\n\n            # Create lock file if it doesn't exist\n            lock_file.touch()\n\n            # Open file for exclusive access\n            with open(lock_file, \"r+b\") as f:\n                # Try to lock the file\n                try:\n                    self.base_engine._fcntl.flock(\n                        f.fileno(),\n                        self.base_engine._fcntl.LOCK_EX | self.base_engine._fcntl.LOCK_NB,\n                    )\n                    return True\n                except OSError:",
            "        try:\n            if not self.base_engine._msvcrt:\n                return False\n\n            # Create lock file if it doesn't exist\n            lock_file.touch()\n\n            # Open file for exclusive access\n            with open(lock_file, \"r+b\") as f:\n                # Try to lock the file\n                try:\n                    self.base_engine._msvcrt.locking(\n                        f.fileno(), self.base_engine._msvcrt.LK_NBLCK, 1\n                    )\n                    return True\n                except OSError:"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_engine.py",
          "count": 2,
          "lines": [
            "            try:\n                import fcntl\n                self._fcntl = fcntl\n            except ImportError:",
            "            try:\n                import msvcrt\n                self._msvcrt = msvcrt\n            except ImportError:"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_engine_operations.py",
          "count": 3,
          "lines": [
            "        try:\n            # Get all lock files in the system\n            lock_files = []\n            for lock_info in self.base_engine._active_locks.values():\n                lock_file = Path(lock_info.lock_file)\n                if lock_file.exists() and self.base_engine.platform_ops._is_lock_stale(lock_file):\n                    lock_files.append(lock_file)\n\n            # Remove stale locks\n            for lock_file in lock_files:\n                self.base_engine.platform_ops._remove_stale_lock(lock_file)\n                cleaned_count += 1\n\n            if cleaned_count > 0:\n                self.base_engine._update_metrics(\"cleanup_stale_locks\", True, 0.0)\n\n            return cleaned_count\n\n        except Exception as e:",
            "        try:\n            lock_file = Path(lock_info.lock_file)\n\n            # Check if lock file exists and is stale\n            if lock_file.exists() and self.base_engine.platform_ops._is_lock_stale(lock_file):\n                self.base_engine.platform_ops._remove_stale_lock(lock_file)\n\n            # Try to acquire lock using platform operations\n            success = False\n            if self.base_engine._is_windows:\n                success = self.base_engine.platform_ops._acquire_windows_lock(lock_file)\n            else:\n                success = self.base_engine.platform_ops._acquire_unix_lock(lock_file)\n\n            if success:\n                with self.base_engine._lock:\n                    self.base_engine._active_locks[lock_info.lock_file] = lock_info\n\n                execution_time = time.time() - start_time\n                self.base_engine._update_metrics(\"acquire_lock\", True, execution_time)\n\n                return LockResult(\n                    success=True,\n                    status=LockStatus.LOCKED,\n                    lock_info=lock_info,\n                )\n            else:\n                execution_time = time.time() - start_time\n                self.base_engine._update_metrics(\"acquire_lock\", False, execution_time)\n\n                return LockResult(\n                    success=False,\n                    status=LockStatus.LOCKED,\n                    error_message=\"Failed to acquire lock - file may be locked by another process\",\n                    lock_info=None,\n                )\n\n        except Exception as e:",
            "        try:\n            lock_file = Path(lock_info.lock_file)\n\n            # Remove from active locks\n            with self.base_engine._lock:\n                if lock_info.lock_file in self.base_engine._active_locks:\n                    del self.base_engine._active_locks[lock_info.lock_file]\n\n            # Remove lock file\n            if lock_file.exists():\n                lock_file.unlink()\n\n            execution_time = time.time() - start_time\n            self.base_engine._update_metrics(\"release_lock\", True, execution_time)\n\n            return LockResult(\n                success=True, status=LockStatus.UNLOCKED, lock_info=lock_info\n            )\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\file_locking\\file_locking_manager.py",
          "count": 1,
          "lines": [
            "        try:\n            if lock_file.exists():\n                lock_file.unlink()\n\n            # Remove from active locks\n            lock_key = f\"{filepath}.lock\"\n            if lock_key in self.engine._active_locks:\n                del self.engine._active_locks[lock_key]\n                self.engine.metrics.active_locks = len(self.engine._active_locks)\n\n            return LockResult(success=True, status=LockStatus.UNLOCKED, execution_time_ms=0.0)\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\gamification\\competition_storage.py",
          "count": 1,
          "lines": [
            "        try:\n            with open(scores_file) as f:\n                data = json.load(f)\n                # Reconstruct scores\n                for agent_id, score_data in data.get(\"scores\", {}).items():\n                    scores[agent_id] = AgentScore(**score_data)\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\import_system\\import_utilities.py",
          "count": 9,
          "lines": [
            "        try:\n            __import__(module_name)\n            return True\n        except ImportError:",
            "        try:\n            base_path = Path(base_module).parent\n            target_path = base_path / relative_path\n            return str(target_path.resolve())\n        except Exception:",
            "        try:\n            compile(import_statement, \"<string>\", \"exec\")\n            return True\n        except SyntaxError:",
            "        try:\n            module = __import__(module_name)\n            if hasattr(module, \"__file__\"):\n                # This is a simplified version - in practice, you'd parse the AST\n                return []\n            return []\n        except ImportError:",
            "        try:\n            module = __import__(module_name)\n            if hasattr(module, \"__file__\"):\n                path = Path(module.__file__)\n                # Walk up to find __init__.py\n                for parent in path.parents:\n                    if (parent / \"__init__.py\").exists():\n                        return str(parent)\n            return None\n        except Exception:",
            "        try:\n            module = __import__(module_name)\n            path = getattr(module, \"__file__\", None)\n            if path:\n                self._cache[module_name] = path\n            return path\n        except ImportError:",
            "        try:\n            module = __import__(module_name)\n            return [name for name in dir(module) if not name.startswith(\"_\")]\n        except ImportError:",
            "        try:\n            module = __import__(module_name)\n            return getattr(module, \"__doc__\", None)\n        except ImportError:",
            "        try:\n            module = sys.modules.get(module_name)\n            if module and hasattr(module, \"__file__\"):\n                return module.__file__\n            return None\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\intelligent_context_emergency.py",
          "count": 1,
          "lines": [
            "        try:\n            emergency_context = EmergencyContext(\n                emergency_id=f\"emergency_{mission.mission_id}\",\n                mission_id=mission.mission_id,\n                emergency_type=\"mission_critical\",\n                severity_level=\"high\",\n                affected_agents=list(mission.agent_assignments.keys()),\n                intervention_protocols=self._get_intervention_protocols(mission),\n                estimated_resolution_time=30.0,\n            )\n\n            execution_time = (time.time() - start_time) * 1000\n            self.engine._update_metrics(\"emergency\", True, execution_time)\n\n            return emergency_context\n\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\intelligent_context_engine.py",
          "count": 2,
          "lines": [
            "        try:\n            self.active_missions[mission_context.mission_id] = mission_context\n            return True\n        except Exception:",
            "        try:\n            self.agent_capabilities[capability.agent_id] = capability\n            return True\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\intelligent_context_search.py",
          "count": 1,
          "lines": [
            "        try:\n            results = []\n\n            # Search in mission contexts\n            if mission_id and mission_id in self.engine.active_missions:\n                mission = self.engine.active_missions[mission_id]\n                if query.lower() in mission.mission_type.lower():\n                    results.append(\n                        SearchResult(\n                            result_id=f\"mission_{mission_id}\",\n                            content=f\"Mission: {mission.mission_type}\",\n                            relevance_score=0.9,\n                            source_type=\"mission\",\n                            source_id=mission_id,\n                        )\n                    )\n\n            # Search in agent capabilities\n            for agent_id, capability in self.engine.agent_capabilities.items():\n                if query.lower() in capability.primary_role.lower():\n                    results.append(\n                        SearchResult(\n                            result_id=f\"agent_{agent_id}\",\n                            content=f\"Agent: {capability.primary_role}\",\n                            relevance_score=0.8,\n                            source_type=\"agent\",\n                            source_id=agent_id,\n                        )\n                    )\n\n            execution_time = (time.time() - start_time) * 1000\n            self.engine._update_metrics(\"search\", True, execution_time)\n\n            return results\n\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\core\\context_core.py",
          "count": 7,
          "lines": [
            "        try:\n            # Get mission context\n            mission = self.get_mission_context(mission_id)\n            if not mission:\n                mission = self.engine.get_mission_context(mission_id)\n            \n            if not mission:\n                # Create minimal mission context for optimization\n                mission = MissionContext(\n                    mission_id=mission_id,\n                    mission_type=\"unknown\",\n                    current_phase=\"planning\",\n                )\n            \n            # Use assignment engine to optimize\n            result = self.assignment_engine.optimize_agent_assignment(mission)\n            \n            if result.success and result.data.get(\"recommendations\"):\n                # Extract agent IDs from recommendations\n                recommendations = result.data[\"recommendations\"]\n                return [rec.agent_id for rec in recommendations]\n            \n            return []\n        except Exception:",
            "        try:\n            # Get mission context\n            mission = self.get_mission_context(mission_id)\n            if not mission:\n                mission = self.engine.get_mission_context(mission_id)\n            \n            if not mission:\n                # Create minimal mission context for risk assessment\n                mission = MissionContext(\n                    mission_id=mission_id,\n                    mission_type=\"unknown\",\n                    current_phase=\"planning\",\n                )\n            \n            # Use risk assessment engine\n            return self.risk_engine.assess_mission_risks(mission)\n        except Exception:",
            "        try:\n            # Import PredictionAnalyzer\n            from ...vector_strategic_oversight.unified_strategic_oversight.analyzers.prediction_analyzer import (\n                PredictionAnalyzer,\n            )\n            \n            analyzer = PredictionAnalyzer()\n            \n            # Get task data from mission context (task_id may be mission_id)\n            mission = self.get_mission_context(task_id)\n            if not mission:\n                mission = self.engine.get_mission_context(task_id)\n            \n            # Convert mission to task data format\n            task_data = {\n                \"task_id\": task_id,\n                \"title\": getattr(mission, 'mission_type', 'unknown') if mission else 'unknown',\n                \"description\": f\"Mission: {getattr(mission, 'mission_type', 'unknown')}\" if mission else \"Unknown task\",\n                \"complexity\": \"high\" if mission and len(getattr(mission, 'risk_factors', [])) > 3 else \"medium\",\n                \"priority\": \"high\" if mission and len(getattr(mission, 'risk_factors', [])) > 5 else \"normal\",\n            }\n            \n            # Generate prediction (async method, but we'll call it synchronously)\n            import asyncio\n            try:\n                loop = asyncio.get_event_loop()\n            except RuntimeError:",
            "        try:\n            # Import SwarmCoordinationAnalyzer for pattern analysis\n            from ...vector_strategic_oversight.unified_strategic_oversight.analyzers.swarm_analyzer import (\n                SwarmCoordinationAnalyzer,\n            )\n            \n            analyzer = SwarmCoordinationAnalyzer()\n            \n            # Convert contexts to agent data format\n            agent_data = []\n            for agent_id, capability in self.capabilities.items():\n                agent_data.append({\n                    \"agent_id\": agent_id,\n                    \"capability\": capability.to_dict() if hasattr(capability, 'to_dict') else {},\n                })\n            \n            # Convert missions to mission data format\n            mission_data = []\n            for mission_id, mission in self.contexts.items():\n                mission_data.append(mission.to_dict() if hasattr(mission, 'to_dict') else {\n                    \"mission_id\": mission_id,\n                    \"mission_type\": getattr(mission, 'mission_type', 'unknown'),\n                })\n            \n            # Analyze patterns (async method, but we'll call it synchronously)\n            import asyncio\n            try:\n                loop = asyncio.get_event_loop()\n            except RuntimeError:",
            "        try:\n            # Simple search implementation\n            results = []\n            for context in self.contexts.values():\n                if query.lower() in context.mission_type.lower():\n                    results.append(\n                        SearchResult(\n                            result_id=f\"search_{len(results)}\",\n                            content=context.mission_type,\n                            relevance_score=0.8,\n                            source_type=\"mission\",\n                            source_id=context.mission_id,\n                        )\n                    )\n\n            return ContextRetrievalResult(\n                success=True,\n                search_results=results,\n                execution_time_ms=0.1,\n            )\n        except Exception as e:",
            "        try:\n            # Try to find mission by emergency_id (may be mission_id)\n            mission = self.get_mission_context(emergency_id)\n            if not mission:\n                # Try to find in engine's active missions\n                mission = self.engine.get_mission_context(emergency_id)\n            \n            if mission:\n                # Use emergency handler to get emergency context\n                return self.emergency_handler.get_emergency_context(mission)\n            \n            # If no mission found, create minimal emergency context\n            return EmergencyContext(\n                emergency_id=emergency_id,\n                mission_id=emergency_id,\n                emergency_type=\"unknown\",\n                severity_level=\"medium\",\n            )\n        except Exception:",
            "        try:\n            self.contexts[context.mission_id] = context\n            return True\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\engines\\agent_assignment_engine.py",
          "count": 1,
          "lines": [
            "        try:\n            recommendations = []\n\n            for agent_id, capability in self.parent_engine.agent_capabilities.items():\n                if capability.availability_status == AgentStatus.AVAILABLE.value:\n                    score = self._calculate_agent_score(capability, mission)\n                    specialization = self._calculate_specialization_match(capability, mission)\n\n                    recommendation = AgentRecommendation(\n                        agent_id=agent_id,\n                        recommendation_score=score,\n                        specialization_match=specialization,\n                        estimated_completion_time=self._estimate_completion_time(\n                            capability, mission\n                        ),\n                        confidence_level=self._calculate_confidence_level(capability, mission),\n                    )\n\n                    recommendations.append(recommendation)\n\n            # Sort by recommendation score\n            recommendations.sort(key=lambda x: x.recommendation_score, reverse=True)\n\n            execution_time = (time.time() - start_time) * 1000\n            self.parent_engine._update_metrics(\"agent_assignment\", True, execution_time)\n\n            return OptimizationResult(\n                success=True,\n                data={\n                    \"recommendations\": recommendations,\n                    \"total_agents_considered\": len(recommendations),\n                    \"top_recommendation\": (recommendations[0] if recommendations else None),\n                },\n                execution_time=execution_time,\n            )\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\engines\\risk_assessment_engine.py",
          "count": 1,
          "lines": [
            "        try:\n            risk_factors = mission.risk_factors\n            risk_level = self._determine_risk_level(risk_factors)\n            mitigation_strategies = self._generate_risk_mitigations(risk_level, mission)\n\n            risk_assessment = RiskAssessment(\n                risk_id=f\"risk_{mission.mission_id}\",\n                risk_level=risk_level,\n                risk_factors=risk_factors,\n                mitigation_strategies=mitigation_strategies,\n                probability=0.5,\n                impact=0.7,\n            )\n\n            execution_time = (time.time() - start_time) * 1000\n            self.parent_engine._update_metrics(\"risk_assessment\", True, execution_time)\n\n            return risk_assessment\n\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\intelligent_context\\unified_intelligent_context\\search_operations.py",
          "count": 6,
          "lines": [
            "        try:\n            # Preprocess query\n            processed_query = self._preprocess_query(query)\n\n            # Get search results\n            results = self._perform_search(\n                processed_query, context_type, priority_filter, status_filter\n            )\n\n            # Sort by relevance\n            results.sort(key=lambda x: x.relevance_score, reverse=True)\n\n            # Apply limit\n            results = results[:limit]\n\n            # Record search\n            self.base_search._record_search(query, len(results))\n\n            return results\n\n        except Exception as e:",
            "        try:\n            # Remove extra whitespace\n            query = \" \".join(query.split())\n\n            # Convert to lowercase\n            query = query.lower()\n\n            # Remove special characters except spaces\n            query = re.sub(r\"[^\\w\\s]\", \"\", query)\n\n            # Remove duplicate words\n            words = query.split()\n            unique_words = []\n            for word in words:\n                if word not in unique_words:\n                    unique_words.append(word)\n\n            optimized_query = \" \".join(unique_words)\n\n            return optimized_query\n\n        except Exception as e:",
            "        try:\n            # Try to use real vector database search\n            results = self._search_vector_database(\n                query, context_type, priority_filter, status_filter\n            )\n            \n            # If vector DB search returns results, use them\n            if results:\n                return results\n            \n            # Fallback to mock results if vector DB unavailable or returns no results\n            if self.logger:\n                self.logger.warning(\n                    f\"Vector database search returned no results, using fallback for query: {query}\"\n                )\n            return self._create_mock_results(query, context_type, priority_filter, status_filter)\n            \n        except Exception as e:",
            "        try:\n            from src.services.vector_database_service_unified import get_vector_database_service\n            from src.web.vector_database.models import SearchRequest as VectorSearchRequest\n            \n            # Get vector database service\n            vector_db = get_vector_database_service()\n            if not vector_db:\n                return []\n            \n            # Build search filters based on context type\n            filters = {}\n            if context_type:\n                filters[\"context_type\"] = context_type.value\n            if priority_filter:\n                filters[\"priority\"] = priority_filter.value\n            if status_filter:\n                filters[\"status\"] = status_filter.value\n            \n            # Create search request\n            search_request = VectorSearchRequest(\n                query=query,\n                collection=\"all\",\n                limit=20,  # Get more results for filtering\n                filters=filters,\n            )\n            \n            # Perform search\n            vector_results = vector_db.search(search_request)\n            \n            # Convert vector database results to SearchResult format\n            results = []\n            for vec_result in vector_results:\n                # Map vector DB result to intelligent context SearchResult\n                result = SearchResult(\n                    result_id=vec_result.id,\n                    title=vec_result.title or \"\",\n                    description=vec_result.content[:200] if vec_result.content else \"\",  # Truncate\n                    relevance_score=vec_result.relevance or vec_result.score or 0.0,\n                    context_type=self._infer_context_type(vec_result),\n                    metadata={\n                        \"collection\": vec_result.collection,\n                        \"tags\": vec_result.tags,\n                        **vec_result.metadata,\n                    },\n                )\n                results.append(result)\n            \n            return results\n            \n        except ImportError:",
            "        try:\n            if pattern_name not in self.base_search.search_patterns:\n                return []\n\n            pattern = self.base_search.search_patterns[pattern_name]\n\n            # Apply pattern to query\n            if pattern:\n                # Simple pattern matching - in real implementation this would be more sophisticated\n                if re.search(pattern, query, re.IGNORECASE):\n                    return self.search_contexts(query)\n\n            return []\n\n        except Exception as e:",
            "        try:\n            metadata = vec_result.metadata or {}\n            collection = vec_result.collection or \"\"\n            \n            # Check metadata for context type\n            if \"context_type\" in metadata:\n                try:\n                    return ContextType(metadata[\"context_type\"])\n                except ValueError:"
          ]
        },
        {
          "file": "src\\core\\managers\\base_manager_helpers.py",
          "count": 2,
          "lines": [
            "        try:\n            # Validate configuration updates\n            validation_result = validation_manager.validate_config(\n                config_data=updates, component_type=state_tracker.manager_type.value\n            )\n\n            if not validation_result.is_valid:\n                logger.error(f\"Invalid configuration updates: {validation_result.errors}\")\n                return False\n\n            # Update configuration\n            success = configuration_manager.update_component_config(\n                component_id=state_tracker.manager_id, updates=updates\n            )\n\n            if success:\n                state_tracker.config.update(updates)\n                logger.info(f\"Configuration updated for {state_tracker.manager_name}\")\n                ManagerPropertySync.sync_properties(manager, state_tracker, manager.metrics_tracker)\n                return True\n            else:\n                logger.error(f\"Failed to update configuration for {state_tracker.manager_name}\")\n                return False\n\n        except Exception as e:",
            "        try:\n            base_status = status_manager.get_component_status(state_tracker.manager_id)\n            manager_status = state_tracker.get_status_dict()\n            manager_status.update(metrics_tracker.get_metrics_for_status())\n\n            if base_status:\n                manager_status.update(base_status)\n\n            return manager_status\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\core_resource_manager.py",
          "count": 3,
          "lines": [
            "        try:\n            # Ensure runtime directory exists\n            os.makedirs(\"runtime\", exist_ok=True)\n\n            # Load existing locks\n            self.lock_ops.load_locks()\n\n            context.logger(\"Core Resource Manager initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            # Save locks\n            self.lock_ops.save_locks()\n\n            # Clear contexts\n            self.context_ops.clear_contexts()\n\n            # Clear locks\n            self.lock_ops.clear_locks()\n\n            context.logger(\"Core Resource Manager cleaned up\")\n            return True\n        except Exception as e:",
            "        try:\n            if operation == \"create_resource\":\n                return self.crud_ops.create_resource(\n                    context, payload.get(\"resource_type\", \"\"), payload\n                )\n            elif operation == \"get_resource\":\n                return self.crud_ops.get_resource(context, payload.get(\"resource_id\", \"\"))\n            elif operation == \"update_resource\":\n                return self.crud_ops.update_resource(\n                    context, payload.get(\"resource_id\", \"\"), payload\n                )\n            elif operation == \"delete_resource\":\n                return self.crud_ops.delete_resource(context, payload.get(\"resource_id\", \"\"))\n            elif operation == \"file_operation\":\n                return self.file_ops.handle_operation(context, payload)\n            elif operation == \"lock_operation\":\n                return self.lock_ops.handle_operation(context, payload)\n            elif operation == \"context_operation\":\n                return self.context_ops.handle_operation(context, payload)\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown operation: {operation}\",\n                )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_lifecycle.py",
          "count": 2,
          "lines": [
            "        try:\n            self.state_tracker.set_state(manager_state_enum.CLEANING_UP)\n            self.logger.info(f\"Cleaning up {self.state_tracker.manager_name} manager\")\n            cleanup_context = {\n                \"manager_id\": self.state_tracker.manager_id,\n                \"manager_type\": self.state_tracker.manager_type.value,\n            }\n            success = self.cleanup_manager.cleanup_component(\n                component_id=self.state_tracker.manager_id,\n                component_type=\"manager\",\n                context=cleanup_context,\n            )\n            if success:\n                self.state_tracker.set_state(manager_state_enum.TERMINATED)\n                self.status_manager.unregister_component(self.state_tracker.manager_id)\n                self.logger.info(\n                    f\"{self.state_tracker.manager_name} manager cleaned up successfully\"\n                )\n                return True\n            else:\n                self.state_tracker.mark_error(\"Cleanup failed\")\n                self.logger.error(f\"Failed to cleanup {self.state_tracker.manager_name} manager\")\n                return False\n        except Exception as e:",
            "        try:\n            self.state_tracker.set_state(manager_state_enum.INITIALIZING)\n            self.state_tracker.context = context\n            self.state_tracker.config = context.config.copy()\n            self.logger.info(f\"Initializing {self.state_tracker.manager_name} manager\")\n            init_context = {\n                \"manager_id\": self.state_tracker.manager_id,\n                \"manager_type\": self.state_tracker.manager_type.value,\n                \"config\": self.state_tracker.config,\n                \"timestamp\": context.timestamp,\n            }\n            success = self.initialization_manager.initialize_component(\n                component_id=self.state_tracker.manager_id,\n                component_type=\"manager\",\n                context=init_context,\n            )\n            if success:\n                self.state_tracker.mark_initialized()\n                self.status_manager.register_component(\n                    component_id=self.state_tracker.manager_id,\n                    component_type=self.state_tracker.manager_type.value,\n                    metadata={\n                        \"manager_name\": self.state_tracker.manager_name,\n                        \"initialized_at\": self.state_tracker.initialized_at.isoformat(),\n                        \"config_keys\": list(self.state_tracker.config.keys()),\n                    },\n                )\n                self.logger.info(\n                    f\"{self.state_tracker.manager_name} manager initialized successfully\"\n                )\n                return True\n            else:\n                self.state_tracker.mark_error(\"Initialization failed\")\n                self.logger.error(f\"Failed to initialize {self.state_tracker.manager_name} manager\")\n                return False\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_metrics.py",
          "count": 1,
          "lines": [
            "        try:\n            self.operation_count = 0\n            self.success_count = 0\n            self.error_count = 0\n            return True\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\manager_operations.py",
          "count": 1,
          "lines": [
            "        try:\n            self.metrics_tracker.record_operation_start()\n            self.state_tracker.mark_operation()\n\n            # Validate input\n            validation_result = self.validation_manager.validate_operation(\n                operation=operation,\n                payload=payload,\n                component_type=self.state_tracker.manager_type.value,\n            )\n\n            if not validation_result.is_valid:\n                self.metrics_tracker.record_error()\n                return self.result_manager.create_error_result(\n                    error=f\"Validation failed: {validation_result.errors}\",\n                    operation=operation,\n                    component_id=self.state_tracker.manager_id,\n                )\n\n            # Execute operation via callback\n            result = execute_callback(context, operation, payload)\n\n            if result.success:\n                self.metrics_tracker.record_success()\n            else:\n                self.metrics_tracker.record_error()\n                self.state_tracker.last_error = result.error\n\n            # Create standardized result\n            return self.result_manager.create_result(\n                data=result.data if result.success else {},\n                operation=operation,\n                component_id=self.state_tracker.manager_id,\n                success=result.success,\n                error=result.error,\n                metrics=result.metrics,\n            )\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\registry.py",
          "count": 3,
          "lines": [
            "            try:\n                manager.cleanup(context)\n            except Exception as e:",
            "        try:\n            # Register default manager types\n            for name, manager_type in MANAGER_TYPES.items():\n                self.register_manager_type(name, manager_type)\n\n            # Create core managers\n            core_managers = [\n                \"resource\",\n                \"configuration\",\n                \"execution\",\n                \"monitoring\",\n                \"service\",\n            ]\n            for manager_name in core_managers:\n                if not self.create_manager(manager_name, context):\n                    context.logger(f\"Failed to initialize core manager: {manager_name}\")\n                    return False\n\n            self._initialized = True\n            return True\n        except Exception as e:",
            "class ManagerRegistry:\n    \"\"\"DIP registry: high-level depends on abstraction, not concretion.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize manager registry.\"\"\"\n        self._managers: dict[str, Manager] = {}\n        self._manager_types: dict[str, type[Manager]] = {}\n        self._initialized = False\n\n    def register_manager_type(self, name: str, manager_class: type[Manager]) -> None:\n        \"\"\"Register a manager type.\"\"\"\n        if name in self._manager_types:\n            raise ValueError(f\"Manager type already registered: {name}\")\n        self._manager_types[name] = manager_class\n\n    def create_manager(self, name: str, context: ManagerContext) -> Manager | None:\n        \"\"\"Create and initialize a manager instance.\"\"\"\n        try:\n            if name not in self._manager_types:\n                raise ValueError(f\"Unknown manager type: {name}\")\n\n            manager_class = self._manager_types[name]\n            manager = manager_class()\n\n            # Initialize the manager\n            if manager.initialize(context):\n                self._managers[name] = manager\n                return manager\n            else:\n                return None\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_context_operations.py",
          "count": 2,
          "lines": [
            "        try:\n            if operation == \"set\":\n                return self._set_context(agent_id, payload.get(\"context_data\", {}))\n            elif operation == \"get\":\n                return self._get_context(agent_id)\n            elif operation == \"update\":\n                return self._update_context(agent_id, payload.get(\"updates\", {}))\n            elif operation == \"delete\":\n                return self._delete_context(agent_id)\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown context operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            self._agent_contexts[agent_id] = {\n                **context_data,\n                \"last_updated\": datetime.now().isoformat(),\n            }\n            self.operations_count += 1\n            return ManagerResult(\n                success=True,\n                data={\"type\": \"context\", \"agent_id\": agent_id, \"created\": True},\n                metrics={\"context_keys\": len(context_data)},\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_crud_operations.py",
          "count": 4,
          "lines": [
            "        try:\n            # Try file deletion\n            if os.path.exists(resource_id):\n                if os.path.isfile(resource_id):\n                    os.remove(resource_id)\n                    return ManagerResult(\n                        success=True,\n                        data={\"type\": \"file\", \"path\": resource_id, \"deleted\": True},\n                        metrics={},\n                    )\n                elif os.path.isdir(resource_id):\n                    shutil.rmtree(resource_id)\n                    return ManagerResult(\n                        success=True,\n                        data={\"type\": \"directory\", \"path\": resource_id, \"deleted\": True},\n                        metrics={},\n                    )\n\n            # Try context deletion\n            if self.context_ops.delete_context_if_exists(resource_id):\n                return ManagerResult(\n                    success=True,\n                    data={\"type\": \"context\", \"context_id\": resource_id, \"deleted\": True},\n                    metrics={},\n                )\n\n            return ManagerResult(\n                success=False,\n                data={},\n                metrics={},\n                error=f\"Resource not found: {resource_id}\",\n            )\n        except Exception as e:",
            "        try:\n            # Try file first\n            if os.path.exists(resource_id):\n                if os.path.isfile(resource_id):\n                    content = self.file_ops.read_file(resource_id)\n                    return ManagerResult(\n                        success=True,\n                        data={\"type\": \"file\", \"content\": content, \"path\": resource_id},\n                        metrics={\"file_size\": len(content)},\n                    )\n                elif os.path.isdir(resource_id):\n                    files = os.listdir(resource_id)\n                    return ManagerResult(\n                        success=True,\n                        data={\"type\": \"directory\", \"files\": files, \"path\": resource_id},\n                        metrics={\"file_count\": len(files)},\n                    )\n\n            # Try context\n            agent_context = self.context_ops.get_context_if_exists(resource_id)\n            if agent_context:\n                return ManagerResult(\n                    success=True,\n                    data={\"type\": \"context\", \"context\": agent_context},\n                    metrics={\"context_keys\": len(agent_context)},\n                )\n\n            return ManagerResult(\n                success=False,\n                data={},\n                metrics={},\n                error=f\"Resource not found: {resource_id}\",\n            )\n        except Exception as e:",
            "        try:\n            # Try file update\n            if os.path.exists(resource_id) and os.path.isfile(resource_id):\n                if \"content\" in updates:\n                    self.file_ops.write_file(resource_id, updates[\"content\"])\n                    return ManagerResult(\n                        success=True,\n                        data={\"type\": \"file\", \"path\": resource_id, \"updated\": True},\n                        metrics={\"file_size\": len(updates[\"content\"])},\n                    )\n\n            # Try context update\n            updated_context = self.context_ops.update_context_direct(resource_id, updates)\n            if updated_context:\n                return ManagerResult(\n                    success=True,\n                    data={\"type\": \"context\", \"context\": updated_context},\n                    metrics={\"context_keys\": len(updated_context)},\n                )\n\n            return ManagerResult(\n                success=False,\n                data={},\n                metrics={},\n                error=f\"Resource not found or cannot be updated: {resource_id}\",\n            )\n        except Exception as e:",
            "        try:\n            if resource_type == \"file\":\n                return self.file_ops.create_file(\n                    context, data.get(\"file_path\", \"\"), data.get(\"content\", \"\")\n                )\n            elif resource_type == \"directory\":\n                return self.file_ops.create_directory(context, data.get(\"dir_path\", \"\"))\n            elif resource_type == \"context\":\n                return self.context_ops.create_context(\n                    context, data.get(\"agent_id\", \"\"), data.get(\"context_data\", {})\n                )\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown resource type: {resource_type}\",\n                )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_file_operations.py",
          "count": 3,
          "lines": [
            "        try:\n            if operation == \"read\":\n                return self._read_operation(file_path)\n            elif operation == \"write\":\n                return self._write_operation(file_path, payload.get(\"content\", \"\"))\n            elif operation == \"copy\":\n                return self._copy_operation(file_path, payload.get(\"destination\", \"\"))\n            elif operation == \"move\":\n                return self._move_operation(file_path, payload.get(\"destination\", \"\"))\n            elif operation == \"delete\":\n                return self._delete_operation(file_path)\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown file operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            os.makedirs(dir_path, exist_ok=True)\n            self.operations_count += 1\n            return ManagerResult(\n                success=True,\n                data={\"type\": \"directory\", \"path\": dir_path, \"created\": True},\n                metrics={},\n            )\n        except Exception as e:",
            "        try:\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n            self.write_file(file_path, content)\n            self.operations_count += 1\n            return ManagerResult(\n                success=True,\n                data={\"type\": \"file\", \"path\": file_path, \"created\": True},\n                metrics={\"file_size\": len(content)},\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\resource_lock_operations.py",
          "count": 3,
          "lines": [
            "        try:\n            if operation == \"acquire\":\n                return self._acquire_lock(lock_id)\n            elif operation == \"release\":\n                return self._release_lock(lock_id)\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown lock operation: {operation}\",\n                )\n        except Exception as e:",
            "        try:\n            if os.path.exists(self._lock_file):\n                with open(self._lock_file, encoding=\"utf-8\") as f:\n                    lock_data = json.load(f)\n                    # Note: We can't restore actual Lock objects, just track them\n                    self._locks = {k: threading.Lock() for k in lock_data.get(\"locks\", [])}\n        except Exception:",
            "        try:\n            lock_data = {\"locks\": list(self._locks.keys())}\n            with open(self._lock_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump(lock_data, f, indent=2)\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\base_manager.py",
          "count": 2,
          "lines": [
            "        try:\n            self.metrics_tracker.record_operation_start()\n            self.state_tracker.mark_operation()\n\n            # Validate input\n            validation_result = self.validation_manager.validate_operation(\n                operation=operation,\n                payload=payload,\n                component_type=self.state_tracker.manager_type.value,\n            )\n\n            if not validation_result.is_valid:\n                self.metrics_tracker.record_error()\n                return self.result_manager.create_error_result(\n                    error=f\"Validation failed: {validation_result.errors}\",\n                    operation=operation,\n                    component_id=self.state_tracker.manager_id,\n                )\n\n            # Execute operation (implemented by subclasses)\n            result = self._execute_operation(context, operation, payload)\n\n            if result.success:\n                self.metrics_tracker.record_success()\n            else:\n                self.metrics_tracker.record_error()\n                self.state_tracker.last_error = result.error\n\n            # Create standardized result\n            return self.result_manager.create_result(\n                data=result.data if result.success else {},\n                operation=operation,\n                component_id=self.state_tracker.manager_id,\n                success=result.success,\n                error=result.error,\n                metrics=result.metrics,\n            )\n\n        except Exception as e:",
            "        try:\n            success = self.metrics_tracker.reset()\n            if success:\n                self.logger.info(f\"Metrics reset for {self.state_tracker.manager_name}\")\n                ManagerPropertySync.sync_properties(self, self.state_tracker, self.metrics_tracker)\n            return success\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_coordinator.py",
          "count": 4,
          "lines": [
            "        try:\n            # Cleanup specialized managers\n            task_success = self.task_manager.cleanup(context)\n            protocol_success = self.protocol_manager.cleanup(context)\n\n            # Cleanup base manager\n            base_success = super().cleanup(context)\n\n            success = base_success and task_success and protocol_success\n\n            if success:\n                context.logger(\"Execution Coordinator cleaned up\")\n\n            return success\n\n        except Exception as e:",
            "        try:\n            # Initialize base manager\n            base_success = super().initialize(context)\n\n            # Initialize specialized managers\n            task_success = self.task_manager.initialize(context)\n            protocol_success = self.protocol_manager.initialize(context)\n\n            # Sync shared state\n            self._sync_managers()\n\n            success = base_success and task_success and protocol_success\n\n            if success:\n                context.logger(\"Execution Coordinator initialized with specialized managers\")\n\n            return success\n\n        except Exception as e:",
            "        try:\n            # Route to appropriate manager based on operation\n            if operation in [\n                \"create_task\",\n                \"execute_task\",\n                \"cancel_task\",\n                \"list_tasks\",\n                \"get_task_status\",\n            ]:\n                return self.task_manager.execute(context, operation, payload)\n            elif operation in [\n                \"register_protocol\",\n                \"execute_protocol\",\n                \"list_protocols\",\n                \"enable_protocol\",\n                \"disable_protocol\",\n            ]:\n                return self.protocol_manager.execute(context, operation, payload)\n            elif operation in [\"get_execution_status\"]:\n                return super().execute(context, operation, payload)\n            else:\n                return super().execute(context, operation, payload)\n\n        except Exception as e:",
            "        try:\n            base_status = super().get_status()\n            task_status = self.task_manager.get_status()\n            protocol_status = self.protocol_manager.get_status()\n\n            return {\n                **base_status,\n                \"task_manager\": task_status,\n                \"protocol_manager\": protocol_status,\n                \"coordinator_active\": True,\n                \"v2_compliant\": True,\n            }\n\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_runner.py",
          "count": 2,
          "lines": [
            "        try:\n            if not execution_id:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    message=\"Execution ID is required\",\n                    errors=[\"Execution ID is required\"],\n                )\n            execution = self.executions.get(execution_id)\n            if not execution:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    message=f\"Execution not found: {execution_id}\",\n                    errors=[f\"Execution not found: {execution_id}\"],\n                )\n            return ManagerResult(\n                success=True,\n                data=execution,\n                message=f\"Execution status: {execution['status']}\",\n                errors=[],\n            )\n        except Exception as e:",
            "        try:\n            if task_id and task_id not in self.tasks:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    message=f\"Task not found: {task_id}\",\n                    errors=[f\"Task not found: {task_id}\"],\n                )\n            task = self.tasks[task_id] if task_id else {}\n            execution_id = str(uuid.uuid4())\n            execution = {\n                \"execution_id\": execution_id,\n                \"task_id\": task_id,\n                \"started_at\": datetime.now().isoformat(),\n                \"status\": \"running\",\n                \"result\": None,\n            }\n            self.executions[execution_id] = execution\n            if task_id:\n                task[\"status\"] = task_status_enum.RUNNING\n                task[\"started_at\"] = execution[\"started_at\"]\n            # Execute task in thread\n            thread = threading.Thread(\n                target=self.task_executor.execute_task_thread,\n                args=(\n                    context,\n                    execution_id,\n                    task,\n                    task_data,\n                    self.tasks,\n                    self.executions,\n                    task_status_enum,\n                ),\n            )\n            thread.daemon = True\n            thread.start()\n            if task_id:\n                self.execution_threads[task_id] = thread\n            return ManagerResult(\n                success=True,\n                data={\"execution_id\": execution_id, \"task_id\": task_id},\n                message=f\"Task execution started: {execution_id}\",\n                errors=[],\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\protocol_manager.py",
          "count": 1,
          "lines": [
            "        try:\n            self.protocols[protocol_name] = {\n                \"type\": protocol_type,\n                \"priority\": priority,\n                \"timeout\": timeout,\n                \"max_retries\": 1,\n                \"description\": f\"{protocol_name} protocol\",\n            }\n            return True\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\task_executor.py",
          "count": 2,
          "lines": [
            "        try:\n            execution = executions[execution_id]\n            task_type = task[\"type\"]\n\n            # Execute based on task type\n            if task_type == \"file\":\n                result = self.execute_file_task(task_data)\n            elif task_type == \"data\":\n                result = self.execute_data_task(task_data)\n            elif task_type == \"api\":\n                result = self.execute_api_task(task_data)\n            else:\n                result = {\"status\": \"completed\", \"message\": f\"General task {task_type} completed\"}\n\n            # Update execution\n            execution[\"status\"] = \"completed\"\n            execution[\"completed_at\"] = datetime.now().isoformat()\n            execution[\"result\"] = result\n\n            # Update task\n            task[\"status\"] = task_status_enum.COMPLETED\n            task[\"completed_at\"] = execution[\"completed_at\"]\n            task[\"result\"] = result\n\n        except Exception as e:",
            "        try:\n            started_at = datetime.fromisoformat(execution[\"started_at\"])\n            if \"completed_at\" in execution:\n                completed_at = datetime.fromisoformat(execution[\"completed_at\"])\n                return (completed_at - started_at).total_seconds()\n            elif \"failed_at\" in execution:\n                failed_at = datetime.fromisoformat(execution[\"failed_at\"])\n                return (failed_at - started_at).total_seconds()\n            else:\n                return (datetime.now() - started_at).total_seconds()\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\base_execution_manager.py",
          "count": 4,
          "lines": [
            "        try:\n            # Call parent initialization first\n            if not super().initialize(context):\n                return False\n\n            # Execution-specific initialization\n            self.protocol_manager.register_default_protocols()\n            self._start_task_processor()\n            self.logger.info(\"Base Execution Manager initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            # Start background thread to process tasks from queue\n            def process_tasks():\n                \"\"\"Background task processing loop.\"\"\"\n                while True:\n                    try:\n                        # Check if there are tasks in queue\n                        if self.task_queue:\n                            task_id = self.task_queue[0]  # Get first task\n                            if task_id in self.tasks:\n                                task = self.tasks[task_id]\n                                \n                                # Update task status\n                                task['status'] = 'running'\n                                \n                                # Execute task using task executor\n                                try:\n                                    # Determine task type and execute\n                                    task_type = task.get('type', 'general')\n                                    task_data = task.get('data', {})\n                                    \n                                    if task_type == 'file':\n                                        result = self.task_executor.execute_file_task(task_data)\n                                    elif task_type == 'data':\n                                        result = self.task_executor.execute_data_task(task_data)\n                                    elif task_type == 'api':\n                                        result = self.task_executor.execute_api_task(task_data)\n                                    else:\n                                        result = {'status': 'completed', 'message': 'Task executed'}\n                                    \n                                    if result.get('status') == 'completed':\n                                        task['status'] = 'completed'\n                                    else:\n                                        task['status'] = 'failed'\n                                except Exception as e:",
            "        try:\n            if not protocol_name:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"Protocol name is required\",\n                )\n            protocol_type = protocol_data.get(\"type\", \"routine\")\n            priority = protocol_data.get(\"priority\", 1)\n            timeout = protocol_data.get(\"timeout\", 300)\n            success = self.protocol_manager.register_protocol(\n                protocol_name, protocol_type, priority, timeout\n            )\n            if success:\n                return ManagerResult(\n                    success=True,\n                    data={\"protocol_name\": protocol_name},\n                    metrics={\"protocols_registered\": 1},\n                )\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Failed to register protocol: {protocol_name}\",\n                )\n        except Exception as e:",
            "        try:\n            protocols = self.protocol_manager.list_protocols()\n            return ManagerResult(\n                success=True,\n                data={\"protocols\": protocols, \"count\": len(protocols)},\n                metrics={\"protocols_listed\": len(protocols)},\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\execution\\execution_operations.py",
          "count": 3,
          "lines": [
            "        try:\n            return ManagerResult(\n                success=True,\n                data={\"tasks\": list(self.tasks.keys()), \"count\": len(self.tasks)},\n                message=f\"Found {len(self.tasks)} tasks\",\n                errors=[],\n            )\n        except Exception as e:",
            "        try:\n            task_id = payload.get(\"task_id\")\n            if not task_id or task_id not in self.tasks:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    message=f\"Task not found: {task_id}\",\n                    errors=[f\"Task not found: {task_id}\"],\n                )\n            task = self.tasks[task_id]\n            if task[\"status\"] == TaskStatus.RUNNING:\n                task[\"status\"] = TaskStatus.CANCELLED\n                task[\"cancelled_at\"] = datetime.now().isoformat()\n                if task_id in self.task_queue:\n                    self.task_queue.remove(task_id)\n                return ManagerResult(\n                    success=True,\n                    data={\"task_id\": task_id},\n                    message=f\"Task cancelled: {task_id}\",\n                    errors=[],\n                )\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    message=f\"Task not running: {task_id}\",\n                    errors=[f\"Task status: {task['status']}\"],\n                )\n        except Exception as e:",
            "        try:\n            task_id = str(uuid.uuid4())\n            task = {\n                \"task_id\": task_id,\n                \"type\": payload.get(\"type\", \"general\"),\n                \"status\": TaskStatus.PENDING,\n                \"created_at\": datetime.now().isoformat(),\n                \"data\": payload.get(\"data\", {}),\n            }\n            self.tasks[task_id] = task\n            self.task_queue.append(task_id)\n            return ManagerResult(\n                success=True,\n                data={\"task_id\": task_id},\n                message=f\"Task created: {task_id}\",\n                errors=[],\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\alert_manager.py",
          "count": 3,
          "lines": [
            "            try:\n                if rule.get(\"level\") == alert[\"level\"]:\n                    if rule.get(\"action\") == \"escalate\":\n                        self._escalate_alert(alert, rule)\n                    elif rule.get(\"action\") == \"notify\":\n                        self._notify_alert(alert, rule)\n                    elif rule.get(\"action\") == \"auto_resolve\":\n                        self._auto_resolve_alert(alert, rule)\n            except Exception:",
            "        try:\n            level_filter = payload.get(\"level\")\n            unresolved_only = payload.get(\"unresolved_only\", False)\n\n            filtered_alerts = []\n            for alert in self.alerts.values():\n                if level_filter and alert[\"level\"] != level_filter:\n                    continue\n                if unresolved_only and alert[\"resolved\"]:\n                    continue\n                filtered_alerts.append(alert)\n\n            return ManagerResult(\n                success=True,\n                data={\"alerts\": filtered_alerts, \"count\": len(filtered_alerts)},\n                message=f\"Found {len(filtered_alerts)} alerts\",\n                errors=[],\n            )\n        except Exception as e:",
            "        try:\n            with self._alert_lock:\n                alert_id = str(uuid.uuid4())\n                alert = {\n                    \"alert_id\": alert_id,\n                    \"level\": alert_data.get(\"level\", AlertLevel.MEDIUM.value),\n                    \"message\": alert_data.get(\"message\", \"\"),\n                    \"source\": alert_data.get(\"source\", \"system\"),\n                    \"created_at\": datetime.now().isoformat(),\n                    \"acknowledged\": False,\n                    \"resolved\": False,\n                    \"metadata\": alert_data.get(\"metadata\", {}),\n                }\n                self.alerts[alert_id] = alert\n\n                # Check alert rules\n                self._check_alert_rules(alert)\n\n                # Call alert callbacks\n                for callback in self.alert_callbacks.values():\n                    try:\n                        callback(alert)\n                    except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\alert_operations.py",
          "count": 2,
          "lines": [
            "        try:\n            alert_id = payload.get(\"alert_id\")\n            if not alert_id or alert_id not in alerts:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    message=f\"Alert not found: {alert_id}\",\n                    errors=[f\"Alert not found: {alert_id}\"],\n                )\n\n            alert = alerts[alert_id]\n            alert[\"acknowledged\"] = True\n            alert[\"acknowledged_at\"] = datetime.now().isoformat()\n            alert[\"acknowledged_by\"] = payload.get(\"acknowledged_by\", \"system\")\n\n            return ManagerResult(\n                success=True,\n                data={\"alert_id\": alert_id},\n                message=f\"Alert acknowledged: {alert_id}\",\n                errors=[],\n            )\n        except Exception as e:",
            "        try:\n            alert_id = payload.get(\"alert_id\")\n            if not alert_id or alert_id not in alerts:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    message=f\"Alert not found: {alert_id}\",\n                    errors=[f\"Alert not found: {alert_id}\"],\n                )\n\n            alert = alerts[alert_id]\n            alert[\"resolved\"] = True\n            alert[\"resolved_at\"] = datetime.now().isoformat()\n            alert[\"resolved_by\"] = payload.get(\"resolved_by\", \"system\")\n            alert[\"resolution_notes\"] = payload.get(\"resolution_notes\", \"\")\n\n            return ManagerResult(\n                success=True,\n                data={\"alert_id\": alert_id},\n                message=f\"Alert resolved: {alert_id}\",\n                errors=[],\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\metric_manager.py",
          "count": 2,
          "lines": [
            "        try:\n            metric_name = payload.get(\"metric_name\")\n            if metric_name:\n                if metric_name in self.metrics:\n                    metric = self.metrics[metric_name]\n                    history = self.metric_history.get(metric_name, [])\n                    return ManagerResult(\n                        success=True,\n                        data={\"metric\": metric, \"history\": history},\n                        message=f\"Metric found: {metric_name}\",\n                        errors=[],\n                    )\n                else:\n                    return ManagerResult(\n                        success=False,\n                        data={},\n                        message=f\"Metric not found: {metric_name}\",\n                        errors=[f\"Metric not found: {metric_name}\"],\n                    )\n            else:\n                return ManagerResult(\n                    success=True,\n                    data={\"metrics\": list(self.metrics.values()), \"count\": len(self.metrics)},\n                    message=f\"Found {len(self.metrics)} metrics\",\n                    errors=[],\n                )\n        except Exception as e:",
            "        try:\n            with self._metric_lock:\n                if metric_name not in self.metrics:\n                    self.metrics[metric_name] = {\n                        \"metric_name\": metric_name,\n                        \"type\": MetricType.GAUGE.value,\n                        \"current_value\": metric_value,\n                        \"created_at\": datetime.now().isoformat(),\n                        \"updated_at\": datetime.now().isoformat(),\n                        \"count\": 1,\n                    }\n                    self.metric_history[metric_name] = []\n                else:\n                    metric = self.metrics[metric_name]\n                    metric[\"current_value\"] = metric_value\n                    metric[\"updated_at\"] = datetime.now().isoformat()\n                    metric[\"count\"] += 1\n\n                # Add to history\n                history_entry = {\n                    \"value\": metric_value,\n                    \"timestamp\": datetime.now().isoformat(),\n                }\n                self.metric_history[metric_name].append(history_entry)\n\n                # Trim history if too large\n                if len(self.metric_history[metric_name]) > self.max_history_size:\n                    self.metric_history[metric_name] = self.metric_history[metric_name][\n                        -self.max_history_size :\n                    ]\n\n                # Call metric callbacks\n                for callback in self.metric_callbacks.values():\n                    try:\n                        callback(metric_name, metric_value)\n                    except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\metrics_manager.py",
          "count": 4,
          "lines": [
            "        try:\n            export_format = payload.get(\"format\", \"json\")\n            metric_names = payload.get(\"metric_names\", [])\n            include_history = payload.get(\"include_history\", False)\n\n            # Filter metrics\n            if metric_names:\n                export_data = {\n                    name: self.metrics.get(name) for name in metric_names if name in self.metrics\n                }\n            else:\n                export_data = dict(self.metrics)\n\n            # Add history if requested\n            if include_history:\n                for name in export_data:\n                    if name in self.metric_history:\n                        export_data[name][\"history\"] = self.metric_history[name]\n\n            if export_format == \"json\":\n                import json\n\n                export_string = json.dumps(export_data, indent=2)\n            elif export_format == \"csv\":\n                # Simple CSV export for numeric metrics\n                csv_lines = [\"metric_name,timestamp,value,type\"]\n                for name, metric in export_data.items():\n                    if isinstance(metric.get(\"value\"), (int, float)):\n                        csv_lines.append(\n                            f\"{name},{metric['timestamp']},{metric['value']},{metric.get('type', 'unknown')}\"\n                        )\n                export_string = \"\\n\".join(csv_lines)\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unsupported export format: {export_format}\",\n                )\n\n            return ManagerResult(\n                success=True,\n                data={\n                    \"export_format\": export_format,\n                    \"metrics_exported\": len(export_data),\n                    \"export_data\": export_string,\n                },\n                metrics={\"metrics_exported\": len(export_data)},\n            )\n\n        except Exception as e:",
            "        try:\n            if operation == \"record_metric\":\n                return self.record_metric(context, payload)\n            elif operation == \"get_metrics\":\n                return self._get_metrics(context, payload)\n            elif operation == \"get_metric_aggregation\":\n                return self._get_metric_aggregation(context, payload)\n            elif operation == \"get_metric_trends\":\n                return self._get_metric_trends(context, payload)\n            elif operation == \"export_metrics\":\n                return self._export_metrics(context, payload)\n            else:\n                return super().execute(context, operation, payload)\n        except Exception as e:",
            "        try:\n            metric_name = payload.get(\"metric_name\")\n            aggregation_type = payload.get(\"aggregation_type\", \"summary\")\n            time_window = payload.get(\"time_window\", 3600)  # 1 hour default\n\n            if not metric_name or metric_name not in self.metric_history:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Metric {metric_name} not found or has no history\",\n                )\n\n            history = self.metric_history[metric_name]\n\n            # Filter by time window\n            cutoff_time = datetime.now() - timedelta(seconds=time_window)\n            filtered_history = [\n                entry\n                for entry in history\n                if datetime.fromisoformat(entry[\"timestamp\"]) > cutoff_time\n            ]\n\n            if not filtered_history:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"No data in specified time window\",\n                )\n\n            # Calculate aggregation\n            values = [\n                entry[\"value\"]\n                for entry in filtered_history\n                if isinstance(entry[\"value\"], (int, float))\n            ]\n\n            if not values:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"No numeric values found for aggregation\",\n                )\n\n            if aggregation_type == \"summary\":\n                result = {\n                    \"count\": len(values),\n                    \"min\": min(values),\n                    \"max\": max(values),\n                    \"sum\": sum(values),\n                    \"average\": sum(values) / len(values),\n                }\n\n                # Calculate variance and standard deviation\n                mean = result[\"average\"]\n                variance = sum((x - mean) ** 2 for x in values) / len(values)\n                result[\"variance\"] = variance\n                result[\"std_deviation\"] = variance**0.5\n\n            elif aggregation_type == \"percentiles\":\n                sorted_values = sorted(values)\n                n = len(sorted_values)\n                result = {\n                    \"p50\": sorted_values[int(n * 0.5)],\n                    \"p90\": sorted_values[int(n * 0.9)],\n                    \"p95\": sorted_values[int(n * 0.95)],\n                    \"p99\": sorted_values[int(n * 0.99)],\n                }\n            else:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Unknown aggregation type: {aggregation_type}\",\n                )\n\n            return ManagerResult(\n                success=True,\n                data={\n                    \"metric_name\": metric_name,\n                    \"aggregation_type\": aggregation_type,\n                    \"time_window\": time_window,\n                    \"data_points\": len(filtered_history),\n                    \"result\": result,\n                },\n                metrics={\"aggregations_calculated\": 1},\n            )\n\n        except Exception as e:",
            "        try:\n            metric_name = payload.get(\"metric_name\")\n            trend_window = payload.get(\"trend_window\", 7200)  # 2 hours default\n\n            if not metric_name or metric_name not in self.metric_history:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=f\"Metric {metric_name} not found or has no history\",\n                )\n\n            history = self.metric_history[metric_name]\n\n            # Filter by time window\n            cutoff_time = datetime.now() - timedelta(seconds=trend_window)\n            filtered_history = [\n                entry\n                for entry in history\n                if datetime.fromisoformat(entry[\"timestamp\"]) > cutoff_time\n            ]\n\n            if len(filtered_history) < 2:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"Insufficient data points for trend analysis\",\n                )\n\n            # Sort by timestamp\n            filtered_history.sort(key=lambda x: x[\"timestamp\"])\n\n            # Extract values and timestamps\n            values = [\n                entry[\"value\"]\n                for entry in filtered_history\n                if isinstance(entry[\"value\"], (int, float))\n            ]\n            timestamps = [\n                entry[\"timestamp\"]\n                for entry in filtered_history\n                if isinstance(entry[\"value\"], (int, float))\n            ]\n\n            if len(values) < 2:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"Insufficient numeric values for trend analysis\",\n                )\n\n            # Calculate trend\n            first_value = values[0]\n            last_value = values[-1]\n            trend_direction = (\n                \"increasing\"\n                if last_value > first_value\n                else \"decreasing\" if last_value < first_value else \"stable\"\n            )\n            trend_magnitude = abs(last_value - first_value) / first_value if first_value != 0 else 0\n\n            # Calculate moving average (simple 3-point)\n            moving_averages = []\n            for i in range(2, len(values)):\n                moving_averages.append(sum(values[i - 2 : i + 1]) / 3)\n\n            return ManagerResult(\n                success=True,\n                data={\n                    \"metric_name\": metric_name,\n                    \"trend_window\": trend_window,\n                    \"data_points\": len(values),\n                    \"trend_direction\": trend_direction,\n                    \"trend_magnitude\": trend_magnitude,\n                    \"first_value\": first_value,\n                    \"last_value\": last_value,\n                    \"moving_averages\": moving_averages[-10:],  # Last 10 moving averages\n                },\n                metrics={\"trends_calculated\": 1},\n            )\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_crud.py",
          "count": 3,
          "lines": [
            "        try:\n            alert_id = str(uuid.uuid4())\n            level = payload.get(\"level\", AlertLevel.MEDIUM)\n            source = payload.get(\"source\", \"system\")\n            message = payload.get(\"message\", \"\")\n            metadata = payload.get(\"metadata\", {})\n            alert = {\n                \"id\": alert_id,\n                \"level\": level.value if hasattr(level, \"value\") else str(level),\n                \"source\": source,\n                \"message\": message,\n                \"metadata\": metadata,\n                \"status\": \"active\",\n                \"created_at\": datetime.now().isoformat(),\n                \"acknowledged\": False,\n                \"resolved\": False,\n            }\n            self.state.add_alert(alert_id, alert)\n            # Check alert rules if rules manager is available\n            if self.rules:\n                self.rules.check_alert_rules(alert)\n            return ManagerResult(\n                success=True,\n                data={\"alert_id\": alert_id, \"alert\": alert},\n                metrics={\"alerts_created\": 1},\n            )\n        except Exception as e:",
            "        try:\n            metric_name = payload.get(\"metric_name\")\n            metric_value = payload.get(\"metric_value\")\n            metric_type = payload.get(\"metric_type\", MetricType.GAUGE)\n            tags = payload.get(\"tags\", {})\n            if not metric_name or metric_value is None:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"metric_name and metric_value are required\",\n                )\n            # Convert enum to string if needed\n            if hasattr(metric_type, \"value\"):\n                metric_type = metric_type.value\n            metric_entry = {\n                \"name\": metric_name,\n                \"value\": metric_value,\n                \"type\": metric_type,\n                \"tags\": tags,\n                \"timestamp\": datetime.now().isoformat(),\n            }\n            self.state.add_metric(metric_name, metric_entry)\n            return ManagerResult(\n                success=True,\n                data={\"metric_name\": metric_name, \"value\": metric_value},\n                metrics={\"metrics_recorded\": 1},\n            )\n        except Exception as e:",
            "        try:\n            widget_id = str(uuid.uuid4())\n            widget_type = payload.get(\"widget_type\", WidgetType.METRIC)\n            title = payload.get(\"title\", \"Untitled Widget\")\n            config = payload.get(\"config\", {})\n            # Convert enum to string if needed\n            if hasattr(widget_type, \"value\"):\n                widget_type = widget_type.value\n            widget = {\n                \"id\": widget_id,\n                \"type\": widget_type,\n                \"title\": title,\n                \"config\": config,\n                \"created_at\": datetime.now().isoformat(),\n                \"updated_at\": datetime.now().isoformat(),\n            }\n            self.state.add_widget(widget_id, widget)\n            return ManagerResult(\n                success=True,\n                data={\"widget_id\": widget_id, \"widget\": widget},\n                metrics={\"widgets_created\": 1},\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_query.py",
          "count": 5,
          "lines": [
            "        try:\n            alert_id = payload.get(\"alert_id\")\n            if not alert_id or not self.state.get_alert(alert_id):\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"Alert ID is required and must exist\",\n                )\n            updates = {\n                \"acknowledged\": True,\n                \"acknowledged_at\": datetime.now().isoformat(),\n            }\n            self.state.update_alert(alert_id, updates)\n            return ManagerResult(\n                success=True,\n                data={\"alert_id\": alert_id},\n                metrics={\"alerts_acknowledged\": 1},\n            )\n        except Exception as e:",
            "        try:\n            alert_id = payload.get(\"alert_id\")\n            resolution_notes = payload.get(\"resolution_notes\", \"\")\n            if not alert_id or not self.state.get_alert(alert_id):\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"Alert ID is required and must exist\",\n                )\n            updates = {\n                \"resolved\": True,\n                \"resolved_at\": datetime.now().isoformat(),\n                \"resolution_notes\": resolution_notes,\n                \"status\": \"resolved\",\n            }\n            self.state.update_alert(alert_id, updates)\n            return ManagerResult(\n                success=True,\n                data={\"alert_id\": alert_id},\n                metrics={\"alerts_resolved\": 1},\n            )\n        except Exception as e:",
            "        try:\n            level_filter = payload.get(\"level\")\n            status_filter = payload.get(\"status\")\n            source_filter = payload.get(\"source\")\n            alerts = self.state.get_all_alerts()\n            # Apply filters\n            if level_filter:\n                alerts = {k: v for k, v in alerts.items() if v.get(\"level\") == level_filter}\n            if status_filter:\n                alerts = {k: v for k, v in alerts.items() if v.get(\"status\") == status_filter}\n            if source_filter:\n                alerts = {k: v for k, v in alerts.items() if v.get(\"source\") == source_filter}\n            return ManagerResult(\n                success=True, data={\"alerts\": alerts}, metrics={\"alerts_found\": len(alerts)}\n            )\n        except Exception as e:",
            "        try:\n            metric_name = payload.get(\"metric_name\")\n            include_history = payload.get(\"include_history\", False)\n            if metric_name:\n                metric = self.state.get_metric(metric_name)\n                if not metric:\n                    return ManagerResult(\n                        success=False,\n                        data={},\n                        metrics={},\n                        error=f\"Metric {metric_name} not found\",\n                    )\n                metrics = {metric_name: metric}\n                if include_history:\n                    metrics[metric_name][\"history\"] = self.state.get_metric_history(metric_name)\n            else:\n                metrics = self.state.get_all_metrics()\n                if include_history:\n                    for name in metrics:\n                        metrics[name][\"history\"] = self.state.get_metric_history(name)\n            return ManagerResult(\n                success=True,\n                data={\"metrics\": metrics},\n                metrics={\"metrics_found\": len(metrics)},\n            )\n        except Exception as e:",
            "        try:\n            widget_type_filter = payload.get(\"widget_type\")\n            widgets = self.state.get_all_widgets()\n            # Apply filters\n            if widget_type_filter:\n                widgets = {k: v for k, v in widgets.items() if v.get(\"type\") == widget_type_filter}\n            return ManagerResult(\n                success=True,\n                data={\"widgets\": widgets},\n                metrics={\"widgets_found\": len(widgets)},\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_rules.py",
          "count": 4,
          "lines": [
            "        try:\n            alert_rules = self.state.get_alert_rules()\n            for rule_name, rule in alert_rules.items():\n                if not rule.get(\"enabled\", True):\n                    continue\n\n                # Check if rule matches alert\n                if not self._rule_matches_alert(rule, alert):\n                    continue\n\n                # Execute rule action\n                self._execute_rule_action(rule, alert)\n\n        except Exception:",
            "        try:\n            updates = {\n                \"escalated\": True,\n                \"escalated_at\": datetime.now().isoformat(),\n                \"escalation_rule\": rule.get(\"name\", \"unknown\"),\n            }\n            self.state.update_alert(alert[\"id\"], updates)\n        except Exception:",
            "        try:\n            updates = {\n                \"notified\": True,\n                \"notified_at\": datetime.now().isoformat(),\n                \"notification_rule\": rule.get(\"name\", \"unknown\"),\n            }\n            self.state.update_alert(alert[\"id\"], updates)\n        except Exception:",
            "        try:\n            updates = {\n                \"resolved\": True,\n                \"resolved_at\": datetime.now().isoformat(),\n                \"auto_resolved\": True,\n                \"auto_resolve_rule\": rule.get(\"name\", \"unknown\"),\n                \"status\": \"resolved\",\n            }\n            self.state.update_alert(alert[\"id\"], updates)\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\widget_manager.py",
          "count": 2,
          "lines": [
            "        try:\n            widget_id = str(uuid.uuid4())\n            widget = {\n                \"widget_id\": widget_id,\n                \"type\": widget_data.get(\"type\", WidgetType.METRIC.value),\n                \"title\": widget_data.get(\"title\", \"\"),\n                \"data_source\": widget_data.get(\"data_source\", \"\"),\n                \"config\": widget_data.get(\"config\", {}),\n                \"created_at\": datetime.now().isoformat(),\n                \"updated_at\": datetime.now().isoformat(),\n            }\n            self.widgets[widget_id] = widget\n\n            return ManagerResult(\n                success=True,\n                data={\"widget_id\": widget_id, \"widget\": widget},\n                message=f\"Widget created: {widget_id}\",\n                errors=[],\n            )\n        except Exception as e:",
            "        try:\n            widget_type = payload.get(\"type\")\n            if widget_type:\n                filtered_widgets = [w for w in self.widgets.values() if w[\"type\"] == widget_type]\n            else:\n                filtered_widgets = list(self.widgets.values())\n\n            return ManagerResult(\n                success=True,\n                data={\"widgets\": filtered_widgets, \"count\": len(filtered_widgets)},\n                message=f\"Found {len(filtered_widgets)} widgets\",\n                errors=[],\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\monitoring\\monitoring_lifecycle.py",
          "count": 3,
          "lines": [
            "            try:\n                # Clean up old resolved alerts (older than 24 hours)\n                self._cleanup_old_alerts()\n\n                # Clean up old metric history (older than 7 days)\n                self._cleanup_old_metrics()\n\n                # Wait 5 minutes before next cleanup cycle\n                if self._stop_event.wait(300):\n                    break\n\n            except Exception:",
            "        try:\n            # Setup default alert rules\n            self._setup_default_alert_rules()\n\n            # Start background monitoring\n            self._start_background_monitoring()\n\n            context.logger(\"Monitoring Lifecycle initialized\")\n            return True\n        except Exception as e:",
            "        try:\n            # Stop background monitoring\n            self._stop_background_monitoring()\n\n            # Clear all state\n            self.state.clear_all()\n\n            context.logger(\"Monitoring Lifecycle cleaned up\")\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\analysis_results_processor.py",
          "count": 1,
          "lines": [
            "        try:\n            analysis_type = result_data.get(\"analysis_type\", \"general\")\n            data_points = result_data.get(\"data_points\", [])\n            analysis_config = result_data.get(\"analysis_config\", {})\n\n            # Perform basic analysis\n            if not data_points:\n                return {\n                    \"analysis_success\": False,\n                    \"error\": \"No data points provided\",\n                    \"original_data\": result_data,\n                }\n\n            # Calculate basic statistics\n            numeric_points = [x for x in data_points if isinstance(x, (int, float))]\n\n            if numeric_points:\n                analysis_result = {\n                    \"count\": len(numeric_points),\n                    \"sum\": sum(numeric_points),\n                    \"average\": sum(numeric_points) / len(numeric_points),\n                    \"min\": min(numeric_points),\n                    \"max\": max(numeric_points),\n                }\n\n                # Calculate variance and standard deviation\n                mean = analysis_result[\"average\"]\n                variance = sum((x - mean) ** 2 for x in numeric_points) / len(numeric_points)\n                analysis_result[\"variance\"] = variance\n                analysis_result[\"std_deviation\"] = variance**0.5\n            else:\n                analysis_result = {\n                    \"count\": len(data_points),\n                    \"data_type\": \"non_numeric\",\n                    \"unique_values\": len(set(str(x) for x in data_points)),\n                }\n\n            return {\n                \"analysis_success\": True,\n                \"analysis_type\": analysis_type,\n                \"analysis_result\": analysis_result,\n                \"data_points_processed\": len(data_points),\n                \"original_data\": result_data,\n            }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\base_results_manager.py",
          "count": 3,
          "lines": [
            "        try:\n            # Results-specific cleanup\n            self.processor.archive_old_results(self.results)\n            self.results.clear()\n            self.result_processors.clear()\n            self.result_callbacks.clear()\n            self.logger.info(\"Results manager cleaned up\")\n\n            # Call parent cleanup\n            return super().cleanup(context)\n        except Exception as e:",
            "        try:\n            processor_type = payload.get(\"processor_type\")\n            processor_func = payload.get(\"processor_func\")\n            if not processor_type or not processor_func:\n                return ManagerResult(\n                    success=False,\n                    data={},\n                    metrics={},\n                    error=\"processor_type and processor_func are required\",\n                )\n            self.result_processors[processor_type] = processor_func\n            return ManagerResult(\n                success=True,\n                data={\"processor_type\": processor_type},\n                metrics={\"processors_registered\": len(self.result_processors)},\n            )\n        except Exception as e:",
            "        try:\n            result_data = payload.get(\"result_data\", {})\n            result_type = payload.get(\"result_type\", \"general\")\n            validation_rules = payload.get(\"validation_rules\", [])\n            callback_key = payload.get(\"callback_key\")\n            result_id = str(uuid.uuid4())\n            result = {\n                \"id\": result_id,\n                \"type\": result_type,\n                \"data\": result_data,\n                \"status\": ResultStatus.PENDING.value,\n                \"created_at\": datetime.now().isoformat(),\n                \"validation_rules\": validation_rules,\n                \"callback_key\": callback_key,\n            }\n            self.results[result_id] = result\n            result[\"status\"] = ResultStatus.PROCESSING.value\n            processed_data = self.processor.process_result_by_type(\n                context, result_type, result_data\n            )\n            validation_passed = self.validator.validate_result(result, validation_rules)\n            if validation_passed:\n                result[\"status\"] = ResultStatus.COMPLETED.value\n                result[\"processed_data\"] = processed_data\n                result[\"completed_at\"] = datetime.now().isoformat()\n                if callback_key and callback_key in self.result_callbacks:\n                    try:\n                        self.result_callbacks[callback_key](result)\n                    except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\results_processing.py",
          "count": 2,
          "lines": [
            "            try:\n                return self.result_processors[result_type](result_data)\n            except Exception as e:",
            "        try:\n            cutoff_date = datetime.now() - timedelta(days=self.archive_after_days)\n            to_archive = []\n            for result_id, result in results.items():\n                created_at = datetime.fromisoformat(result[\"created_at\"])\n                if created_at < cutoff_date:\n                    to_archive.append(result_id)\n            for result_id in to_archive:\n                result = results[result_id]\n                result[\"status\"] = ResultStatus.ARCHIVED.value\n                result[\"archived_at\"] = datetime.now().isoformat()\n                self.archived_results[result_id] = result\n                del results[result_id]\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\results_query_helpers.py",
          "count": 2,
          "lines": [
            "        try:\n            result_id = payload.get(\"result_id\")\n            result_type = payload.get(\"result_type\")\n            status = payload.get(\"status\")\n            include_archived = payload.get(\"include_archived\", False)\n\n            combined_results = dict(results)\n            if include_archived:\n                combined_results.update(archived_results)\n\n            if result_id:\n                combined_results = {k: v for k, v in combined_results.items() if k == result_id}\n            if result_type:\n                combined_results = {\n                    k: v for k, v in combined_results.items() if v.get(\"type\") == result_type\n                }\n            if status:\n                combined_results = {\n                    k: v for k, v in combined_results.items() if v.get(\"status\") == status\n                }\n\n            return ManagerResult(\n                success=True,\n                data={\"results\": combined_results},\n                metrics={\"results_found\": len(combined_results)},\n            )\n        except Exception as e:",
            "        try:\n            result_ids = payload.get(\"result_ids\", [])\n            archive_all = payload.get(\"archive_all\", False)\n\n            if archive_all:\n                result_ids = list(results.keys())\n\n            archived_count = 0\n            for result_id in result_ids:\n                if result_id in results:\n                    result = results[result_id]\n                    result[\"status\"] = ResultStatus.ARCHIVED.value\n                    result[\"archived_at\"] = datetime.now().isoformat()\n                    archived_results[result_id] = result\n                    del results[result_id]\n                    archived_count += 1\n\n            return ManagerResult(\n                success=True,\n                data={\"archived_count\": archived_count},\n                metrics={\"results_archived\": archived_count},\n            )\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\results\\validation_results_processor.py",
          "count": 1,
          "lines": [
            "        try:\n            validation_rules = result_data.get(\"validation_rules\", [])\n            data_to_validate = result_data.get(\"data\", {})\n\n            validation_results = []\n            overall_success = True\n\n            for rule in validation_rules:\n                rule_result = self._validate_rule(rule, data_to_validate)\n                validation_results.append(\n                    {\n                        \"rule\": rule,\n                        \"passed\": rule_result,\n                        \"field\": rule.get(\"field\"),\n                        \"type\": rule.get(\"type\"),\n                    }\n                )\n                if not rule_result:\n                    overall_success = False\n\n            return {\n                \"validation_success\": overall_success,\n                \"validation_results\": validation_results,\n                \"rules_checked\": len(validation_rules),\n                \"rules_passed\": sum(1 for r in validation_results if r[\"passed\"]),\n                \"rules_failed\": sum(1 for r in validation_results if not r[\"passed\"]),\n                \"original_data\": data_to_validate,\n            }\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\domains\\execution_domain_manager.py",
          "count": 7,
          "lines": [
            "            try:\n                return protocol_handler(context, payload)\n            except Exception as e:",
            "            try:\n                service_statuses[name] = manager.get_status()\n            except Exception:",
            "        try:\n            # Cleanup coordinator\n            self.coordinator.cleanup(context)\n\n            # Cleanup service managers\n            for manager in self._service_managers.values():\n                try:\n                    manager.cleanup(context)\n                except Exception:",
            "        try:\n            # Initialize execution coordinator\n            if not self.coordinator.initialize(context):\n                return False\n\n            # Register core execution protocols\n            self._register_core_protocols()\n\n            self.initialized = True\n            return True\n\n        except Exception as e:",
            "        try:\n            # Route task operations to coordinator\n            if operation in {\"execute_task\", \"create_task\", \"get_task_status\"}:\n                return self._execute_task_operation(context, operation, payload)\n\n            # Route service operations to appropriate managers\n            if operation in self._get_service_operations():\n                return self._execute_service_operation(context, operation, payload)\n\n            # Route protocol operations\n            if operation.startswith(\"protocol_\"):\n                return self._execute_protocol_operation(context, operation, payload)\n\n            # Route general execution operations to coordinator\n            return self.coordinator.execute(context, operation, payload)\n\n        except Exception as e:",
            "        try:\n            self._protocols[protocol_name] = protocol_handler\n            return ManagerResult(\n                True, {\"protocol_registered\": protocol_name}, {}, \"Protocol registered\"\n            )\n        except Exception as e:",
            "        try:\n            self._service_managers[service_name] = manager\n            return True\n        except Exception:"
          ]
        },
        {
          "file": "src\\core\\managers\\domains\\resource_domain_manager.py",
          "count": 4,
          "lines": [
            "        try:\n            # Cleanup all operation modules\n            cleanup_results = [\n                self.file_ops.cleanup(context),\n                self.lock_ops.cleanup(context),\n                self.crud_ops.cleanup(context),\n                self.context_ops.cleanup(context),\n            ]\n\n            # Log any cleanup failures but don't fail overall cleanup\n            failed_cleanups = [\n                name for name, result in zip(\n                    [\"file_ops\", \"lock_ops\", \"crud_ops\", \"context_ops\"],\n                    cleanup_results\n                ) if not result\n            ]\n\n            if failed_cleanups:\n                context.logger.warning(f\"Resource cleanup incomplete for: {failed_cleanups}\")\n\n            return True\n\n        except Exception as e:",
            "        try:\n            # Get individual module statuses\n            file_status = self.file_ops.get_status()\n            lock_status = self.lock_ops.get_status()\n            crud_status = self.crud_ops.get_status()\n            context_status = self.context_ops.get_status()\n\n            # Calculate aggregate metrics\n            total_operations = (\n                file_status.get(\"total_operations\", 0) +\n                lock_status.get(\"total_operations\", 0) +\n                crud_status.get(\"total_operations\", 0) +\n                context_status.get(\"total_operations\", 0)\n            )\n\n            active_locks = lock_status.get(\"active_locks\", 0)\n            failed_operations = (\n                file_status.get(\"failed_operations\", 0) +\n                lock_status.get(\"failed_operations\", 0) +\n                crud_status.get(\"failed_operations\", 0) +\n                context_status.get(\"failed_operations\", 0)\n            )\n\n            return {\n                \"health_score\": 100 - min(100, failed_operations * 5),  # Penalty per failure\n                \"total_operations\": total_operations,\n                \"active_locks\": active_locks,\n                \"failed_operations\": failed_operations,\n                \"modules_healthy\": all([\n                    file_status.get(\"healthy\", True),\n                    lock_status.get(\"healthy\", True),\n                    crud_status.get(\"healthy\", True),\n                    context_status.get(\"healthy\", True),\n                ])\n            }\n\n        except Exception:",
            "        try:\n            # Initialize all operation modules\n            modules_initialized = [\n                self.file_ops.initialize(context),\n                self.lock_ops.initialize(context),\n                self.crud_ops.initialize(context),\n                self.context_ops.initialize(context),\n            ]\n\n            # Check if all modules initialized successfully\n            if not all(modules_initialized):\n                failed_modules = [\n                    name for name, init in zip(\n                        [\"file_ops\", \"lock_ops\", \"crud_ops\", \"context_ops\"],\n                        modules_initialized\n                    ) if not init\n                ]\n                context.logger.error(f\"Resource domain initialization failed for: {failed_modules}\")\n                return False\n\n            self.initialized = True\n            return True\n\n        except Exception as e:",
            "        try:\n            # Route file operations\n            if operation in self._get_file_operations():\n                return self._execute_file_operation(context, operation, payload)\n\n            # Route lock operations\n            if operation in self._get_lock_operations():\n                return self._execute_lock_operation(context, operation, payload)\n\n            # Route CRUD operations\n            if operation in self._get_crud_operations():\n                return self._execute_crud_operation(context, operation, payload)\n\n            # Route context operations\n            if operation in self._get_context_operations():\n                return self._execute_context_operation(context, operation, payload)\n\n            return ManagerResult(\n                False, {}, {}, f\"Unknown resource operation: {operation}\"\n            )\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\managers\\domains\\results_domain_manager.py",
          "count": 1,
          "lines": [
            "            try:\n                callback(context, result_id, result_data)\n            except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\orchestration\\base_orchestrator.py",
          "count": 2,
          "lines": [
            "        try:\n            self.logger.info(f\"Cleaning up orchestrator: {self.name}\")\n\n            # Cleanup using lifecycle helper\n            success = OrchestratorLifecycle.cleanup_components(self.components, self.logger)\n\n            # Clear registrations\n            self._component_mgr.clear_all_components()\n            self._event_mgr.clear_listeners()\n\n            self.initialized = False\n            self.logger.info(f\"Orchestrator {self.name} cleaned up successfully\")\n            return success\n\n        except Exception as e:",
            "        try:\n            self.logger.info(f\"Initializing orchestrator: {self.name}\")\n\n            # Register all components\n            self._register_components()\n\n            # Initialize using lifecycle helper\n            success = OrchestratorLifecycle.initialize_components(self.components, self.logger)\n            if not success:\n                return False\n\n            self.initialized = True\n            self.logger.info(f\"Orchestrator {self.name} initialized successfully\")\n            return True\n\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_events.py",
          "count": 1,
          "lines": [
            "            try:\n                callback(data)\n            except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_lifecycle.py",
          "count": 2,
          "lines": [
            "        try:\n            for component_name in reversed(list(components.keys())):\n                component = components[component_name]\n                if hasattr(component, \"cleanup\"):\n                    logger_instance.debug(f\"Cleaning up component: {component_name}\")\n                    component.cleanup()\n            return True\n        except Exception as e:",
            "        try:\n            for component_name, component in components.items():\n                if hasattr(component, \"initialize\"):\n                    logger_instance.debug(f\"Initializing component: {component_name}\")\n                    component.initialize()\n            return True\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\orchestration\\orchestrator_utilities.py",
          "count": 1,
          "lines": [
            "        try:\n            logger_instance.debug(f\"Executing {operation_name}\")\n            result = operation(**kwargs)\n            emit_func(f\"{operation_name}_success\", result)\n            return result\n        except Exception as e:"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_decorators.py",
          "count": 2,
          "lines": [
            "            try:\n                result = func(*args, **kwargs)\n                duration = time.time() - start_time\n                monitor.record_operation_completion(op_name, duration, success=True)\n                return result\n            except Exception as e:",
            "try:\n    from .coordination_performance_monitor import get_performance_monitor\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\performance\\coordination_performance_monitor.py",
          "count": 9,
          "lines": [
            "                try:\n                    # Record system health metrics\n                    self._record_system_health()\n                    time.sleep(60)  # Check every minute\n                except Exception as e:",
            "        try:\n            import psutil\n            memory = psutil.virtual_memory()\n            self.collector.record_metric(\"system_memory_usage\", memory.percent)\n            self.collector.record_metric(\n                \"system_memory_available\", memory.available / (1024**3)\n            )  # GB\n        except ImportError:",
            "    try:\n        from src.core.performance.performance_analyzer import PerformanceAnalyzer\n    except ImportError:",
            "    try:\n        from src.core.performance.performance_collector import PerformanceCollector, MetricType\n    except ImportError:",
            "    try:\n        import logging\n        def get_logger(name):\n            return logging.getLogger(name)\n    except Exception:",
            "try:\n    from .performance_analyzer import PerformanceAnalyzer\nexcept ImportError:",
            "try:\n    from .performance_collector import PerformanceCollector, MetricType\nexcept ImportError:",
            "try:\n    from src.core.config.timeout_constants import TimeoutConstants\nexcept ImportError:",
            "try:\n    from src.utils.logging_utils import get_logger\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_cli.py",
          "count": 2,
          "lines": [
            "    try:\n        if args.command == 'monitor':\n            handle_monitor_command(args)\n        elif args.command == 'optimize':\n            handle_optimize_command(args)\n        elif args.command == 'dashboard':\n            handle_dashboard_command(args)\n        else:\n            logger.info(f'Unknown command: {args.command}')\n            sys.exit(1)\n    except Exception as e:",
            "try:\n    from .performance_optimization_engine import get_optimization_engine, start_performance_optimization, stop_performance_optimization\nexcept ImportError:"
          ]
        },
        {
          "file": "src\\core\\performance\\performance_monitoring_system.py",
          "count": 7,
          "lines": [
            "        try:\n            # Collect current metrics\n            current_metrics = self.collect_metrics()\n\n            # Generate report\n            report_id = f\"perf_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n            # Simple summary\n            summary = f\"Performance report with {len(current_metrics)} metrics\"\n            if current_metrics:\n                cpu_metric = next((m for m in current_metrics if m.name == \"cpu_usage\"), None)\n                if cpu_metric:\n                    summary += f\" - CPU: {cpu_metric.value:.1f}%\"\n\n            report = PerformanceReport(\n                report_id=report_id,\n                timestamp=datetime.now(),\n                metrics=current_metrics,\n                summary=summary,\n            )\n\n            return report\n\n        except Exception as e:",
            "        try:\n            if not self.metrics_history:\n                return {\"message\": \"No metrics collected\"}\n\n            # Simple summary\n            latest_metrics = self.metrics_history[-10:]  # Last 10 metrics\n\n            summary = {\n                \"total_metrics\": len(self.metrics_history),\n                \"latest_metrics\": len(latest_metrics),\n                \"monitoring_active\": self.is_monitoring,\n            }\n\n            # Add latest values if available\n            if latest_metrics:\n                cpu_metrics = [m for m in latest_metrics if m.name == \"cpu_usage\"]\n                if cpu_metrics:\n                    summary[\"latest_cpu\"] = cpu_metrics[-1].value\n\n                memory_metrics = [m for m in latest_metrics if m.name == \"memory_usage\"]\n                if memory_metrics:\n                    summary[\"latest_memory\"] = memory_metrics[-1].value\n\n            return summary\n\n        except Exception as e:",
            "        try:\n            if self.is_monitoring:\n                return True\n\n            self.is_monitoring = True\n            self.logger.info(\"Performance monitoring started\")\n            return True\n\n        except Exception as e:",
            "        try:\n            metrics = []\n            current_time = datetime.now()\n\n            # CPU usage\n            cpu_percent = psutil.cpu_percent(interval=1)\n            metrics.append(\n                PerformanceMetric(\n                    name=\"cpu_usage\",\n                    value=cpu_percent,\n                    timestamp=current_time,\n                    category=\"system\",\n                    unit=\"percent\",\n                )\n            )\n\n            # Memory usage\n            memory = psutil.virtual_memory()\n            metrics.append(\n                PerformanceMetric(\n                    name=\"memory_usage\",\n                    value=memory.percent,\n                    timestamp=current_time,\n                    category=\"system\",\n                    unit=\"percent\",\n                )\n            )\n\n            # Disk usage\n            disk = psutil.disk_usage(\"/\")\n            disk_percent = (disk.used / disk.total) * 100\n            metrics.append(\n                PerformanceMetric(\n                    name=\"disk_usage\",\n                    value=disk_percent,\n                    timestamp=current_time,\n                    category=\"system\",\n                    unit=\"percent\",\n                )\n            )\n\n            # Store metrics\n            self.metrics_history.extend(metrics)\n\n            return metrics\n\n        except Exception as e:",
            "        try:\n            return {\n                \"is_monitoring\": self.is_monitoring,\n                \"metrics_count\": len(self.metrics_history),\n                \"monitoring_interval\": self.monitoring_interval,\n                \"status\": \"active\" if self.is_monitoring else \"stopped\",\n            }\n        except Exception as e:",
            "        try:\n            self.is_monitoring = False\n            self.logger.info(\"Performance monitoring stopped\")\n            return True\n\n        except Exception as e:",
            "        try:\n            self.stop_monitoring()\n            self.metrics_history.clear()\n        except Exception as e:"
          ]
        }
      ],
      "total_count": 1084
    }
  },
  "hotspots": [
    {
      "file": "src\\core\\debate_to_gas_integration.py",
      "total_patterns": 17
    },
    {
      "file": "src\\core\\error_handling\\recovery_strategies.py",
      "total_patterns": 16
    },
    {
      "file": "src\\discord_commander\\controllers\\status_controller_view.py",
      "total_patterns": 14
    },
    {
      "file": "src\\core\\auto_gas_pipeline_system.py",
      "total_patterns": 14
    },
    {
      "file": "src\\core\\message_queue_persistence.py",
      "total_patterns": 14
    },
    {
      "file": "src\\core\\unified_service_base.py",
      "total_patterns": 14
    },
    {
      "file": "src\\core\\service_base.py",
      "total_patterns": 14
    },
    {
      "file": "src\\core\\performance\\coordination_performance_monitor.py",
      "total_patterns": 14
    },
    {
      "file": "src\\swarm_pulse\\intelligence_service.py",
      "total_patterns": 13
    },
    {
      "file": "src\\discord_commander\\trading_data_service.py",
      "total_patterns": 13
    }
  ],
  "duplication_estimate": {
    "total_patterns": 1829,
    "files_analyzed": 1366,
    "avg_patterns_per_file": 1.3389458272327965,
    "high_duplication_files": 129
  }
}