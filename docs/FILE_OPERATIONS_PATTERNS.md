# ğŸ“ File Operations Patterns Documentation

**Module**: `src/utils/unified_file_utils.py`  
**Purpose**: Autonomous file management system  
**Author**: Agent-8 (Operations & Support Specialist)  
**V2 Compliance**: âœ… All files < 400 lines  
**Last Updated**: 2025-10-12

---

## ğŸ¯ **Overview**

The unified file utilities provide a modular, V2-compliant system for all file operations. The system is split into focused modules for better maintainability and autonomous operation.

### **Architecture**

```
src/utils/
â”œâ”€â”€ unified_file_utils.py (233 lines)        # Main interface
â””â”€â”€ file_operations/
    â”œâ”€â”€ __init__.py (47 lines)               # Module exports
    â”œâ”€â”€ backup_operations.py (254 lines)     # Backup & restore
    â”œâ”€â”€ directory_operations.py (60 lines)   # Directory ops
    â”œâ”€â”€ file_metadata.py (96 lines)          # File info
    â”œâ”€â”€ file_serialization.py (82 lines)     # JSON/YAML
    â”œâ”€â”€ scanner_operations.py (127 lines)    # File discovery
    â””â”€â”€ validation_operations.py (127 lines) # Validation
```

---

## ğŸ“š **Module Overview**

### **1. Backup Operations** (`backup_operations.py`)

**Purpose**: Handle backup, restore, and file copying operations

**Classes:**
- `BackupOperations`: Individual file backup/restore
- `BackupManager`: Agent workspace backup management

**Key Patterns:**

#### **Pattern: Safe File Backup**
```python
from src.utils.unified_file_utils import BackupOperations

# Create backup before modification
backup_path = BackupOperations.create_backup('config.json')
if backup_path:
    # Modify file safely
    modify_file('config.json')
    # If error, can restore from backup_path
```

#### **Pattern: Workspace Backup**
```python
from src.utils.unified_file_utils import create_backup_manager

# Create backup manager
manager = create_backup_manager(
    root='agent_workspaces/Agent-8',
    dest='backups/'
)

# Backup specific agents
backup_dir = manager.create_backup(agents=['Agent-8', 'Agent-7'])

# List available backups
backups = manager.list_backups()

# Restore from backup
manager.restore_backup(backup_dir)

# Cleanup old backups (keep 5 most recent)
deleted = manager.cleanup_old_backups(keep_count=5)
```

#### **Pattern: Safe File Operations**
```python
# Copy with directory creation
BackupOperations.copy_file('source.json', 'dest/path/file.json')

# Safe delete with auto-backup
BackupOperations.safe_delete_file('old_file.json')
# Creates old_file.json.pre_delete_backup before deleting
```

---

### **2. Validation Operations** (`validation_operations.py`)

**Purpose**: Comprehensive file validation and safety checks

**Classes:**
- `FileValidator`: Validation logic
- `FileValidationResult`: Validation results dataclass

**Key Patterns:**

#### **Pattern: Comprehensive Validation**
```python
from src.utils.unified_file_utils import FileValidator

result = FileValidator.validate_file_path('config.json')

if result.exists and result.readable and result.writable:
    # Safe to read/write
    process_file(result.path)
else:
    print(f"Validation errors: {result.errors}")
```

#### **Pattern: Path Safety Check**
```python
# Prevent directory traversal attacks
allowed_dirs = ['agent_workspaces/', 'data/']

if FileValidator.is_path_safe(user_path, allowed_dirs):
    # Safe to access
    process_file(user_path)
else:
    raise SecurityError("Path outside allowed directories")
```

#### **Pattern: Extension Validation**
```python
# Only allow specific file types
allowed = ['.json', '.yaml', '.yml']

if FileValidator.validate_file_extension(file_path, allowed):
    # Safe to process
    load_config(file_path)
```

---

### **3. Scanner Operations** (`scanner_operations.py`)

**Purpose**: Efficient directory scanning and file discovery

**Classes:**
- `UnifiedFileScanner`: Directory scanning with filtering

**Key Patterns:**

#### **Pattern: Extension-Based Scanning**
```python
from src.utils.unified_file_utils import UnifiedFileScanner

scanner = UnifiedFileScanner('src/')

# Scan for Python files only
python_files = scanner.scan_directory(extensions=['.py'])

# Scan for config files
config_files = scanner.scan_directory(
    extensions=['.json', '.yaml', '.yml']
)
```

#### **Pattern: Pattern Exclusion**
```python
# Exclude common directories
files = scanner.scan_directory(
    extensions=['.py'],
    exclude_patterns=['__pycache__', '.git', 'node_modules', 'venv']
)
```

#### **Pattern: Glob Pattern Scanning**
```python
# Use glob patterns for complex matching
test_files = scanner.scan_by_pattern('**/test_*.py')
config_files = scanner.scan_by_pattern('**/*config*.json')
```

#### **Pattern: Scan Analytics**
```python
# Scan directory
scanner.scan_directory(extensions=['.py', '.js', '.ts'])

# Get all scanned files
all_files = scanner.get_scanned_files()

# Count by extension
counts = scanner.count_files_by_extension()
# Returns: {'.py': 150, '.js': 80, '.ts': 45}

# Reset for new scan
scanner.reset_scan()
```

---

### **4. Directory Operations** (`directory_operations.py`)

**Purpose**: Directory listing and size calculation

**Key Patterns:**

#### **Pattern: Directory Listing**
```python
from src.utils.unified_file_utils import DirectoryOperations

ops = DirectoryOperations()

# List all files
all_files = ops.list_files('src/')

# List with pattern
py_files = ops.list_files('src/', '*.py')
```

#### **Pattern: Directory Size**
```python
# Get total directory size
size_bytes = ops.get_directory_size('agent_workspaces/')
size_mb = size_bytes / (1024 * 1024)

print(f"Workspace size: {size_mb:.2f} MB")
```

---

### **5. File Metadata** (`file_metadata.py`)

**Purpose**: File information and metadata retrieval

**Key Patterns:**

#### **Pattern: File Existence & Info**
```python
from src.utils.unified_file_utils import FileMetadataOperations

ops = FileMetadataOperations()

if ops.file_exists('config.json'):
    size = ops.get_file_size('config.json')
    hash = ops.get_file_hash('config.json')
    modified = ops.get_file_modified_time('config.json')
    
    print(f"Size: {size} bytes")
    print(f"Hash: {hash}")
    print(f"Modified: {modified}")
```

#### **Pattern: File Permissions**
```python
if ops.is_file_readable('data.json'):
    data = read_file('data.json')

if ops.is_file_writable('log.txt'):
    write_log('log.txt', message)
```

---

### **6. Data Serialization** (`file_serialization.py`)

**Purpose**: JSON and YAML serialization with safety

**Key Patterns:**

#### **Pattern: Safe JSON Operations**
```python
from src.utils.unified_file_utils import DataSerializationOperations

ops = DataSerializationOperations()

# Read with error handling
data = ops.read_json('config.json')
if data:
    process_config(data)

# Write with directory creation
success = ops.write_json('output/results.json', results)
```

#### **Pattern: YAML Operations**
```python
# Read YAML
config = ops.read_yaml('config.yaml')

# Write YAML
ops.write_yaml('output.yaml', data)
```

#### **Pattern: Directory Ensuring**
```python
# Ensure directory exists before write
ops.ensure_directory('deep/nested/path/')
# Creates all parent directories if needed
```

---

## ğŸš€ **Unified Interface Pattern**

The `UnifiedFileUtils` class provides a single interface to all operations:

```python
from src.utils.unified_file_utils import UnifiedFileUtils

# Create unified utilities instance
utils = UnifiedFileUtils()

# File metadata
if utils.file_exists('config.json'):
    size = utils.get_file_size('config.json')
    hash = utils.get_file_hash('config.json')

# Serialization
data = utils.read_json('config.json')
utils.write_yaml('config.yaml', data)

# Directory operations
files = utils.list_files('src/', '*.py')
dir_size = utils.get_directory_size('src/')

# Backup operations
backup = utils.create_backup('config.json')
utils.copy_file('source.txt', 'dest.txt')

# Validation
result = utils.validate_file('config.json')
if result.readable and result.writable:
    process_file(result.path)
```

---

## ğŸ¤– **Autonomous Operation Patterns**

### **Pattern: Self-Healing File Operations**

```python
from src.utils.unified_file_utils import (
    UnifiedFileUtils,
    BackupOperations,
    FileValidator
)

def autonomous_file_update(file_path: str, new_data: dict):
    """Autonomous file update with safety checks and rollback."""
    utils = UnifiedFileUtils()
    
    # Validate before operation
    result = utils.validate_file(file_path)
    if not result.writable:
        raise PermissionError(f"Cannot write to {file_path}")
    
    # Create backup
    backup = BackupOperations.create_backup(file_path)
    if not backup:
        raise RuntimeError("Failed to create backup")
    
    try:
        # Attempt update
        success = utils.write_json(file_path, new_data)
        if not success:
            raise RuntimeError("Write failed")
            
        # Verify update
        verify_data = utils.read_json(file_path)
        if verify_data != new_data:
            raise RuntimeError("Data verification failed")
            
        return True
        
    except Exception as e:
        # Auto-rollback on error
        BackupOperations.restore_from_backup(backup, file_path)
        raise RuntimeError(f"Update failed, rolled back: {e}")
```

### **Pattern: Autonomous File Discovery**

```python
from src.utils.unified_file_utils import UnifiedFileScanner

def discover_config_files(root: str) -> dict:
    """Autonomously discover and categorize config files."""
    scanner = UnifiedFileScanner(root)
    
    # Scan for all config files
    config_files = scanner.scan_directory(
        extensions=['.json', '.yaml', '.yml', '.toml'],
        exclude_patterns=['node_modules', '__pycache__', '.git']
    )
    
    # Categorize by extension
    categorized = scanner.count_files_by_extension()
    
    return {
        'files': config_files,
        'counts': categorized,
        'total': len(config_files)
    }
```

### **Pattern: Autonomous Workspace Management**

```python
from src.utils.unified_file_utils import create_backup_manager

def autonomous_workspace_backup(agent_id: str):
    """Autonomously backup agent workspace with cleanup."""
    manager = create_backup_manager(
        root=f'agent_workspaces/{agent_id}',
        dest=f'backups/{agent_id}'
    )
    
    # Create timestamped backup
    backup_path = manager.create_backup()
    
    # Cleanup old backups (keep 5 most recent)
    deleted = manager.cleanup_old_backups(keep_count=5)
    
    return {
        'backup_path': backup_path,
        'deleted_count': deleted,
        'available_backups': manager.list_backups()
    }
```

---

## ğŸ“Š **V2 Compliance Metrics**

### **File Size Compliance**

| File | Lines | Status |
|------|-------|--------|
| unified_file_utils.py | 233 | âœ… Compliant |
| backup_operations.py | 254 | âœ… Compliant |
| scanner_operations.py | 127 | âœ… Compliant |
| validation_operations.py | 127 | âœ… Compliant |
| directory_operations.py | 60 | âœ… Compliant |
| file_metadata.py | 96 | âœ… Compliant |
| file_serialization.py | 82 | âœ… Compliant |
| __init__.py | 47 | âœ… Compliant |

**Total**: 8 files, all < 400 lines âœ…

### **Refactor Results**

- **Before**: 1 file, 321 lines
- **After**: 8 files, avg 141 lines/file
- **Reduction**: Main file reduced by 88 lines (27%)
- **Modularity**: âœ… Excellent (7 focused modules)
- **Documentation**: âœ… Comprehensive
- **Test Coverage**: âœ… Import-verified

---

## ğŸ¯ **Best Practices**

### **1. Always Validate Before Operations**
```python
# âœ… Good
result = utils.validate_file(path)
if result.writable:
    utils.write_json(path, data)

# âŒ Bad
utils.write_json(path, data)  # No validation
```

### **2. Use Backups for Critical Operations**
```python
# âœ… Good
backup = BackupOperations.create_backup(path)
modify_file(path)

# âŒ Bad
modify_file(path)  # No backup
```

### **3. Handle Exceptions Gracefully**
```python
# âœ… Good
try:
    data = utils.read_json(path)
except Exception as e:
    logger.error(f"Failed to read {path}: {e}")
    return None

# âŒ Bad
data = utils.read_json(path)  # No error handling
```

### **4. Use Path Safety Checks**
```python
# âœ… Good
if FileValidator.is_path_safe(user_path, allowed_dirs):
    process_file(user_path)

# âŒ Bad
process_file(user_path)  # Security risk
```

---

## ğŸ”„ **Migration Guide**

### **Old Code:**
```python
# Direct imports
from src.utils.unified_file_utils import (
    BackupOperations,
    BackupManager,
    FileValidator
)
```

### **New Code (Recommended):**
```python
# Use unified interface
from src.utils.unified_file_utils import UnifiedFileUtils

utils = UnifiedFileUtils()
utils.create_backup('file.json')
utils.validate_file('file.json')
```

### **Or Specific Imports:**
```python
# Import from submodules
from src.utils.file_operations import (
    BackupOperations,
    BackupManager,
    FileValidator
)
```

**Both approaches work!** The unified interface is recommended for simplicity.

---

## ğŸ“ **Summary**

The unified file utilities system provides:

âœ… **Modular Architecture**: 7 focused modules, all V2 compliant  
âœ… **Comprehensive Operations**: Backup, validate, scan, serialize  
âœ… **Autonomous Patterns**: Self-healing, auto-discovery, auto-management  
âœ… **Safety First**: Validation, backups, error handling  
âœ… **Easy Integration**: Single interface or modular imports  
âœ… **Well Documented**: Complete pattern library  

**Perfect for autonomous agent file operations!** ğŸ¤–âœ¨

---

**Documentation Owner**: Agent-8 (Operations & Support Specialist)  
**Position**: (1611, 941) Monitor 2, Bottom-Right  
**Status**: âœ… COMPLETE  
**Framework**: V2 Compliance, SSOT Principles  

ğŸ **WE. ARE. SWARM.** âš¡

