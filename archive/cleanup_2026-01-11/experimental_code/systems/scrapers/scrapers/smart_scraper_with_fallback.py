#!/usr/bin/env python3
"""
Smart Scraper with Cookie Fallback
Tries cookies first, falls back to manual login if needed, then does aggressive scrolling.
"""

import sys
import os
import time
import json
from datetime import datetime

# Add the project root to the path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    import undetected_chromedriver as uc
    print("‚úÖ undetected-chromedriver available")
except ImportError:
    print("‚ùå undetected-chromedriver not available")
    sys.exit(1)

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, StaleElementReferenceException
from dreamscape.scrapers.cookie_manager import CookieManager
from dreamscape.scrapers.login_handler import LoginHandler
from dreamscape.scrapers.browser_manager import BrowserManager
from dreamscape.scrapers.login_utils import ensure_login_unified, create_login_components

class SmartConversationScraper:
    """Smart conversation scraper with cookie fallback and aggressive scrolling."""
    
    def __init__(self, timeout: int = 30):
        self.timeout = timeout
    
    def ensure_login(self, driver, cookie_manager, login_handler):
        """Ensure we're logged in using the unified login utility."""
        return ensure_login_unified(driver, cookie_manager, login_handler)
    
    def get_conversation_list(self, driver) -> list:
        """Get all conversations with aggressive scrolling."""
        if not driver:
            print("‚ùå No driver provided")
            return []
        
        try:
            print("üìã Extracting conversation list with aggressive scrolling...")
            
            # Wait for page to load
            wait = WebDriverWait(driver, self.timeout)
            
            # Find the conversation sidebar
            sidebar_selectors = [
                "//div[contains(@class, 'bg-token-sidebar-surface-primary')]",
                "//div[contains(@class, 'sidebar')]",
                "//nav[contains(@class, 'sidebar')]",
                "//aside[contains(@class, 'sidebar')]",
                "//div[contains(@class, 'conversations')]",
                "//nav[contains(@class, 'conversations')]",
                "//div[contains(@class, 'flex-col') and contains(@class, 'overflow')]"
            ]
            
            sidebar = None
            for selector in sidebar_selectors:
                try:
                    sidebar = wait.until(EC.presence_of_element_located((By.XPATH, selector)))
                    print(f"‚úÖ Found sidebar using selector: {selector}")
                    break
                except TimeoutException:
                    continue
            
            if not sidebar:
                print("‚ùå Could not find conversation sidebar")
                return []
            
            # Wait for initial conversations to load
            print("‚è≥ Waiting for initial conversations to load...")
            time.sleep(5)
            
            # Super aggressive scrolling for large conversation lists
            print("üîÑ Starting super aggressive scrolling...")
            self._super_aggressive_scroll(driver, sidebar)
            
            # Extract all conversations
            print("üìù Extracting conversation data...")
            conversations = self._extract_conversations(driver)
            
            print(f"‚úÖ Successfully extracted {len(conversations)} conversations!")
            return conversations
            
        except Exception as e:
            print(f"‚ùå Error extracting conversations: {e}")
            return []
    
    def _super_aggressive_scroll(self, driver, sidebar):
        """Super aggressive scrolling for very large conversation lists."""
        try:
            # Get initial count
            initial_count = len(driver.find_elements(By.XPATH, "//a[contains(@href, '/c/')]"))
            print(f"üìä Initial conversation count: {initial_count}")
            
            # Very aggressive scrolling
            max_scrolls = 150  # Even higher limit for 1,300+ conversations
            no_change_count = 0
            max_no_change = 10  # More patience
            
            for i in range(max_scrolls):
                # Scroll to bottom
                driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", sidebar)
                time.sleep(2)
                
                # Check new count
                new_count = len(driver.find_elements(By.XPATH, "//a[contains(@href, '/c/')]"))
                
                # Progress indicator
                if (i + 1) % 10 == 0:
                    print(f"üìà Scroll {i+1}/{max_scrolls}: {new_count} conversations")
                
                # Check if we're still loading new conversations
                if new_count <= initial_count:
                    no_change_count += 1
                    
                    # Try more aggressive scrolling after a few attempts
                    if no_change_count >= 3:
                        print("üöÄ Trying burst scrolling...")
                        for j in range(10):  # Multiple quick scrolls
                            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", sidebar)
                            time.sleep(0.3)
                        
                        # Check again
                        new_count = len(driver.find_elements(By.XPATH, "//a[contains(@href, '/c/')]"))
                        print(f"üìä After burst scrolling: {new_count} conversations")
                        
                        if new_count <= initial_count:
                            no_change_count += 1
                        else:
                            no_change_count = 0
                else:
                    no_change_count = 0
                
                # Stop if no changes for several attempts
                if no_change_count >= max_no_change:
                    print(f"‚èπÔ∏è Stopping scroll after {max_no_change} attempts with no new conversations")
                    break
                
                initial_count = new_count
            
            final_count = len(driver.find_elements(By.XPATH, "//a[contains(@href, '/c/')]"))
            print(f"üéØ Final conversation count after scrolling: {final_count}")
            
            # If we still have a low count, try one more round
            if final_count < 1000:  # If less than 1000, try more
                print("üîÑ Low count detected, trying final aggressive scrolling...")
                for i in range(30):
                    driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", sidebar)
                    time.sleep(1)
                
                final_count = len(driver.find_elements(By.XPATH, "//a[contains(@href, '/c/')]"))
                print(f"üìä After final scrolling: {final_count} conversations")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error during scrolling: {e}")
    
    def _extract_conversations(self, driver) -> list:
        """Extract conversation data from the page."""
        try:
            # Get all conversation links
            conversation_links = driver.find_elements(By.XPATH, "//a[contains(@href, '/c/')]")
            
            conversations = []
            seen_ids = set()
            
            for link in conversation_links:
                try:
                    href = link.get_attribute('href')
                    if not href:
                        continue
                    
                    # Extract conversation ID
                    conversation_id = None
                    if '/c/' in href:
                        conversation_id = href.split('/c/')[-1].split('?')[0]
                    
                    if not conversation_id or conversation_id in seen_ids:
                        continue
                    
                    seen_ids.add(conversation_id)
                    
                    # Get title
                    title = link.text.strip()
                    if not title:
                        try:
                            parent = link.find_element(By.XPATH, "./..")
                            title = parent.text.strip()
                        except:
                            title = f"Conversation {conversation_id[:8]}"
                    
                    # Skip demo conversations
                    if 'demo' in title.lower() or 'demo' in conversation_id.lower():
                        continue
                    
                    conversations.append({
                        'id': conversation_id,
                        'title': title,
                        'url': href
                    })
                    
                except StaleElementReferenceException:
                    continue
                except Exception as e:
                    print(f"‚ö†Ô∏è Error extracting conversation: {e}")
                    continue
            
            return conversations
            
        except Exception as e:
            print(f"‚ùå Error extracting conversations: {e}")
            return []

def main():
    print("üöÄ Smart Scraper with Cookie Fallback")
    print("=" * 45)
    
    try:
        # EDIT START: Use BrowserManager for automatic driver management
        print("üì± Creating browser driver with automatic version management...")
        driver = BrowserManager(headless=False, use_undetected=True).create_driver()
        if not driver:
            print("‚ùå Failed to create browser driver")
            return 1
        print("‚úÖ Browser driver created successfully")
        # EDIT END
        
        # Navigate to ChatGPT
        print("üåê Navigating to ChatGPT...")
        driver.get('https://chat.openai.com/')
        print("‚úÖ Navigated to ChatGPT")
        
        # Wait for page to load
        time.sleep(3)
        
        # Initialize managers using unified components
        cookie_manager, login_handler = create_login_components('data/chatgpt_cookies.pkl')
        scraper = SmartConversationScraper()
        
        # Ensure login (tries cookies first, falls back to manual)
        if not scraper.ensure_login(driver, cookie_manager, login_handler):
            print("‚ùå Failed to login")
            driver.quit()
            return 1
        
        # Extract conversations with super aggressive scrolling
        print("üìã Extracting conversations with super aggressive scrolling...")
        conversations = scraper.get_conversation_list(driver)
        
        if conversations:
            print(f"üéâ Successfully found {len(conversations)} conversations!")
            
            # Save to file
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'data/conversations/smart_scraper_{timestamp}.json'
            
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(conversations, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Saved to {output_file}")
            
            # Show first 20 conversations
            print("\nüìù First 20 conversations:")
            for i, conv in enumerate(conversations[:20]):
                title = conv.get('title', 'No title')
                conv_id = conv.get('id', 'No ID')
                print(f"  {i+1:2d}. {title} (ID: {conv_id})")
            
            if len(conversations) > 20:
                print(f"  ... and {len(conversations) - 20} more conversations")
            
        else:
            print("‚ùå No conversations found")
        
        # Close browser
        driver.quit()
        print("‚úÖ Browser closed")
        
        print("\nüéâ Smart scraper completed!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 