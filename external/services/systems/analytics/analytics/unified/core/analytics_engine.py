"""
Unified Analytics Engine - Core Analytics Foundation
==================================================

This module provides the core analytics engine that serves as the foundation
for the unified analytics system. It handles common analytics functionality,
data management, and coordination between different analytics modules.
"""

import json
import logging
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
from collections import defaultdict, Counter
import statistics
import functools
import sqlite3
import threading
from dataclasses_json import dataclass_json

logger = logging.getLogger(__name__)


class AnalyticsType(Enum):
    """Types of analytics available in the unified system."""
    CONTENT_QUALITY = "content_quality"
    TEMPLATE_PERFORMANCE = "template_performance"
    TIME_SERIES = "time_series"
    TOPIC_ANALYSIS = "topic_analysis"
    CONVERSATION_ANALYTICS = "conversation_analytics"
    INTEGRATED_ANALYTICS = "integrated_analytics"


class InsightType(Enum):
    """Types of insights generated by the analytics system."""
    PERFORMANCE_TREND = "performance_trend"
    QUALITY_IMPROVEMENT = "quality_improvement"
    OPTIMIZATION_OPPORTUNITY = "optimization_opportunity"
    BREAKTHROUGH_DETECTED = "breakthrough_detected"
    ANOMALY_DETECTED = "anomaly_detected"
    PATTERN_RECOGNIZED = "pattern_recognized"
    CORRELATION_FOUND = "correlation_found"


@dataclass_json
@dataclass
class AnalyticsConfig:
    """Configuration for the unified analytics system."""
    enabled: bool = True
    data_retention_days: int = 90
    auto_analysis: bool = True
    export_formats: List[str] = None
    confidence_threshold: float = 0.7
    cache_enabled: bool = True
    cache_ttl_seconds: int = 3600
    max_concurrent_analyses: int = 5
    batch_size: int = 100
    
    def __post_init__(self):
        if self.export_formats is None:
            self.export_formats = ["json", "csv", "html", "pdf"]


@dataclass_json
@dataclass
class AnalyticsInsight:
    """Individual analytics insight with metadata."""
    insight_id: str
    insight_type: InsightType
    title: str
    description: str
    confidence: float
    impact_score: float
    recommendations: List[str]
    metadata: Dict[str, Any]
    timestamp: datetime
    source_data: Dict[str, Any]
    analytics_type: AnalyticsType
    tags: List[str] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []


@dataclass_json
@dataclass
class AnalyticsMetrics:
    """Base analytics metrics with standardized structure."""
    metric_id: str
    metric_name: str
    value: float
    unit: str
    timestamp: datetime
    metadata: Dict[str, Any]
    analytics_type: AnalyticsType
    confidence: float = 1.0
    trend: Optional[str] = None  # increasing, decreasing, stable


@dataclass_json
@dataclass
class UnifiedAnalyticsReport:
    """Comprehensive analytics report combining all analytics types."""
    report_id: str
    generated_at: datetime
    time_period: str
    analytics_summary: Dict[str, Any]
    insights: List[AnalyticsInsight]
    metrics: List[AnalyticsMetrics]
    correlations: Dict[str, float]
    recommendations: List[str]
    export_data: Dict[str, Any]
    confidence_score: float
    data_coverage: float


class UnifiedAnalyticsEngine:
    """
    Core analytics engine that coordinates all analytics modules.
    
    This engine provides:
    - Centralized data management
    - Cross-analytics correlations
    - Unified reporting
    - Performance optimization
    - Caching and batching
    """
    
    def __init__(self, data_dir: str = "data/unified_analytics", config: AnalyticsConfig = None):
        """Initialize the unified analytics engine."""
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.config = config or AnalyticsConfig()
        
        # Initialize databases
        self._init_databases()
        
        # Analytics modules (to be initialized by subclasses)
        self.analytics_modules: Dict[AnalyticsType, Any] = {}
        
        # Caching
        self._cache = {}
        self._cache_lock = threading.Lock()
        
        # Performance tracking
        self.performance_metrics = defaultdict(list)
        self.analysis_history = []
        
        logger.info(f"Unified Analytics Engine initialized at {self.data_dir}")
    
    def _init_databases(self):
        """Initialize analytics databases."""
        # Main analytics database
        self.analytics_db = self.data_dir / "analytics.db"
        self._create_analytics_tables()
        
        # Cache database
        self.cache_db = self.data_dir / "cache.db"
        self._create_cache_tables()
        
        # Performance database
        self.performance_db = self.data_dir / "performance.db"
        self._create_performance_tables()
    
    def _create_analytics_tables(self):
        """Create analytics database tables."""
        with sqlite3.connect(self.analytics_db) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS analytics_insights (
                    insight_id TEXT PRIMARY KEY,
                    insight_type TEXT NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    confidence REAL,
                    impact_score REAL,
                    recommendations TEXT,
                    metadata TEXT,
                    timestamp TEXT,
                    source_data TEXT,
                    analytics_type TEXT,
                    tags TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS analytics_metrics (
                    metric_id TEXT PRIMARY KEY,
                    metric_name TEXT NOT NULL,
                    value REAL,
                    unit TEXT,
                    timestamp TEXT,
                    metadata TEXT,
                    analytics_type TEXT,
                    confidence REAL,
                    trend TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS analytics_reports (
                    report_id TEXT PRIMARY KEY,
                    generated_at TEXT,
                    time_period TEXT,
                    analytics_summary TEXT,
                    insights TEXT,
                    metrics TEXT,
                    correlations TEXT,
                    recommendations TEXT,
                    export_data TEXT,
                    confidence_score REAL,
                    data_coverage REAL
                )
            """)
    
    def _create_cache_tables(self):
        """Create cache database tables."""
        with sqlite3.connect(self.cache_db) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS analytics_cache (
                    cache_key TEXT PRIMARY KEY,
                    cache_value TEXT,
                    created_at TEXT,
                    expires_at TEXT
                )
            """)
    
    def _create_performance_tables(self):
        """Create performance database tables."""
        with sqlite3.connect(self.performance_db) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    metric_id TEXT PRIMARY KEY,
                    operation TEXT,
                    duration REAL,
                    timestamp TEXT,
                    metadata TEXT
                )
            """)
    
    def register_analytics_module(self, analytics_type: AnalyticsType, module: Any):
        """Register an analytics module with the engine."""
        self.analytics_modules[analytics_type] = module
        logger.info(f"Registered analytics module: {analytics_type.value}")
    
    def get_analytics_module(self, analytics_type: AnalyticsType) -> Optional[Any]:
        """Get a registered analytics module."""
        return self.analytics_modules.get(analytics_type)
    
    def analyze_data(self, data: Dict[str, Any], analytics_types: List[AnalyticsType] = None) -> Dict[AnalyticsType, Any]:
        """Analyze data using specified analytics modules."""
        if analytics_types is None:
            analytics_types = list(self.analytics_modules.keys())
        
        results = {}
        
        for analytics_type in analytics_types:
            if analytics_type in self.analytics_modules:
                try:
                    start_time = datetime.now()
                    module = self.analytics_modules[analytics_type]
                    
                    # Check cache first
                    cache_key = self._generate_cache_key(data, analytics_type)
                    cached_result = self._get_cached_result(cache_key)
                    
                    if cached_result is not None:
                        results[analytics_type] = cached_result
                        logger.debug(f"Using cached result for {analytics_type.value}")
                    else:
                        # Perform analysis
                        result = module.analyze(data)
                        results[analytics_type] = result
                        
                        # Cache the result
                        self._cache_result(cache_key, result)
                        
                        # Track performance
                        duration = (datetime.now() - start_time).total_seconds()
                        self._track_performance(analytics_type.value, duration)
                        
                        logger.debug(f"Analysis completed for {analytics_type.value} in {duration:.2f}s")
                
                except Exception as e:
                    logger.error(f"Error in {analytics_type.value} analysis: {e}")
                    results[analytics_type] = {"error": str(e)}
        
        return results
    
    def generate_unified_report(self, time_period: str = "30d") -> UnifiedAnalyticsReport:
        """Generate a comprehensive report combining all analytics."""
        start_time = datetime.now()
        
        # Collect data from all modules
        all_insights = []
        all_metrics = []
        analytics_summary = {}
        
        for analytics_type, module in self.analytics_modules.items():
            try:
                # Get insights from module
                module_insights = module.get_insights(time_period)
                all_insights.extend(module_insights)
                
                # Get metrics from module
                module_metrics = module.get_metrics(time_period)
                all_metrics.extend(module_metrics)
                
                # Get summary from module
                module_summary = module.get_summary(time_period)
                analytics_summary[analytics_type.value] = module_summary
                
            except Exception as e:
                logger.error(f"Error getting data from {analytics_type.value}: {e}")
        
        # Generate correlations
        correlations = self._generate_correlations(all_metrics)
        
        # Generate recommendations
        recommendations = self._generate_recommendations(all_insights, analytics_summary)
        
        # Calculate confidence and coverage
        confidence_score = self._calculate_confidence_score(all_insights)
        data_coverage = self._calculate_data_coverage(analytics_summary)
        
        # Create report
        report = UnifiedAnalyticsReport(
            report_id=f"unified_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            generated_at=datetime.now(),
            time_period=time_period,
            analytics_summary=analytics_summary,
            insights=all_insights,
            metrics=all_metrics,
            correlations=correlations,
            recommendations=recommendations,
            export_data=self._prepare_export_data(all_insights, all_metrics, analytics_summary),
            confidence_score=confidence_score,
            data_coverage=data_coverage
        )
        
        # Save report
        self._save_report(report)
        
        # Track performance
        duration = (datetime.now() - start_time).total_seconds()
        self._track_performance("unified_report_generation", duration)
        
        logger.info(f"Unified report generated in {duration:.2f}s")
        return report
    
    def _generate_cache_key(self, data: Dict[str, Any], analytics_type: AnalyticsType) -> str:
        """Generate a cache key for data and analytics type."""
        import hashlib
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.md5(f"{data_str}_{analytics_type.value}".encode()).hexdigest()
    
    def _get_cached_result(self, cache_key: str) -> Optional[Any]:
        """Get a cached result."""
        if not self.config.cache_enabled:
            return None
        
        with sqlite3.connect(self.cache_db) as conn:
            cursor = conn.execute(
                "SELECT cache_value, expires_at FROM analytics_cache WHERE cache_key = ?",
                (cache_key,)
            )
            result = cursor.fetchone()
            
            if result:
                cache_value, expires_at = result
                if datetime.fromisoformat(expires_at) > datetime.now():
                    return json.loads(cache_value)
                else:
                    # Remove expired cache entry
                    conn.execute("DELETE FROM analytics_cache WHERE cache_key = ?", (cache_key,))
        
        return None
    
    def _cache_result(self, cache_key: str, result: Any):
        """Cache a result."""
        if not self.config.cache_enabled:
            return
        
        expires_at = datetime.now() + timedelta(seconds=self.config.cache_ttl_seconds)
        
        with sqlite3.connect(self.cache_db) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO analytics_cache (cache_key, cache_value, created_at, expires_at) VALUES (?, ?, ?, ?)",
                (cache_key, json.dumps(result), datetime.now().isoformat(), expires_at.isoformat())
            )
    
    def _track_performance(self, operation: str, duration: float):
        """Track performance metrics."""
        metric_id = f"{operation}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        with sqlite3.connect(self.performance_db) as conn:
            conn.execute(
                "INSERT INTO performance_metrics (metric_id, operation, duration, timestamp, metadata) VALUES (?, ?, ?, ?, ?)",
                (metric_id, operation, duration, datetime.now().isoformat(), json.dumps({}))
            )
    
    def _generate_correlations(self, metrics: List[AnalyticsMetrics]) -> Dict[str, float]:
        """Generate correlations between different metrics."""
        correlations = {}
        
        # Group metrics by type
        metrics_by_type = defaultdict(list)
        for metric in metrics:
            metrics_by_type[metric.analytics_type].append(metric)
        
        # Calculate correlations between different analytics types
        analytics_types = list(metrics_by_type.keys())
        for i, type1 in enumerate(analytics_types):
            for type2 in analytics_types[i+1:]:
                correlation = self._calculate_correlation(
                    metrics_by_type[type1], 
                    metrics_by_type[type2]
                )
                if correlation is not None:
                    key = f"{type1.value}_vs_{type2.value}"
                    correlations[key] = correlation
        
        return correlations
    
    def _calculate_correlation(self, metrics1: List[AnalyticsMetrics], metrics2: List[AnalyticsMetrics]) -> Optional[float]:
        """Calculate correlation between two sets of metrics."""
        if len(metrics1) < 2 or len(metrics2) < 2:
            return None
        
        try:
            values1 = [m.value for m in metrics1]
            values2 = [m.value for m in metrics2]
            
            # Ensure same length by truncating to shorter list
            min_length = min(len(values1), len(values2))
            values1 = values1[:min_length]
            values2 = values2[:min_length]
            
            if len(values1) < 2:
                return None
            
            # Calculate Pearson correlation
            mean1 = statistics.mean(values1)
            mean2 = statistics.mean(values2)
            
            numerator = sum((x - mean1) * (y - mean2) for x, y in zip(values1, values2))
            denominator1 = sum((x - mean1) ** 2 for x in values1)
            denominator2 = sum((y - mean2) ** 2 for y in values2)
            
            if denominator1 == 0 or denominator2 == 0:
                return None
            
            correlation = numerator / (denominator1 * denominator2) ** 0.5
            return max(-1.0, min(1.0, correlation))  # Clamp to [-1, 1]
            
        except Exception as e:
            logger.error(f"Error calculating correlation: {e}")
            return None
    
    def _generate_recommendations(self, insights: List[AnalyticsInsight], summary: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on insights and summary."""
        recommendations = []
        
        # Analyze insights for patterns
        insight_types = Counter(insight.insight_type for insight in insights)
        
        # Generate recommendations based on insight types
        if insight_types[InsightType.QUALITY_IMPROVEMENT] > 3:
            recommendations.append("Multiple quality improvement opportunities detected. Consider implementing systematic quality enhancement processes.")
        
        if insight_types[InsightType.OPTIMIZATION_OPPORTUNITY] > 2:
            recommendations.append("Several optimization opportunities identified. Review and prioritize optimization efforts.")
        
        if insight_types[InsightType.BREAKTHROUGH_DETECTED] > 0:
            recommendations.append("Breakthroughs detected! Investigate and capitalize on these positive developments.")
        
        if insight_types[InsightType.ANOMALY_DETECTED] > 1:
            recommendations.append("Multiple anomalies detected. Investigate potential issues or unusual patterns.")
        
        # Add general recommendations
        recommendations.append("Continue monitoring analytics trends for ongoing optimization opportunities.")
        recommendations.append("Consider implementing automated alerts for significant changes in key metrics.")
        
        return recommendations
    
    def _calculate_confidence_score(self, insights: List[AnalyticsInsight]) -> float:
        """Calculate overall confidence score for insights."""
        if not insights:
            return 0.0
        
        confidence_scores = [insight.confidence for insight in insights]
        return statistics.mean(confidence_scores)
    
    def _calculate_data_coverage(self, summary: Dict[str, Any]) -> float:
        """Calculate data coverage percentage."""
        if not summary:
            return 0.0
        
        total_modules = len(self.analytics_modules)
        active_modules = len([s for s in summary.values() if s])
        
        return active_modules / total_modules if total_modules > 0 else 0.0
    
    def _prepare_export_data(self, insights: List[AnalyticsInsight], metrics: List[AnalyticsMetrics], summary: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare data for export."""
        return {
            "insights": [asdict(insight) for insight in insights],
            "metrics": [asdict(metric) for metric in metrics],
            "summary": summary,
            "exported_at": datetime.now().isoformat(),
            "total_insights": len(insights),
            "total_metrics": len(metrics)
        }
    
    def _save_report(self, report: UnifiedAnalyticsReport):
        """Save a report to the database."""
        with sqlite3.connect(self.analytics_db) as conn:
            conn.execute(
                """INSERT INTO analytics_reports 
                   (report_id, generated_at, time_period, analytics_summary, insights, metrics, 
                    correlations, recommendations, export_data, confidence_score, data_coverage)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                (
                    report.report_id,
                    report.generated_at.isoformat(),
                    report.time_period,
                    json.dumps(report.analytics_summary),
                    json.dumps([asdict(insight) for insight in report.insights]),
                    json.dumps([asdict(metric) for metric in report.metrics]),
                    json.dumps(report.correlations),
                    json.dumps(report.recommendations),
                    json.dumps(report.export_data),
                    report.confidence_score,
                    report.data_coverage
                )
            )
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary of analytics operations."""
        with sqlite3.connect(self.performance_db) as conn:
            cursor = conn.execute("""
                SELECT operation, AVG(duration) as avg_duration, COUNT(*) as count
                FROM performance_metrics 
                GROUP BY operation
                ORDER BY avg_duration DESC
            """)
            
            performance_data = []
            for row in cursor.fetchall():
                performance_data.append({
                    "operation": row[0],
                    "avg_duration": row[1],
                    "count": row[2]
                })
        
        return {
            "performance_summary": performance_data,
            "total_operations": sum(p["count"] for p in performance_data),
            "avg_duration": statistics.mean(p["avg_duration"] for p in performance_data) if performance_data else 0
        }
    
    def clear_cache(self):
        """Clear all cached data."""
        with sqlite3.connect(self.cache_db) as conn:
            conn.execute("DELETE FROM analytics_cache")
        logger.info("Analytics cache cleared")
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get overall system status."""
        return {
            "enabled": self.config.enabled,
            "modules_registered": len(self.analytics_modules),
            "cache_enabled": self.config.cache_enabled,
            "data_dir": str(self.data_dir),
            "performance_summary": self.get_performance_summary()
        } 