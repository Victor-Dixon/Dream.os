From 9b92eace53f6b36a0017f9f9e0f939daa629cd0f Mon Sep 17 00:00:00 2001
From: OrganizerApp <organizer@app.local>
Date: Fri, 15 Aug 2025 22:13:09 -0500
Subject: [PATCH] feat(prd): Add comprehensive Project Requirements Document

---
 AI agent/main.py            | 250 ++++++++++++++++++------------------
 PRD.md                      |  73 +++++++++++
 TASK_LIST.md                | 109 ++++------------
 TASK_LIST.md.backup         |  90 +++++++++++++
 config/training_config.yaml |   1 +
 pytest.ini                  |   1 +
 test_q_learning.py          |   1 +
 7 files changed, 318 insertions(+), 207 deletions(-)
 create mode 100644 PRD.md
 create mode 100644 TASK_LIST.md.backup

diff --git a/AI agent/main.py b/AI agent/main.py
index 83867c9..e611970 100644
--- a/AI agent/main.py	
+++ b/AI agent/main.py	
@@ -1,125 +1,125 @@
-import gym
-import random
-import numpy as np
-from collections import deque
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import warnings
-warnings.filterwarnings("ignore", category=DeprecationWarning)
-
-# Define the Q-network
-class QNetwork(nn.Module):
-    def __init__(self, state_size, action_size, hidden_size=64):
-        super(QNetwork, self).__init__()
-        self.fc1 = nn.Linear(state_size, hidden_size)
-        self.relu = nn.ReLU()
-        self.fc2 = nn.Linear(hidden_size, action_size)
-    
-    def forward(self, state):
-        x = self.fc1(state)
-        x = self.relu(x)
-        return self.fc2(x)
-
-def train_batch(q_network, optimizer, memory, batch_size, gamma):
-    if len(memory) < batch_size:
-        return
-    batch = random.sample(memory, batch_size)
-    states, actions, rewards, next_states, dones = zip(*batch)
-
-    # Convert states to proper tensor format - states are stored as [[x,y,z,w]] so we need to squeeze
-    states = torch.FloatTensor(states).squeeze(1)  # Remove extra dimension
-    actions = torch.LongTensor(actions)
-    rewards = torch.FloatTensor(rewards)
-    next_states = torch.FloatTensor(next_states).squeeze(1)  # Remove extra dimension
-    dones = torch.FloatTensor(dones)
-
-    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
-    next_q_values = q_network(next_states).max(1)[0]
-    expected_q_values = rewards + gamma * next_q_values * (1 - dones)
-    
-    loss = nn.MSELoss()(q_values, expected_q_values.detach())
-    optimizer.zero_grad()
-    loss.backward()
-    optimizer.step()
-
-def save_model(q_network, file_name="q_network.pth"):
-    torch.save(q_network.state_dict(), file_name)
-
-# Initialize environment, network, and optimizer
-env = gym.make('CartPole-v1')
-state_size = env.observation_space.shape[0]
-action_size = env.action_space.n
-q_network = QNetwork(state_size, action_size)
-optimizer = optim.Adam(q_network.parameters(), lr=0.001)
-
-# Training hyperparameters
-episodes = 1000
-gamma = 0.99  # Discount rate
-epsilon = 1.0  # Exploration rate
-epsilon_min = 0.01
-epsilon_decay = 0.995
-batch_size = 20
-memory = deque(maxlen=2000)
-
-# Function to choose an action
-def choose_action(state, epsilon, environment):
-    if random.uniform(0, 1) < epsilon:
-        return environment.action_space.sample()  # Explore
-    else:
-        state = torch.FloatTensor(state).unsqueeze(0)
-        with torch.no_grad():
-            action_values = q_network(state)
-        return np.argmax(action_values.numpy())  # Exploit
-
-def load_model(q_network, file_name="q_network.pth"):
-    """Load a trained model from file"""
-    try:
-        q_network.load_state_dict(torch.load(file_name))
-        q_network.eval()  # Set to evaluation mode
-        print(f"Model loaded successfully from {file_name}")
-        return True
-    except FileNotFoundError:
-        print(f"Model file {file_name} not found")
-        return False
-    except Exception as e:
-        print(f"Error loading model: {e}")
-        return False
-
-# Only run training if this file is run directly (not imported)
-if __name__ == "__main__":
-    # Training loop
-    for e in range(episodes):
-        state, _ = env.reset()
-        state = np.reshape(state, [1, state_size])
-        total_reward = 0
-
-        for time in range(500):  # 500 timesteps max
-            action = choose_action(state, epsilon, env)
-            next_state, reward, done, _, _ = env.step(action)
-            next_state = np.reshape(next_state, [1, state_size])
-            
-            # Store the transition in memory
-            memory.append((state, action, reward, next_state, done))
-            
-            # Move to the next state
-            state = next_state
-            total_reward += reward
-
-            # Perform one step of the optimization
-            train_batch(q_network, optimizer, memory, batch_size, gamma)
-            
-            if done:
-                break
-        
-        if epsilon > epsilon_min:
-            epsilon *= epsilon_decay
-
-        print(f"Episode: {e+1}/{episodes}, Score: {total_reward}")
-
-        # Optionally save the model every N episodes
-        if e % 100 == 0:
-            save_model(q_network, f"q_network_{e}.pth")
-
-    # Save the final model
-    save_model(q_network, "q_network_final.pth")
+import gym
+import random
+import numpy as np
+from collections import deque
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import warnings
+warnings.filterwarnings("ignore", category=DeprecationWarning)
+
+# Define the Q-network
+class QNetwork(nn.Module):
+    def __init__(self, state_size, action_size, hidden_size=64):
+        super(QNetwork, self).__init__()
+        self.fc1 = nn.Linear(state_size, hidden_size)
+        self.relu = nn.ReLU()
+        self.fc2 = nn.Linear(hidden_size, action_size)
+    
+    def forward(self, state):
+        x = self.fc1(state)
+        x = self.relu(x)
+        return self.fc2(x)
+
+def train_batch(q_network, optimizer, memory, batch_size, gamma):
+    if len(memory) < batch_size:
+        return
+    batch = random.sample(memory, batch_size)
+    states, actions, rewards, next_states, dones = zip(*batch)
+
+    # Convert states to proper tensor format - states are stored as [[x,y,z,w]] so we need to squeeze
+    states = torch.FloatTensor(states).squeeze(1)  # Remove extra dimension
+    actions = torch.LongTensor(actions)
+    rewards = torch.FloatTensor(rewards)
+    next_states = torch.FloatTensor(next_states).squeeze(1)  # Remove extra dimension
+    dones = torch.FloatTensor(dones)
+
+    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
+    next_q_values = q_network(next_states).max(1)[0]
+    expected_q_values = rewards + gamma * next_q_values * (1 - dones)
+    
+    loss = nn.MSELoss()(q_values, expected_q_values.detach())
+    optimizer.zero_grad()
+    loss.backward()
+    optimizer.step()
+
+def save_model(q_network, file_name="q_network.pth"):
+    torch.save(q_network.state_dict(), file_name)
+
+# Initialize environment, network, and optimizer
+env = gym.make('CartPole-v1')
+state_size = env.observation_space.shape[0]
+action_size = env.action_space.n
+q_network = QNetwork(state_size, action_size)
+optimizer = optim.Adam(q_network.parameters(), lr=0.001)
+
+# Training hyperparameters
+episodes = 1000
+gamma = 0.99  # Discount rate
+epsilon = 1.0  # Exploration rate
+epsilon_min = 0.01
+epsilon_decay = 0.995
+batch_size = 20
+memory = deque(maxlen=2000)
+
+# Function to choose an action
+def choose_action(state, epsilon, environment):
+    if random.uniform(0, 1) < epsilon:
+        return environment.action_space.sample()  # Explore
+    else:
+        state = torch.FloatTensor(state).unsqueeze(0)
+        with torch.no_grad():
+            action_values = q_network(state)
+        return np.argmax(action_values.numpy())  # Exploit
+
+def load_model(q_network, file_name="q_network.pth"):
+    """Load a trained model from file"""
+    try:
+        q_network.load_state_dict(torch.load(file_name))
+        q_network.eval()  # Set to evaluation mode
+        print(f"Model loaded successfully from {file_name}")
+        return True
+    except FileNotFoundError:
+        print(f"Model file {file_name} not found")
+        return False
+    except Exception as e:
+        print(f"Error loading model: {e}")
+        return False
+
+# Only run training if this file is run directly (not imported)
+if __name__ == "__main__":
+    # Training loop
+    for e in range(episodes):
+        state, _ = env.reset()
+        state = np.reshape(state, [1, state_size])
+        total_reward = 0
+
+        for time in range(500):  # 500 timesteps max
+            action = choose_action(state, epsilon, env)
+            next_state, reward, done, _, _ = env.step(action)
+            next_state = np.reshape(next_state, [1, state_size])
+            
+            # Store the transition in memory
+            memory.append((state, action, reward, next_state, done))
+            
+            # Move to the next state
+            state = next_state
+            total_reward += reward
+
+            # Perform one step of the optimization
+            train_batch(q_network, optimizer, memory, batch_size, gamma)
+            
+            if done:
+                break
+        
+        if epsilon > epsilon_min:
+            epsilon *= epsilon_decay
+
+        print(f"Episode: {e+1}/{episodes}, Score: {total_reward}")
+
+        # Optionally save the model every N episodes
+        if e % 100 == 0:
+            save_model(q_network, f"q_network_{e}.pth")
+
+    # Save the final model
+    save_model(q_network, "q_network_final.pth")
diff --git a/PRD.md b/PRD.md
new file mode 100644
index 0000000..e8bf2ea
--- /dev/null
+++ b/PRD.md
@@ -0,0 +1,73 @@
+# Project Requirements Document (PRD)
+
+## Project Overview
+- **Project Name**: DaDudekC - Q-Learning CartPole AI
+- **Version**: 1.0.0
+- **Last Updated**: 2025-08-15
+- **Status**: Active Development
+
+## Objectives
+- Implement a robust Q-learning neural network system for solving the CartPole environment
+- Create a modular, testable reinforcement learning framework with comprehensive test coverage
+- Develop a standalone training script with configurable hyperparameters
+- Establish a production-ready ML pipeline with proper error handling and logging
+- Provide comprehensive documentation and examples for researchers and developers
+
+## Features
+### Core Features
+- Q-Network neural network implementation using PyTorch
+- Experience replay memory buffer for stable training
+- Epsilon-greedy exploration strategy with configurable decay
+- Modular training pipeline with separate training and evaluation modes
+- Comprehensive test suite with pytest and coverage reporting
+- YAML-based configuration management for hyperparameters
+
+### Future Features
+- Support for additional OpenAI Gym environments
+- Advanced exploration strategies (UCB, Thompson sampling)
+- Distributed training capabilities
+- Real-time training visualization and monitoring
+- Model versioning and experiment tracking
+
+## Requirements
+### Functional Requirements
+- [FR1] Train Q-network to solve CartPole environment with 500+ timestep performance
+- [FR2] Support both training and evaluation modes with command-line interface
+- [FR3] Implement experience replay buffer with configurable memory size
+- [FR4] Provide model saving/loading functionality at configurable intervals
+- [FR5] Generate comprehensive training reports and performance metrics
+- [FR6] Support custom hyperparameter configuration via YAML files
+
+### Non-Functional Requirements
+- [NFR1] Training completion within 1000 episodes for standard configurations
+- [NFR2] Memory usage under 2GB for standard training runs
+- [NFR3] Test coverage above 90% for all core functionality
+- [NFR4] Support for Python 3.8+ environments
+- [NFR5] Cross-platform compatibility (Windows, Linux, macOS)
+
+## Technical Specifications
+- **Language**: Python 3.8+
+- **Framework**: PyTorch 1.9.0+, OpenAI Gym 0.21.0+
+- **Database**: None (file-based storage for models and data)
+- **Architecture**: Modular design with separated concerns
+- **Testing**: pytest with coverage reporting
+- **Dependencies**: torch, gym, numpy, pytest, black, flake8
+
+## Timeline
+- **Phase 1**: 2025-08-15 to 2025-08-22 - Core Q-learning implementation and testing
+- **Phase 2**: 2025-08-23 to 2025-08-30 - Advanced features and optimization
+- **Phase 3**: 2025-09-01 to 2025-09-07 - Documentation and deployment preparation
+
+## Acceptance Criteria
+- [AC1] Q-network successfully balances CartPole for 500+ timesteps in 80% of evaluation runs
+- [AC2] All tests pass with coverage above 90%
+- [AC3] Training script runs without errors for both training and evaluation modes
+- [AC4] Model files are properly saved and loaded with correct performance
+- [AC5] Documentation is complete and accurate for all user-facing functionality
+- [AC6] Code quality meets PEP 8 standards with no linting errors
+
+## Risks & Mitigation
+- **Risk 1**: Training instability due to hyperparameter sensitivity - Mitigation: Implement adaptive learning rates and comprehensive hyperparameter tuning
+- **Risk 2**: Memory overflow during long training runs - Mitigation: Implement memory monitoring and automatic cleanup
+- **Risk 3**: Dependency version conflicts - Mitigation: Use virtual environments and pin dependency versions
+- **Risk 4**: Poor generalization to different environments - Mitigation: Implement environment abstraction layer for easy extension
diff --git a/TASK_LIST.md b/TASK_LIST.md
index c6b15ca..70c3cea 100644
--- a/TASK_LIST.md
+++ b/TASK_LIST.md
@@ -1,90 +1,35 @@
+# Task List
 
-<!-- STANDARD_TASK_LIST_v1 -->
-# TASK_LIST.md ‚Äì Roadmap to Beta
+## üìã Project: DaDudekC
+**Last Updated**: 2025-08-15
+**Status**: Active
 
-Repo: DaDudekC
+## üéØ Current Sprint
+### In Progress
+- [ ] [Task 1] - [Description] - [Assignee] - [Due Date]
+- [ ] [Task 2] - [Description] - [Assignee] - [Due Date]
 
-## Roadmap to Beta
+### Completed
+- [x] [Task 1] - [Description] - [Assignee] - [Completed Date]
+- [x] [Task 2] - [Description] - [Assignee] - [Completed Date]
 
-- [x] **DEPENDENCIES FIXED** - Requirements.txt updated with proper ML dependencies
-- [x] **TESTING FRAMEWORK** - Comprehensive pytest setup with coverage
-- [ ] GUI loads cleanly without errors
-- [ ] Buttons/menus wired to working handlers
-- [ ] Happy‚Äëpath flows implemented and documented
-- [x] Basic tests covering critical paths
-- [ ] README quickstart up‚Äëto‚Äëdate
-- [ ] Triage and address critical issues
+### Blocked
+- [ ] [Task 1] - [Description] - [Assignee] - [Blocker Description]
 
-## Task List (Small, verifiable steps)
+## üöÄ Next Sprint
+- [ ] [Task 1] - [Description] - [Assignee] - [Priority]
+- [ ] [Task 2] - [Description] - [Assignee] - [Priority]
 
-- [x] **Task 1: Fix requirements.txt** ‚úÖ COMPLETED
-  - **What**: Added proper ML dependencies (torch, gym, numpy) and dev tools
-  - **Impact**: Project can now be properly installed and tested
-  - **Evidence**: `requirements.txt` contains all necessary packages
+## üìä Progress Metrics
+- **Total Tasks**: [NUMBER]
+- **Completed**: [NUMBER]
+- **In Progress**: [NUMBER]
+- **Blocked**: [NUMBER]
+- **Completion Rate**: [PERCENTAGE]%
 
-- [x] **Task 2: Create comprehensive test suite** ‚úÖ COMPLETED
-  - **What**: Added `test_q_learning.py` with full test coverage
-  - **Impact**: Code quality assurance, regression prevention
-  - **Evidence**: `test_q_learning.py` covers all major functions and edge cases
-
-- [x] **Task 3: Add pytest configuration** ‚úÖ COMPLETED
-  - **What**: Created `pytest.ini` with coverage and proper test discovery
-  - **Impact**: Professional testing setup with coverage reporting
-  - **Evidence**: `pytest.ini` configured for comprehensive testing
-
-- [ ] **Task 4: Fix main.py import issues** üîÑ IN PROGRESS
-  - **What**: Fix the `choose_action` function to accept environment parameter
-  - **Acceptance**: All tests pass without import errors
-  - **Next**: Update `AI agent/main.py` to fix function signature
-
-- [ ] **Task 5: Add model loading functionality** üìã PENDING
-  - **What**: Implement `load_model` function to complement `save_model`
-  - **Acceptance**: Can save and reload trained models
-  - **Next**: Add function to `AI agent/main.py`
-
-- [ ] **Task 6: Create training script** üìã PENDING
-  - **What**: Separate training logic into standalone script
-  - **Acceptance**: Can run training independently of main execution
-  - **Next**: Create `train_cartpole.py` script
-
-- [ ] **Task 7: Add configuration file** üìã PENDING
-  - **What**: Create `config.yaml` for hyperparameters
-  - **Acceptance**: Training parameters easily configurable
-  - **Next**: Create `config/training_config.yaml`
-
-## Acceptance Criteria (per task)
-
-- Clear, testable criteria
-- Measurable output or evidence
-- All tests pass after implementation
-- No breaking changes to existing functionality
-
-## Evidence Links
-
-- **Task 1**: `requirements.txt` - Proper ML dependencies
-- **Task 2**: `test_q_learning.py` - Comprehensive test coverage
-- **Task 3**: `pytest.ini` - Professional testing configuration
-- **Task 4**: `AI agent/main.py` - Function signature fixes needed
-
-## Progress Log
-
-- **2025-08-15**: Fixed requirements.txt with proper ML dependencies
-- **2025-08-15**: Created comprehensive test suite for Q-learning implementation
-- **2025-08-15**: Added pytest configuration with coverage reporting
-- **2025-08-15**: Identified import issues in main.py that need fixing
-
-## Next High-Leverage Actions
-
-1. **Fix main.py function signatures** to resolve import errors
-2. **Run tests** to verify current functionality
-3. **Add model loading** for complete save/load capability
-4. **Create training script** for better code organization
-5. **Add configuration management** for hyperparameter tuning
-
-## Current Issues to Address
-
-1. **Import Error**: `choose_action` function signature mismatch
-2. **Missing Functionality**: No model loading capability
-3. **Code Organization**: Training logic mixed with main execution
-4. **Configuration**: Hard-coded hyperparameters throughout code
+## üîß Technical Debt
+- [ ] [Technical Debt Item 1]
+- [ ] [Technical Debt Item 2]
 
+## üìù Notes
+[Any additional notes or context]
diff --git a/TASK_LIST.md.backup b/TASK_LIST.md.backup
new file mode 100644
index 0000000..c6b15ca
--- /dev/null
+++ b/TASK_LIST.md.backup
@@ -0,0 +1,90 @@
+
+<!-- STANDARD_TASK_LIST_v1 -->
+# TASK_LIST.md ‚Äì Roadmap to Beta
+
+Repo: DaDudekC
+
+## Roadmap to Beta
+
+- [x] **DEPENDENCIES FIXED** - Requirements.txt updated with proper ML dependencies
+- [x] **TESTING FRAMEWORK** - Comprehensive pytest setup with coverage
+- [ ] GUI loads cleanly without errors
+- [ ] Buttons/menus wired to working handlers
+- [ ] Happy‚Äëpath flows implemented and documented
+- [x] Basic tests covering critical paths
+- [ ] README quickstart up‚Äëto‚Äëdate
+- [ ] Triage and address critical issues
+
+## Task List (Small, verifiable steps)
+
+- [x] **Task 1: Fix requirements.txt** ‚úÖ COMPLETED
+  - **What**: Added proper ML dependencies (torch, gym, numpy) and dev tools
+  - **Impact**: Project can now be properly installed and tested
+  - **Evidence**: `requirements.txt` contains all necessary packages
+
+- [x] **Task 2: Create comprehensive test suite** ‚úÖ COMPLETED
+  - **What**: Added `test_q_learning.py` with full test coverage
+  - **Impact**: Code quality assurance, regression prevention
+  - **Evidence**: `test_q_learning.py` covers all major functions and edge cases
+
+- [x] **Task 3: Add pytest configuration** ‚úÖ COMPLETED
+  - **What**: Created `pytest.ini` with coverage and proper test discovery
+  - **Impact**: Professional testing setup with coverage reporting
+  - **Evidence**: `pytest.ini` configured for comprehensive testing
+
+- [ ] **Task 4: Fix main.py import issues** üîÑ IN PROGRESS
+  - **What**: Fix the `choose_action` function to accept environment parameter
+  - **Acceptance**: All tests pass without import errors
+  - **Next**: Update `AI agent/main.py` to fix function signature
+
+- [ ] **Task 5: Add model loading functionality** üìã PENDING
+  - **What**: Implement `load_model` function to complement `save_model`
+  - **Acceptance**: Can save and reload trained models
+  - **Next**: Add function to `AI agent/main.py`
+
+- [ ] **Task 6: Create training script** üìã PENDING
+  - **What**: Separate training logic into standalone script
+  - **Acceptance**: Can run training independently of main execution
+  - **Next**: Create `train_cartpole.py` script
+
+- [ ] **Task 7: Add configuration file** üìã PENDING
+  - **What**: Create `config.yaml` for hyperparameters
+  - **Acceptance**: Training parameters easily configurable
+  - **Next**: Create `config/training_config.yaml`
+
+## Acceptance Criteria (per task)
+
+- Clear, testable criteria
+- Measurable output or evidence
+- All tests pass after implementation
+- No breaking changes to existing functionality
+
+## Evidence Links
+
+- **Task 1**: `requirements.txt` - Proper ML dependencies
+- **Task 2**: `test_q_learning.py` - Comprehensive test coverage
+- **Task 3**: `pytest.ini` - Professional testing configuration
+- **Task 4**: `AI agent/main.py` - Function signature fixes needed
+
+## Progress Log
+
+- **2025-08-15**: Fixed requirements.txt with proper ML dependencies
+- **2025-08-15**: Created comprehensive test suite for Q-learning implementation
+- **2025-08-15**: Added pytest configuration with coverage reporting
+- **2025-08-15**: Identified import issues in main.py that need fixing
+
+## Next High-Leverage Actions
+
+1. **Fix main.py function signatures** to resolve import errors
+2. **Run tests** to verify current functionality
+3. **Add model loading** for complete save/load capability
+4. **Create training script** for better code organization
+5. **Add configuration management** for hyperparameter tuning
+
+## Current Issues to Address
+
+1. **Import Error**: `choose_action` function signature mismatch
+2. **Missing Functionality**: No model loading capability
+3. **Code Organization**: Training logic mixed with main execution
+4. **Configuration**: Hard-coded hyperparameters throughout code
+
diff --git a/config/training_config.yaml b/config/training_config.yaml
index 473253f..387325f 100644
--- a/config/training_config.yaml
+++ b/config/training_config.yaml
@@ -33,3 +33,4 @@ model:
 evaluation:
   episodes: 10              # Number of evaluation episodes
   no_exploration: true      # No exploration during evaluation
+
diff --git a/pytest.ini b/pytest.ini
index e4c5794..2d3ceb9 100644
--- a/pytest.ini
+++ b/pytest.ini
@@ -15,3 +15,4 @@ markers =
     slow: marks tests as slow (deselect with '-m "not slow"')
     integration: marks tests as integration tests
     unit: marks tests as unit tests
+
diff --git a/test_q_learning.py b/test_q_learning.py
index d57262e..035881e 100644
--- a/test_q_learning.py
+++ b/test_q_learning.py
@@ -174,3 +174,4 @@ class TestModelPersistence:
 
 if __name__ == "__main__":
     pytest.main([__file__, "-v"])
+
-- 
2.50.1.windows.1

